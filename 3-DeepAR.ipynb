{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker/DeepAR demo on Stores Daily Sales\n",
    "\n",
    "In this lab, we will use Amazon SageMaker build-in algorithm DeepAR to forecast 1604 stores daily sales.\n",
    "\n",
    "In particular, we will see how to:\n",
    "* Use the SageMaker Python SDK to train a DeepAR model and deploy it\n",
    "* Make requests to the deployed model to obtain forecasts interactively\n",
    "* Illustrate advanced features of DeepAR: additional time features and category information\n",
    "\n",
    "Running this notebook takes around 40 min on a ml.c4.2xlarge for the training, and inference is done on a ml.m4.xlarge (the usage time will depend on how long you leave your served model running).\n",
    "\n",
    "For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from decouple import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'tko-ts-workshop'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset and upload it to S3 to make it available for Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already has pre-processed the data we will use, it is under the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'data/timeseries_raw.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier. The data has been processed, and the frequency is 1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_NAME, sep=\",\", index_col=0, parse_dates=True, decimal=',')\n",
    "data.index = pd.DatetimeIndex(data.index,freq=\"1D\")\n",
    "\n",
    "num_timeseries = data.shape[1]\n",
    "\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data.iloc[:,i], trim='f').astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the resulting time series for the first ten stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-07-10 00:00:00 to 2019-10-09 23:59:59\n",
    "\n",
    "DATETIME_START_OF_TRAIN = config('DATETIME_START_OF_TRAIN')\n",
    "DATETIME_END_OF_TRAIN = config('DATETIME_END_OF_TRAIN')\n",
    "DATETIME_START_OF_TEST = config('DATETIME_START_OF_TEST')\n",
    "DATETIME_END_OF_TEST = config('DATETIME_END_OF_TEST')\n",
    "DATETIME_START_OF_PREDICT = config('DATETIME_START_OF_PREDICT')\n",
    "DATETIME_END_OF_PREDICT = config('DATETIME_END_OF_PREDICT')\n",
    "\n",
    "# we use 1 day frequency for the time series\n",
    "freq = config('freq')\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = config('prediction_length', cast=int)\n",
    "\n",
    "# we also use 14 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = config('context_length', cast=int)\n",
    "\n",
    "sample_sites = config('sample_sites', cast=lambda v: [int(s.strip()) for s in v.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(len(sample_sites), 1, figsize=(15, 10), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i, site in enumerate(sample_sites):\n",
    "    timeseries[site].loc[DATETIME_START_OF_TRAIN:DATETIME_END_OF_PREDICT].astype(float).plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"sales RMB\")   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify here the portion of the data that is used for training: the model sees data from 2019-07-10 to 2019-09-16 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(DATETIME_START_OF_TRAIN, freq=freq)\n",
    "end_training = pd.Timestamp(DATETIME_END_OF_TRAIN, freq=freq)\n",
    "end_test = pd.Timestamp(DATETIME_END_OF_TEST, freq=freq)\n",
    "start_predict = pd.Timestamp(DATETIME_START_OF_PREDICT, freq=freq)\n",
    "end_predict = pd.Timestamp(DATETIME_END_OF_PREDICT, freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - 1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-09-24 00:00:00', freq='D')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_test - 1].tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.7 ms, sys: 3.93 ms, total: 95.6 ms\n",
      "Wall time: 95.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"data/train.json\", training_data)\n",
    "write_dicts_to_file(\"data/test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-cn-north-1-346044390830/tko-ts-workshop/data/train/train.json\n",
      "Uploading file to s3://sagemaker-cn-north-1-346044390830/tko-ts-workshop/data/test/test.json\n",
      "CPU times: user 32.5 ms, sys: 7.6 ms, total: 40.1 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"data/train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"data/test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2019-07-10 00:00:00\", \"target\": [14.0, 18.0, 22.4, 28.5, 25.0, 7.3, 31.1, 0.0, 27.6, 46.1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-stores-sales-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-13 04:57:07 Starting - Starting the training job...\n",
      "2021-04-13 04:57:30 Starting - Launching requested ML instances.........\n",
      "2021-04-13 04:58:39 Starting - Preparing the instances for training......\n",
      "2021-04-13 04:59:56 Downloading - Downloading input data...\n",
      "2021-04-13 05:00:33 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '7', 'time_freq': '1D', 'context_length': '14', 'epochs': '400', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '32'}\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '7', 'time_freq': '1D', 'context_length': '14', 'epochs': '400'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:50 INFO 140529284842880] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Real time series\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] number of time series: 1604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] number of observations: 110676\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] mean target length: 69.0\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] min/mean/max target: 0.0/21018.129640771138/1653507.875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] mean abs(target): 21018.129640771138\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Real time series\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] number of time series: 1604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] number of observations: 121904\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] mean target length: 76.0\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] min/mean/max target: 0.0/20798.73991889041/1653507.875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] mean abs(target): 20798.73991889041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #memory_usage::<batchbuffer> = 0.60882568359375 mb\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] nvidia-smi took: 0.025339841842651367 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290051.297001, \"EndTime\": 1618290051.3641543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 64.89825248718262, \"count\": 1, \"min\": 64.89825248718262, \"max\": 64.89825248718262}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #memory_usage::<model> = 4 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290051.3642275, \"EndTime\": 1618290051.448762, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 151.64566040039062, \"count\": 1, \"min\": 151.64566040039062, \"max\": 151.64566040039062}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch[0] avg_epoch_loss=9.140727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.140727043151855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch[5] avg_epoch_loss=9.189932\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=9.189931551615397\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch [5]#011Speed: 2028.63 samples/sec#011loss=9.189932\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch[10] avg_epoch_loss=9.276187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=9.379692649841308\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch [10]#011Speed: 830.99 samples/sec#011loss=9.379693\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch[15] avg_epoch_loss=9.059196\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=15 train loss <loss>=8.58181676864624\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:51 INFO 140529284842880] Epoch[0] Batch [15]#011Speed: 2052.17 samples/sec#011loss=8.581817\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[20] avg_epoch_loss=9.088667\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=20 train loss <loss>=9.182973098754882\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [20]#011Speed: 849.78 samples/sec#011loss=9.182973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[25] avg_epoch_loss=8.995315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=25 train loss <loss>=8.603240203857421\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [25]#011Speed: 1912.55 samples/sec#011loss=8.603240\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[30] avg_epoch_loss=8.919957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=30 train loss <loss>=8.528090667724609\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [30]#011Speed: 837.85 samples/sec#011loss=8.528091\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[35] avg_epoch_loss=8.826404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=35 train loss <loss>=8.24637680053711\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [35]#011Speed: 1881.54 samples/sec#011loss=8.246377\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[40] avg_epoch_loss=8.735185\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=40 train loss <loss>=8.078407096862794\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [40]#011Speed: 893.62 samples/sec#011loss=8.078407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[45] avg_epoch_loss=8.697496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=45 train loss <loss>=8.388450622558594\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [45]#011Speed: 2037.28 samples/sec#011loss=8.388451\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch[50] avg_epoch_loss=8.650547\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, batch=50 train loss <loss>=8.218613147735596\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Epoch[0] Batch [50]#011Speed: 1413.01 samples/sec#011loss=8.218613\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] processed a total of 1638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290051.4488351, \"EndTime\": 1618290052.9179842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 1469.0461158752441, \"count\": 1, \"min\": 1469.0461158752441, \"max\": 1469.0461158752441}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1114.9053831423842 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.609231737943796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:52 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_7aba205e-8b67-4dbe-967c-7bf36003d0b4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290052.9180753, \"EndTime\": 1618290052.9352973, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 16.649961471557617, \"count\": 1, \"min\": 16.649961471557617, \"max\": 16.649961471557617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[0] avg_epoch_loss=8.665769\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=8.66576862335205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[5] avg_epoch_loss=8.684336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=8.684336026509603\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [5]#011Speed: 1894.68 samples/sec#011loss=8.684336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[10] avg_epoch_loss=8.684750\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=8.685245895385743\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [10]#011Speed: 721.80 samples/sec#011loss=8.685246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[15] avg_epoch_loss=8.672716\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=15 train loss <loss>=8.64624080657959\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [15]#011Speed: 2090.57 samples/sec#011loss=8.646241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[20] avg_epoch_loss=8.715168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=20 train loss <loss>=8.85101776123047\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [20]#011Speed: 783.43 samples/sec#011loss=8.851018\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[25] avg_epoch_loss=8.663045\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=25 train loss <loss>=8.444128513336182\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [25]#011Speed: 1612.47 samples/sec#011loss=8.444129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch[30] avg_epoch_loss=8.600178\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=30 train loss <loss>=8.273265647888184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:53 INFO 140529284842880] Epoch[1] Batch [30]#011Speed: 669.24 samples/sec#011loss=8.273266\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch[35] avg_epoch_loss=8.540066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=35 train loss <loss>=8.167373752593994\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch [35]#011Speed: 1474.59 samples/sec#011loss=8.167374\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch[40] avg_epoch_loss=8.520897\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=40 train loss <loss>=8.382882690429687\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch [40]#011Speed: 713.08 samples/sec#011loss=8.382883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch[45] avg_epoch_loss=8.488971\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=45 train loss <loss>=8.227174949645995\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch [45]#011Speed: 1597.08 samples/sec#011loss=8.227175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch[50] avg_epoch_loss=8.434004\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, batch=50 train loss <loss>=7.928309345245362\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[1] Batch [50]#011Speed: 1500.50 samples/sec#011loss=7.928309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] processed a total of 1610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290052.9353707, \"EndTime\": 1618290054.5200648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1584.6397876739502, \"count\": 1, \"min\": 1584.6397876739502, \"max\": 1584.6397876739502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1015.9265806384446 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=1, train loss <loss>=8.434004175896739\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_a468a4dc-c9e0-41e0-a53a-0e4461bdf819-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290054.520152, \"EndTime\": 1618290054.5358963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.244722366333008, \"count\": 1, \"min\": 15.244722366333008, \"max\": 15.244722366333008}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[2] Batch[0] avg_epoch_loss=8.470866\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=8.470866203308105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[2] Batch[5] avg_epoch_loss=8.571134\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=8.571133613586426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[2] Batch [5]#011Speed: 1959.96 samples/sec#011loss=8.571134\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[2] Batch[10] avg_epoch_loss=8.564560\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=8.556671714782714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:54 INFO 140529284842880] Epoch[2] Batch [10]#011Speed: 876.76 samples/sec#011loss=8.556672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[15] avg_epoch_loss=8.515749\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=15 train loss <loss>=8.408363723754883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [15]#011Speed: 1890.59 samples/sec#011loss=8.408364\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[20] avg_epoch_loss=8.552976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=20 train loss <loss>=8.672104644775391\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [20]#011Speed: 784.13 samples/sec#011loss=8.672105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[25] avg_epoch_loss=8.513663\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=25 train loss <loss>=8.348548889160156\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [25]#011Speed: 2069.25 samples/sec#011loss=8.348549\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[30] avg_epoch_loss=8.505509\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=30 train loss <loss>=8.463106727600097\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [30]#011Speed: 838.15 samples/sec#011loss=8.463107\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[35] avg_epoch_loss=8.437634\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=35 train loss <loss>=8.016810894012451\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [35]#011Speed: 2094.18 samples/sec#011loss=8.016811\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[40] avg_epoch_loss=8.388281\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=40 train loss <loss>=8.032939434051514\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [40]#011Speed: 896.79 samples/sec#011loss=8.032939\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch[45] avg_epoch_loss=8.391914\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, batch=45 train loss <loss>=8.421701145172118\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Epoch[2] Batch [45]#011Speed: 1887.73 samples/sec#011loss=8.421701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] processed a total of 1580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290054.5359666, \"EndTime\": 1618290055.8974357, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1361.4106178283691, \"count\": 1, \"min\": 1361.4106178283691, \"max\": 1361.4106178283691}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1160.4538500291646 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=2, train loss <loss>=8.31828631401062\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:55 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_67b9da9d-4565-493b-b511-5bb100f04f78-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290055.8975258, \"EndTime\": 1618290055.9135258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.45262336730957, \"count\": 1, \"min\": 15.45262336730957, \"max\": 15.45262336730957}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[0] avg_epoch_loss=9.334334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=9.334334373474121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[5] avg_epoch_loss=8.835225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=8.835225264231363\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [5]#011Speed: 1794.12 samples/sec#011loss=8.835225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[10] avg_epoch_loss=8.688230\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=8.511835289001464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [10]#011Speed: 874.59 samples/sec#011loss=8.511835\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[15] avg_epoch_loss=8.630158\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=15 train loss <loss>=8.502401542663574\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [15]#011Speed: 1855.65 samples/sec#011loss=8.502402\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[20] avg_epoch_loss=8.624479\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=20 train loss <loss>=8.606305694580078\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [20]#011Speed: 806.60 samples/sec#011loss=8.606306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[25] avg_epoch_loss=8.567564\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=25 train loss <loss>=8.328518867492676\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [25]#011Speed: 1873.72 samples/sec#011loss=8.328519\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[30] avg_epoch_loss=8.520076\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=30 train loss <loss>=8.273139953613281\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [30]#011Speed: 788.63 samples/sec#011loss=8.273140\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch[35] avg_epoch_loss=8.442827\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=35 train loss <loss>=7.963885307312012\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:56 INFO 140529284842880] Epoch[3] Batch [35]#011Speed: 2022.53 samples/sec#011loss=7.963885\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[3] Batch[40] avg_epoch_loss=8.353603\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=40 train loss <loss>=7.711190319061279\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[3] Batch [40]#011Speed: 909.07 samples/sec#011loss=7.711190\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[3] Batch[45] avg_epoch_loss=8.338650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, batch=45 train loss <loss>=8.216035556793212\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[3] Batch [45]#011Speed: 1811.49 samples/sec#011loss=8.216036\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] processed a total of 1591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290055.9135945, \"EndTime\": 1618290057.2827954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1369.145393371582, \"count\": 1, \"min\": 1369.145393371582, \"max\": 1369.145393371582}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1161.9246326422872 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=3, train loss <loss>=8.328526744842529\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch[0] avg_epoch_loss=8.513582\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=8.513582229614258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch[5] avg_epoch_loss=8.454606\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=8.454606056213379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch [5]#011Speed: 1854.53 samples/sec#011loss=8.454606\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch[10] avg_epoch_loss=8.539473\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=8.641313552856445\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch [10]#011Speed: 837.59 samples/sec#011loss=8.641314\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch[15] avg_epoch_loss=8.585305\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=15 train loss <loss>=8.68613510131836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch [15]#011Speed: 1799.71 samples/sec#011loss=8.686135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch[20] avg_epoch_loss=8.602769\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=20 train loss <loss>=8.658653068542481\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:57 INFO 140529284842880] Epoch[4] Batch [20]#011Speed: 859.55 samples/sec#011loss=8.658653\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[25] avg_epoch_loss=8.568713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=25 train loss <loss>=8.425679874420165\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [25]#011Speed: 1971.72 samples/sec#011loss=8.425680\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[30] avg_epoch_loss=8.524599\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=30 train loss <loss>=8.295206832885743\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [30]#011Speed: 824.34 samples/sec#011loss=8.295207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[35] avg_epoch_loss=8.451252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=35 train loss <loss>=7.99649829864502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [35]#011Speed: 1947.29 samples/sec#011loss=7.996498\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[40] avg_epoch_loss=8.382088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=40 train loss <loss>=7.884105396270752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [40]#011Speed: 892.44 samples/sec#011loss=7.884105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[45] avg_epoch_loss=8.366886\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=45 train loss <loss>=8.24223518371582\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [45]#011Speed: 1741.72 samples/sec#011loss=8.242235\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch[50] avg_epoch_loss=8.307804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, batch=50 train loss <loss>=7.764249420166015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[4] Batch [50]#011Speed: 1615.52 samples/sec#011loss=7.764249\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] processed a total of 1617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290057.2828941, \"EndTime\": 1618290058.7006063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1417.1712398529053, \"count\": 1, \"min\": 1417.1712398529053, \"max\": 1417.1712398529053}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1140.9047966623664 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=4, train loss <loss>=8.307804313360476\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_cb659098-3313-4b99-b39e-b3c72be33b26-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290058.700694, \"EndTime\": 1618290058.7167153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.499353408813477, \"count\": 1, \"min\": 15.499353408813477, \"max\": 15.499353408813477}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[5] Batch[0] avg_epoch_loss=8.603911\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=8.603911399841309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[5] Batch[5] avg_epoch_loss=8.619209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=8.619208653767904\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:58 INFO 140529284842880] Epoch[5] Batch [5]#011Speed: 2109.04 samples/sec#011loss=8.619209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[10] avg_epoch_loss=8.574957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=8.521854972839355\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [10]#011Speed: 961.73 samples/sec#011loss=8.521855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[15] avg_epoch_loss=8.639671\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=15 train loss <loss>=8.78204116821289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [15]#011Speed: 1864.41 samples/sec#011loss=8.782041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[20] avg_epoch_loss=8.702491\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=20 train loss <loss>=8.903516960144042\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [20]#011Speed: 900.11 samples/sec#011loss=8.903517\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[25] avg_epoch_loss=8.674393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=25 train loss <loss>=8.556381607055664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [25]#011Speed: 1727.33 samples/sec#011loss=8.556382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[30] avg_epoch_loss=8.644716\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=30 train loss <loss>=8.490396881103516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [30]#011Speed: 916.02 samples/sec#011loss=8.490397\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[35] avg_epoch_loss=8.568990\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=35 train loss <loss>=8.099487781524658\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [35]#011Speed: 2086.95 samples/sec#011loss=8.099488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[40] avg_epoch_loss=8.511110\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=40 train loss <loss>=8.094372463226318\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [40]#011Speed: 929.47 samples/sec#011loss=8.094372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch[45] avg_epoch_loss=8.470785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=45 train loss <loss>=8.140116500854493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:00:59 INFO 140529284842880] Epoch[5] Batch [45]#011Speed: 2097.76 samples/sec#011loss=8.140117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[5] Batch[50] avg_epoch_loss=8.403513\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, batch=50 train loss <loss>=7.784611797332763\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[5] Batch [50]#011Speed: 1743.48 samples/sec#011loss=7.784612\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] processed a total of 1639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290058.7167845, \"EndTime\": 1618290060.0413845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1324.5408535003662, \"count\": 1, \"min\": 1324.5408535003662, \"max\": 1324.5408535003662}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1237.2983462286368 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=5, train loss <loss>=8.388048272866468\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[0] avg_epoch_loss=8.769637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=8.769637107849121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[5] avg_epoch_loss=8.570586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=8.570586363474527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [5]#011Speed: 1692.83 samples/sec#011loss=8.570586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[10] avg_epoch_loss=8.445077\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=8.294465065002441\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [10]#011Speed: 860.29 samples/sec#011loss=8.294465\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[15] avg_epoch_loss=8.448893\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=15 train loss <loss>=8.457289695739746\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [15]#011Speed: 2166.47 samples/sec#011loss=8.457290\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[20] avg_epoch_loss=8.508526\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=20 train loss <loss>=8.699351119995118\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [20]#011Speed: 915.20 samples/sec#011loss=8.699351\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[25] avg_epoch_loss=8.443005\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=25 train loss <loss>=8.167817878723145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [25]#011Speed: 2096.14 samples/sec#011loss=8.167818\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch[30] avg_epoch_loss=8.393959\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=30 train loss <loss>=8.138919734954834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:00 INFO 140529284842880] Epoch[6] Batch [30]#011Speed: 912.75 samples/sec#011loss=8.138920\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch[35] avg_epoch_loss=8.340279\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=35 train loss <loss>=8.007462120056152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch [35]#011Speed: 1715.75 samples/sec#011loss=8.007462\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch[40] avg_epoch_loss=8.274320\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=40 train loss <loss>=7.799417304992676\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch [40]#011Speed: 849.33 samples/sec#011loss=7.799417\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch[45] avg_epoch_loss=8.277956\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, batch=45 train loss <loss>=8.307767105102538\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[6] Batch [45]#011Speed: 1648.09 samples/sec#011loss=8.307767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290060.0414658, \"EndTime\": 1618290061.363015, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1320.993185043335, \"count\": 1, \"min\": 1320.993185043335, \"max\": 1320.993185043335}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1178.5137656532875 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=6, train loss <loss>=8.279818904643156\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_14fad8b8-7300-40dc-90dc-58275bff7b46-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290061.363137, \"EndTime\": 1618290061.379191, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.498638153076172, \"count\": 1, \"min\": 15.498638153076172, \"max\": 15.498638153076172}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch[0] avg_epoch_loss=8.420758\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=8.420758247375488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch[5] avg_epoch_loss=8.073795\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=8.073794921239218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch [5]#011Speed: 2004.57 samples/sec#011loss=8.073795\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch[10] avg_epoch_loss=8.275452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=8.51744031906128\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch [10]#011Speed: 877.73 samples/sec#011loss=8.517440\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch[15] avg_epoch_loss=8.451426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=15 train loss <loss>=8.838567543029786\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:01 INFO 140529284842880] Epoch[7] Batch [15]#011Speed: 1808.78 samples/sec#011loss=8.838568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[20] avg_epoch_loss=8.403771\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=20 train loss <loss>=8.251276016235352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [20]#011Speed: 915.73 samples/sec#011loss=8.251276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[25] avg_epoch_loss=8.428702\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=25 train loss <loss>=8.533415031433105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [25]#011Speed: 1686.48 samples/sec#011loss=8.533415\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[30] avg_epoch_loss=8.390258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=30 train loss <loss>=8.190345668792725\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [30]#011Speed: 836.12 samples/sec#011loss=8.190346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[35] avg_epoch_loss=8.362962\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=35 train loss <loss>=8.193730354309082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [35]#011Speed: 2151.39 samples/sec#011loss=8.193730\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[40] avg_epoch_loss=8.316549\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=40 train loss <loss>=7.982369327545166\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [40]#011Speed: 908.55 samples/sec#011loss=7.982369\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[45] avg_epoch_loss=8.289570\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=45 train loss <loss>=8.068342590332032\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [45]#011Speed: 1687.36 samples/sec#011loss=8.068343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch[50] avg_epoch_loss=8.282410\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, batch=50 train loss <loss>=8.216544914245606\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[7] Batch [50]#011Speed: 1369.20 samples/sec#011loss=8.216545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] processed a total of 1704 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290061.3792608, \"EndTime\": 1618290062.8460317, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1466.7155742645264, \"count\": 1, \"min\": 1466.7155742645264, \"max\": 1466.7155742645264}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1161.6721839003487 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=7, train loss <loss>=8.286188425841155\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] Epoch[8] Batch[0] avg_epoch_loss=8.526809\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=8.526808738708496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[5] avg_epoch_loss=8.268093\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=8.268093268076578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [5]#011Speed: 1731.27 samples/sec#011loss=8.268093\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[10] avg_epoch_loss=8.450486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=8.669358253479004\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [10]#011Speed: 951.37 samples/sec#011loss=8.669358\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[15] avg_epoch_loss=8.563774\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=15 train loss <loss>=8.81300754547119\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [15]#011Speed: 1689.90 samples/sec#011loss=8.813008\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[20] avg_epoch_loss=8.550595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=20 train loss <loss>=8.508423137664796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [20]#011Speed: 858.38 samples/sec#011loss=8.508423\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[25] avg_epoch_loss=8.523118\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=25 train loss <loss>=8.407714080810546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [25]#011Speed: 1772.42 samples/sec#011loss=8.407714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[30] avg_epoch_loss=8.496508\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=30 train loss <loss>=8.358131790161133\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [30]#011Speed: 836.40 samples/sec#011loss=8.358132\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch[35] avg_epoch_loss=8.422813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=35 train loss <loss>=7.965906715393066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:03 INFO 140529284842880] Epoch[8] Batch [35]#011Speed: 1859.56 samples/sec#011loss=7.965907\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch[40] avg_epoch_loss=8.365785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=40 train loss <loss>=7.95518388748169\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch [40]#011Speed: 814.35 samples/sec#011loss=7.955184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch[45] avg_epoch_loss=8.305173\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=45 train loss <loss>=7.808154010772705\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch [45]#011Speed: 1828.91 samples/sec#011loss=7.808154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch[50] avg_epoch_loss=8.241080\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, batch=50 train loss <loss>=7.651426982879639\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[8] Batch [50]#011Speed: 1338.24 samples/sec#011loss=7.651427\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] processed a total of 1672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290062.8461316, \"EndTime\": 1618290064.3295708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1482.9185009002686, \"count\": 1, \"min\": 1482.9185009002686, \"max\": 1482.9185009002686}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1127.403557790218 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=8, train loss <loss>=8.27840084399817\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_3ee87495-2b97-4557-92ff-0dd1bd2b3e2f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290064.3296645, \"EndTime\": 1618290064.346196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.86151123046875, \"count\": 1, \"min\": 15.86151123046875, \"max\": 15.86151123046875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch[0] avg_epoch_loss=8.737496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=8.737496376037598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch[5] avg_epoch_loss=8.816836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=8.81683603922526\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch [5]#011Speed: 2007.47 samples/sec#011loss=8.816836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch[10] avg_epoch_loss=8.686798\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=8.530752658843994\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch [10]#011Speed: 962.25 samples/sec#011loss=8.530753\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch[15] avg_epoch_loss=8.588662\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=15 train loss <loss>=8.372761154174805\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch [15]#011Speed: 2090.23 samples/sec#011loss=8.372761\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch[20] avg_epoch_loss=8.555335\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=20 train loss <loss>=8.448690700531007\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:04 INFO 140529284842880] Epoch[9] Batch [20]#011Speed: 963.60 samples/sec#011loss=8.448691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[25] avg_epoch_loss=8.444222\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=25 train loss <loss>=7.9775464057922365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [25]#011Speed: 1708.64 samples/sec#011loss=7.977546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[30] avg_epoch_loss=8.411423\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=30 train loss <loss>=8.240866661071777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [30]#011Speed: 880.72 samples/sec#011loss=8.240867\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[35] avg_epoch_loss=8.363567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=35 train loss <loss>=8.066860103607178\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [35]#011Speed: 1689.43 samples/sec#011loss=8.066860\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[40] avg_epoch_loss=8.315128\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=40 train loss <loss>=7.966372013092041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [40]#011Speed: 853.22 samples/sec#011loss=7.966372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[45] avg_epoch_loss=8.274255\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=45 train loss <loss>=7.939089393615722\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [45]#011Speed: 1875.26 samples/sec#011loss=7.939089\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch[50] avg_epoch_loss=8.230695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, batch=50 train loss <loss>=7.829951572418213\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[9] Batch [50]#011Speed: 1523.50 samples/sec#011loss=7.829952\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] processed a total of 1644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290064.3462646, \"EndTime\": 1618290065.7568493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1410.5267524719238, \"count\": 1, \"min\": 1410.5267524719238, \"max\": 1410.5267524719238}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1165.4182143608648 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=9, train loss <loss>=8.201773634323708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_fbec23b5-b4c6-4d02-96f5-fffc49ff03bb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290065.7569416, \"EndTime\": 1618290065.7668478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.379148483276367, \"count\": 1, \"min\": 9.379148483276367, \"max\": 9.379148483276367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[10] Batch[0] avg_epoch_loss=8.004304\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=8.004303932189941\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[10] Batch[5] avg_epoch_loss=8.346510\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=8.346509536107382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:05 INFO 140529284842880] Epoch[10] Batch [5]#011Speed: 1922.14 samples/sec#011loss=8.346510\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[10] avg_epoch_loss=8.406345\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=8.478148460388184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [10]#011Speed: 775.47 samples/sec#011loss=8.478148\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[15] avg_epoch_loss=8.428184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=15 train loss <loss>=8.476229286193847\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [15]#011Speed: 1777.13 samples/sec#011loss=8.476229\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[20] avg_epoch_loss=8.429416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=20 train loss <loss>=8.433357620239258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [20]#011Speed: 802.29 samples/sec#011loss=8.433358\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[25] avg_epoch_loss=8.387424\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=25 train loss <loss>=8.211057662963867\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [25]#011Speed: 2082.19 samples/sec#011loss=8.211058\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[30] avg_epoch_loss=8.342613\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=30 train loss <loss>=8.10959758758545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [30]#011Speed: 920.44 samples/sec#011loss=8.109598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[35] avg_epoch_loss=8.240379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=35 train loss <loss>=7.606529712677002\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [35]#011Speed: 2150.47 samples/sec#011loss=7.606530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch[40] avg_epoch_loss=8.198672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=40 train loss <loss>=7.898377895355225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:06 INFO 140529284842880] Epoch[10] Batch [40]#011Speed: 909.80 samples/sec#011loss=7.898378\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[10] Batch[45] avg_epoch_loss=8.183453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, batch=45 train loss <loss>=8.058656597137452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[10] Batch [45]#011Speed: 1879.61 samples/sec#011loss=8.058657\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] processed a total of 1577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290065.7669048, \"EndTime\": 1618290067.108869, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1341.914176940918, \"count\": 1, \"min\": 1341.914176940918, \"max\": 1341.914176940918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1175.0675093551783 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=10, train loss <loss>=8.179815807342528\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_739d6086-2b73-4964-9594-c42b1e7a4a2a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290067.1089706, \"EndTime\": 1618290067.1254368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.930414199829102, \"count\": 1, \"min\": 15.930414199829102, \"max\": 15.930414199829102}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[0] avg_epoch_loss=8.779456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=8.77945613861084\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[5] avg_epoch_loss=8.703566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=8.70356559753418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [5]#011Speed: 2144.91 samples/sec#011loss=8.703566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[10] avg_epoch_loss=8.747295\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=8.79976921081543\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [10]#011Speed: 822.25 samples/sec#011loss=8.799769\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[15] avg_epoch_loss=8.618349\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=15 train loss <loss>=8.334668350219726\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [15]#011Speed: 2084.36 samples/sec#011loss=8.334668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[20] avg_epoch_loss=8.526276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=20 train loss <loss>=8.23164415359497\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [20]#011Speed: 909.03 samples/sec#011loss=8.231644\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[25] avg_epoch_loss=8.471198\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=25 train loss <loss>=8.239867782592773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [25]#011Speed: 2090.57 samples/sec#011loss=8.239868\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch[30] avg_epoch_loss=8.438599\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=30 train loss <loss>=8.269088172912598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:07 INFO 140529284842880] Epoch[11] Batch [30]#011Speed: 984.12 samples/sec#011loss=8.269088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch[35] avg_epoch_loss=8.335650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=35 train loss <loss>=7.697361278533935\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch [35]#011Speed: 2091.69 samples/sec#011loss=7.697361\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch[40] avg_epoch_loss=8.265438\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=40 train loss <loss>=7.759911060333252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch [40]#011Speed: 953.05 samples/sec#011loss=7.759911\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch[45] avg_epoch_loss=8.237412\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=45 train loss <loss>=8.00760383605957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch [45]#011Speed: 1967.07 samples/sec#011loss=8.007604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch[50] avg_epoch_loss=8.188210\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, batch=50 train loss <loss>=7.735546588897705\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[11] Batch [50]#011Speed: 1603.92 samples/sec#011loss=7.735547\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] processed a total of 1630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290067.1255054, \"EndTime\": 1618290068.4258347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1300.2727031707764, \"count\": 1, \"min\": 1300.2727031707764, \"max\": 1300.2727031707764}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1253.4843566453478 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=11, train loss <loss>=8.18820972068637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch[0] avg_epoch_loss=8.284048\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=8.284048080444336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch[5] avg_epoch_loss=8.253437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=8.253437280654907\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch [5]#011Speed: 2078.74 samples/sec#011loss=8.253437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch[10] avg_epoch_loss=8.265352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=8.279649353027343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch [10]#011Speed: 925.21 samples/sec#011loss=8.279649\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch[15] avg_epoch_loss=8.448171\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=15 train loss <loss>=8.850373649597168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:08 INFO 140529284842880] Epoch[12] Batch [15]#011Speed: 2087.52 samples/sec#011loss=8.850374\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[20] avg_epoch_loss=8.397530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=20 train loss <loss>=8.235478496551513\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [20]#011Speed: 952.39 samples/sec#011loss=8.235478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[25] avg_epoch_loss=8.424527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=25 train loss <loss>=8.537912368774414\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [25]#011Speed: 2104.66 samples/sec#011loss=8.537912\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[30] avg_epoch_loss=8.386253\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=30 train loss <loss>=8.18723258972168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [30]#011Speed: 919.50 samples/sec#011loss=8.187233\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[35] avg_epoch_loss=8.335376\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=35 train loss <loss>=8.019937324523926\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [35]#011Speed: 1766.79 samples/sec#011loss=8.019937\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[40] avg_epoch_loss=8.268722\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=40 train loss <loss>=7.788809967041016\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [40]#011Speed: 875.01 samples/sec#011loss=7.788810\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[45] avg_epoch_loss=8.245346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=45 train loss <loss>=8.053665256500244\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [45]#011Speed: 1903.46 samples/sec#011loss=8.053665\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch[50] avg_epoch_loss=8.219418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, batch=50 train loss <loss>=7.980879878997802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[12] Batch [50]#011Speed: 1384.82 samples/sec#011loss=7.980880\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] processed a total of 1716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290068.4259057, \"EndTime\": 1618290069.8028288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1376.389980316162, \"count\": 1, \"min\": 1376.389980316162, \"max\": 1376.389980316162}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1246.6271720938305 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=12, train loss <loss>=8.210634734895494\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] Epoch[13] Batch[0] avg_epoch_loss=8.412036\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=8.412035942077637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[5] avg_epoch_loss=8.391416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=8.391416072845459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [5]#011Speed: 2116.61 samples/sec#011loss=8.391416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[10] avg_epoch_loss=8.532938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=8.702765274047852\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [10]#011Speed: 880.78 samples/sec#011loss=8.702765\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[15] avg_epoch_loss=8.632068\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=15 train loss <loss>=8.850153732299805\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [15]#011Speed: 2063.85 samples/sec#011loss=8.850154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[20] avg_epoch_loss=8.624648\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=20 train loss <loss>=8.60090160369873\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [20]#011Speed: 810.53 samples/sec#011loss=8.600902\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[25] avg_epoch_loss=8.568416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=25 train loss <loss>=8.332245349884033\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [25]#011Speed: 1859.01 samples/sec#011loss=8.332245\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[30] avg_epoch_loss=8.506507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=30 train loss <loss>=8.184576797485352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [30]#011Speed: 911.97 samples/sec#011loss=8.184577\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[35] avg_epoch_loss=8.415096\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=35 train loss <loss>=7.848351097106933\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [35]#011Speed: 2020.47 samples/sec#011loss=7.848351\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch[40] avg_epoch_loss=8.367432\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=40 train loss <loss>=8.024249649047851\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:10 INFO 140529284842880] Epoch[13] Batch [40]#011Speed: 913.26 samples/sec#011loss=8.024250\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[13] Batch[45] avg_epoch_loss=8.358138\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=45 train loss <loss>=8.281930351257325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[13] Batch [45]#011Speed: 2033.85 samples/sec#011loss=8.281930\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[13] Batch[50] avg_epoch_loss=8.341985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, batch=50 train loss <loss>=8.193369007110595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[13] Batch [50]#011Speed: 1652.01 samples/sec#011loss=8.193369\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] processed a total of 1628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290069.802916, \"EndTime\": 1618290071.1518202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1348.357915878296, \"count\": 1, \"min\": 1348.357915878296, \"max\": 1348.357915878296}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1207.2794887943023 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=13, train loss <loss>=8.341984524446374\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[0] avg_epoch_loss=8.185675\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=8.185674667358398\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[5] avg_epoch_loss=8.127437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=8.127436876296997\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch [5]#011Speed: 1894.80 samples/sec#011loss=8.127437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[10] avg_epoch_loss=8.226098\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=8.344490814208985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch [10]#011Speed: 861.70 samples/sec#011loss=8.344491\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[15] avg_epoch_loss=8.340146\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=15 train loss <loss>=8.591051197052002\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch [15]#011Speed: 1714.37 samples/sec#011loss=8.591051\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[20] avg_epoch_loss=8.407367\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=20 train loss <loss>=8.622477149963379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch [20]#011Speed: 871.18 samples/sec#011loss=8.622477\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch[25] avg_epoch_loss=8.385241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=25 train loss <loss>=8.292311573028565\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:11 INFO 140529284842880] Epoch[14] Batch [25]#011Speed: 2084.20 samples/sec#011loss=8.292312\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch[30] avg_epoch_loss=8.381639\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=30 train loss <loss>=8.362907695770264\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch [30]#011Speed: 866.24 samples/sec#011loss=8.362908\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch[35] avg_epoch_loss=8.320065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=35 train loss <loss>=7.938303089141845\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch [35]#011Speed: 1716.29 samples/sec#011loss=7.938303\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch[40] avg_epoch_loss=8.270666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=40 train loss <loss>=7.9149980545043945\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch [40]#011Speed: 900.33 samples/sec#011loss=7.914998\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch[45] avg_epoch_loss=8.274986\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=45 train loss <loss>=8.310405731201172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch [45]#011Speed: 1971.20 samples/sec#011loss=8.310406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch[50] avg_epoch_loss=8.237195\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, batch=50 train loss <loss>=7.889514446258545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[14] Batch [50]#011Speed: 1824.83 samples/sec#011loss=7.889514\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] processed a total of 1625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290071.1519113, \"EndTime\": 1618290072.53417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1381.7973136901855, \"count\": 1, \"min\": 1381.7973136901855, \"max\": 1381.7973136901855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1175.8976778771628 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=14, train loss <loss>=8.23719451006721\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch[0] avg_epoch_loss=8.203274\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=8.20327377319336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch[5] avg_epoch_loss=8.201727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=8.201726992925009\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch [5]#011Speed: 2096.48 samples/sec#011loss=8.201727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch[10] avg_epoch_loss=8.289599\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=8.395044326782227\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch [10]#011Speed: 895.99 samples/sec#011loss=8.395044\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch[15] avg_epoch_loss=8.315403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=15 train loss <loss>=8.372174072265626\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:12 INFO 140529284842880] Epoch[15] Batch [15]#011Speed: 2091.77 samples/sec#011loss=8.372174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[20] avg_epoch_loss=8.425764\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=20 train loss <loss>=8.778917503356933\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [20]#011Speed: 887.72 samples/sec#011loss=8.778918\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[25] avg_epoch_loss=8.379234\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=25 train loss <loss>=8.183810997009278\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [25]#011Speed: 1866.91 samples/sec#011loss=8.183811\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[30] avg_epoch_loss=8.303753\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=30 train loss <loss>=7.911252021789551\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [30]#011Speed: 835.73 samples/sec#011loss=7.911252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[35] avg_epoch_loss=8.231979\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=35 train loss <loss>=7.786979103088379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [35]#011Speed: 1859.40 samples/sec#011loss=7.786979\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[40] avg_epoch_loss=8.210287\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=40 train loss <loss>=8.054106616973877\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [40]#011Speed: 887.85 samples/sec#011loss=8.054107\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch[45] avg_epoch_loss=8.214557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, batch=45 train loss <loss>=8.249565505981446\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[15] Batch [45]#011Speed: 1972.26 samples/sec#011loss=8.249566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] processed a total of 1592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290072.534257, \"EndTime\": 1618290073.8519914, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1317.232370376587, \"count\": 1, \"min\": 1317.232370376587, \"max\": 1317.232370376587}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1208.4756621988893 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=15, train loss <loss>=8.18918318748474\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] Epoch[16] Batch[0] avg_epoch_loss=8.568013\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=8.568013191223145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[5] avg_epoch_loss=8.091806\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=8.091806173324585\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [5]#011Speed: 2104.99 samples/sec#011loss=8.091806\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[10] avg_epoch_loss=8.205407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=8.341727256774902\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [10]#011Speed: 904.97 samples/sec#011loss=8.341727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[15] avg_epoch_loss=8.351813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=15 train loss <loss>=8.67390842437744\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [15]#011Speed: 2184.23 samples/sec#011loss=8.673908\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[20] avg_epoch_loss=8.451595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=20 train loss <loss>=8.770894050598145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [20]#011Speed: 946.04 samples/sec#011loss=8.770894\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[25] avg_epoch_loss=8.435065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=25 train loss <loss>=8.365638923645019\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [25]#011Speed: 2161.85 samples/sec#011loss=8.365639\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[30] avg_epoch_loss=8.358460\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=30 train loss <loss>=7.960116672515869\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [30]#011Speed: 938.04 samples/sec#011loss=7.960117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch[35] avg_epoch_loss=8.288382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=35 train loss <loss>=7.853896045684815\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:14 INFO 140529284842880] Epoch[16] Batch [35]#011Speed: 1483.34 samples/sec#011loss=7.853896\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch[40] avg_epoch_loss=8.243359\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=40 train loss <loss>=7.919192981719971\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch [40]#011Speed: 841.81 samples/sec#011loss=7.919193\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch[45] avg_epoch_loss=8.229393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=45 train loss <loss>=8.11487102508545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch [45]#011Speed: 1963.20 samples/sec#011loss=8.114871\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch[50] avg_epoch_loss=8.232339\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, batch=50 train loss <loss>=8.259447193145752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[16] Batch [50]#011Speed: 1772.11 samples/sec#011loss=8.259447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] processed a total of 1649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290073.852083, \"EndTime\": 1618290075.197943, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1345.3192710876465, \"count\": 1, \"min\": 1345.3192710876465, \"max\": 1345.3192710876465}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1225.6303439413107 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=16, train loss <loss>=8.200363470957829\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[0] avg_epoch_loss=8.435839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=8.43583869934082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[5] avg_epoch_loss=8.217767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=8.217766761779785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch [5]#011Speed: 2104.08 samples/sec#011loss=8.217767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[10] avg_epoch_loss=8.327243\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=8.458614730834961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch [10]#011Speed: 930.83 samples/sec#011loss=8.458615\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[15] avg_epoch_loss=8.506318\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=15 train loss <loss>=8.90028190612793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch [15]#011Speed: 1736.39 samples/sec#011loss=8.900282\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[20] avg_epoch_loss=8.491081\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=20 train loss <loss>=8.442322731018066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch [20]#011Speed: 824.47 samples/sec#011loss=8.442323\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch[25] avg_epoch_loss=8.401338\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=25 train loss <loss>=8.024418544769286\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:15 INFO 140529284842880] Epoch[17] Batch [25]#011Speed: 1734.34 samples/sec#011loss=8.024419\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch[30] avg_epoch_loss=8.343567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=30 train loss <loss>=8.043160438537598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch [30]#011Speed: 851.30 samples/sec#011loss=8.043160\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch[35] avg_epoch_loss=8.281537\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=35 train loss <loss>=7.896951198577881\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch [35]#011Speed: 2010.17 samples/sec#011loss=7.896951\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch[40] avg_epoch_loss=8.206490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=40 train loss <loss>=7.666149711608886\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch [40]#011Speed: 907.32 samples/sec#011loss=7.666150\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch[45] avg_epoch_loss=8.202666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=45 train loss <loss>=8.171303367614746\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch [45]#011Speed: 2115.15 samples/sec#011loss=8.171303\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch[50] avg_epoch_loss=8.134135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, batch=50 train loss <loss>=7.503658771514893\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[17] Batch [50]#011Speed: 1717.24 samples/sec#011loss=7.503659\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] processed a total of 1616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290075.198019, \"EndTime\": 1618290076.544082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1345.5064296722412, \"count\": 1, \"min\": 1345.5064296722412, \"max\": 1345.5064296722412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1200.9251744071291 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=17, train loss <loss>=8.134135442621568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_afc3e4e8-db65-48f8-ac41-02e26a35d9f2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290076.5441673, \"EndTime\": 1618290076.5542488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.537696838378906, \"count\": 1, \"min\": 9.537696838378906, \"max\": 9.537696838378906}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[18] Batch[0] avg_epoch_loss=8.757832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=8.757831573486328\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[18] Batch[5] avg_epoch_loss=8.218796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=8.218796094258627\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[18] Batch [5]#011Speed: 1989.08 samples/sec#011loss=8.218796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[18] Batch[10] avg_epoch_loss=8.334951\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=8.47433681488037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:16 INFO 140529284842880] Epoch[18] Batch [10]#011Speed: 963.23 samples/sec#011loss=8.474337\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[15] avg_epoch_loss=8.393454\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=15 train loss <loss>=8.522161293029786\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [15]#011Speed: 1789.61 samples/sec#011loss=8.522161\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[20] avg_epoch_loss=8.374710\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=20 train loss <loss>=8.314728355407714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [20]#011Speed: 908.60 samples/sec#011loss=8.314728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[25] avg_epoch_loss=8.306505\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=25 train loss <loss>=8.020042896270752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [25]#011Speed: 1864.88 samples/sec#011loss=8.020043\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[30] avg_epoch_loss=8.271770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=30 train loss <loss>=8.09114637374878\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [30]#011Speed: 825.14 samples/sec#011loss=8.091146\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[35] avg_epoch_loss=8.226174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=35 train loss <loss>=7.943480682373047\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [35]#011Speed: 1719.06 samples/sec#011loss=7.943481\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[40] avg_epoch_loss=8.163084\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=40 train loss <loss>=7.708837604522705\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [40]#011Speed: 851.84 samples/sec#011loss=7.708838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[45] avg_epoch_loss=8.135084\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=45 train loss <loss>=7.90548152923584\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [45]#011Speed: 1727.05 samples/sec#011loss=7.905482\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch[50] avg_epoch_loss=8.094993\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, batch=50 train loss <loss>=7.72615385055542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Epoch[18] Batch [50]#011Speed: 1510.36 samples/sec#011loss=7.726154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] processed a total of 1692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290076.5543392, \"EndTime\": 1618290077.9701595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1415.7605171203613, \"count\": 1, \"min\": 1415.7605171203613, \"max\": 1415.7605171203613}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1195.0004585170082 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=18, train loss <loss>=8.094826698303223\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:17 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_7f596aea-627b-4e4a-87e0-ec90526a39c6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290077.9702601, \"EndTime\": 1618290077.9865966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.780925750732422, \"count\": 1, \"min\": 15.780925750732422, \"max\": 15.780925750732422}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[0] avg_epoch_loss=8.070808\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=8.070808410644531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[5] avg_epoch_loss=8.187960\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=8.187959829966227\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [5]#011Speed: 2074.62 samples/sec#011loss=8.187960\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[10] avg_epoch_loss=8.312150\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=8.461178016662597\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [10]#011Speed: 753.63 samples/sec#011loss=8.461178\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[15] avg_epoch_loss=8.327346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=15 train loss <loss>=8.360777473449707\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [15]#011Speed: 1731.02 samples/sec#011loss=8.360777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[20] avg_epoch_loss=8.419455\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=20 train loss <loss>=8.714202690124512\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [20]#011Speed: 887.92 samples/sec#011loss=8.714203\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[25] avg_epoch_loss=8.376322\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=25 train loss <loss>=8.195162677764893\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [25]#011Speed: 2064.46 samples/sec#011loss=8.195163\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch[30] avg_epoch_loss=8.356704\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=30 train loss <loss>=8.254693698883056\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:18 INFO 140529284842880] Epoch[19] Batch [30]#011Speed: 817.32 samples/sec#011loss=8.254694\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch[35] avg_epoch_loss=8.237828\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=35 train loss <loss>=7.500792407989502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch [35]#011Speed: 1920.33 samples/sec#011loss=7.500792\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch[40] avg_epoch_loss=8.236009\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=40 train loss <loss>=8.222917461395264\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch [40]#011Speed: 917.99 samples/sec#011loss=8.222917\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch[45] avg_epoch_loss=8.222784\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=45 train loss <loss>=8.114337921142578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch [45]#011Speed: 2080.22 samples/sec#011loss=8.114338\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch[50] avg_epoch_loss=8.196544\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, batch=50 train loss <loss>=7.955135440826416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[19] Batch [50]#011Speed: 1371.69 samples/sec#011loss=7.955135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] processed a total of 1650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290077.9866765, \"EndTime\": 1618290079.4185963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1431.858777999878, \"count\": 1, \"min\": 1431.858777999878, \"max\": 1431.858777999878}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1152.2423734262727 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=19, train loss <loss>=8.176398561551022\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch[0] avg_epoch_loss=7.616052\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=7.61605167388916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch[5] avg_epoch_loss=8.177221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=8.177221457163492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch [5]#011Speed: 1672.85 samples/sec#011loss=8.177221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch[10] avg_epoch_loss=8.254147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=8.346456718444824\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch [10]#011Speed: 837.41 samples/sec#011loss=8.346457\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch[15] avg_epoch_loss=8.292732\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=15 train loss <loss>=8.377621078491211\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:19 INFO 140529284842880] Epoch[20] Batch [15]#011Speed: 1560.33 samples/sec#011loss=8.377621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[20] avg_epoch_loss=8.373325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=20 train loss <loss>=8.631220626831055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [20]#011Speed: 802.59 samples/sec#011loss=8.631221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[25] avg_epoch_loss=8.322827\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=25 train loss <loss>=8.110736179351807\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [25]#011Speed: 1855.61 samples/sec#011loss=8.110736\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[30] avg_epoch_loss=8.265382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=30 train loss <loss>=7.96666898727417\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [30]#011Speed: 803.53 samples/sec#011loss=7.966669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[35] avg_epoch_loss=8.198952\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=35 train loss <loss>=7.787087059020996\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [35]#011Speed: 1921.02 samples/sec#011loss=7.787087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[40] avg_epoch_loss=8.182104\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=40 train loss <loss>=8.060794639587403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [40]#011Speed: 782.85 samples/sec#011loss=8.060795\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[45] avg_epoch_loss=8.171592\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=45 train loss <loss>=8.085392951965332\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [45]#011Speed: 2003.87 samples/sec#011loss=8.085393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch[50] avg_epoch_loss=8.149924\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, batch=50 train loss <loss>=7.950584125518799\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[20] Batch [50]#011Speed: 1945.13 samples/sec#011loss=7.950584\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] processed a total of 1612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290079.4186912, \"EndTime\": 1618290080.86074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1441.4825439453125, \"count\": 1, \"min\": 1441.4825439453125, \"max\": 1441.4825439453125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1118.194904959806 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=20, train loss <loss>=8.149924325008019\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] Epoch[21] Batch[0] avg_epoch_loss=8.149730\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=8.14972972869873\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[5] avg_epoch_loss=8.225629\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=8.225628773371378\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [5]#011Speed: 1813.68 samples/sec#011loss=8.225629\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[10] avg_epoch_loss=8.302300\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=8.394306564331055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [10]#011Speed: 848.19 samples/sec#011loss=8.394307\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[15] avg_epoch_loss=8.419875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=15 train loss <loss>=8.6785400390625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [15]#011Speed: 1773.01 samples/sec#011loss=8.678540\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[20] avg_epoch_loss=8.435236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=20 train loss <loss>=8.484389209747315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [20]#011Speed: 854.11 samples/sec#011loss=8.484389\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[25] avg_epoch_loss=8.382174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=25 train loss <loss>=8.159313488006593\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [25]#011Speed: 1676.33 samples/sec#011loss=8.159313\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[30] avg_epoch_loss=8.341710\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=30 train loss <loss>=8.13129539489746\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [30]#011Speed: 877.31 samples/sec#011loss=8.131295\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch[35] avg_epoch_loss=8.279003\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=35 train loss <loss>=7.8902201652526855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:21 INFO 140529284842880] Epoch[21] Batch [35]#011Speed: 2170.20 samples/sec#011loss=7.890220\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch[40] avg_epoch_loss=8.227374\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=40 train loss <loss>=7.855650901794434\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch [40]#011Speed: 928.04 samples/sec#011loss=7.855651\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch[45] avg_epoch_loss=8.214773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=45 train loss <loss>=8.111445713043214\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch [45]#011Speed: 1724.41 samples/sec#011loss=8.111446\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch[50] avg_epoch_loss=8.179994\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, batch=50 train loss <loss>=7.8600266456604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[21] Batch [50]#011Speed: 1394.86 samples/sec#011loss=7.860027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] processed a total of 1693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290080.8608277, \"EndTime\": 1618290082.2921224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1430.798053741455, \"count\": 1, \"min\": 1430.798053741455, \"max\": 1430.798053741455}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1183.1496652637402 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=21, train loss <loss>=8.149627586580673\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch[0] avg_epoch_loss=7.797103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=7.797102928161621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch[5] avg_epoch_loss=8.199490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=8.19948959350586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch [5]#011Speed: 2089.14 samples/sec#011loss=8.199490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch[10] avg_epoch_loss=8.029637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=7.825812816619873\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch [10]#011Speed: 828.51 samples/sec#011loss=7.825813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch[15] avg_epoch_loss=8.243566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=15 train loss <loss>=8.714210510253906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch [15]#011Speed: 1989.27 samples/sec#011loss=8.714211\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch[20] avg_epoch_loss=8.317987\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=20 train loss <loss>=8.556134891510009\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:22 INFO 140529284842880] Epoch[22] Batch [20]#011Speed: 927.49 samples/sec#011loss=8.556135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[25] avg_epoch_loss=8.275432\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=25 train loss <loss>=8.096700668334961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [25]#011Speed: 1737.25 samples/sec#011loss=8.096701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[30] avg_epoch_loss=8.245664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=30 train loss <loss>=8.090869045257568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [30]#011Speed: 842.00 samples/sec#011loss=8.090869\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[35] avg_epoch_loss=8.199961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=35 train loss <loss>=7.916604232788086\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [35]#011Speed: 2052.07 samples/sec#011loss=7.916604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[40] avg_epoch_loss=8.178521\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=40 train loss <loss>=8.024153709411621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [40]#011Speed: 874.47 samples/sec#011loss=8.024154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[45] avg_epoch_loss=8.178418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=45 train loss <loss>=8.177568721771241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [45]#011Speed: 2075.23 samples/sec#011loss=8.177569\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch[50] avg_epoch_loss=8.156376\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, batch=50 train loss <loss>=7.953590011596679\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[22] Batch [50]#011Speed: 1578.31 samples/sec#011loss=7.953590\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] processed a total of 1657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290082.2922122, \"EndTime\": 1618290083.7055004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1412.7697944641113, \"count\": 1, \"min\": 1412.7697944641113, \"max\": 1412.7697944641113}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1172.7684250527664 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=22, train loss <loss>=8.128322949776283\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[23] Batch[0] avg_epoch_loss=8.309395\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=8.309394836425781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[23] Batch[5] avg_epoch_loss=8.341709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=8.341708978017172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:23 INFO 140529284842880] Epoch[23] Batch [5]#011Speed: 2049.91 samples/sec#011loss=8.341709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[10] avg_epoch_loss=8.323904\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=8.30253734588623\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [10]#011Speed: 893.61 samples/sec#011loss=8.302537\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[15] avg_epoch_loss=8.324700\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=15 train loss <loss>=8.326451683044434\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [15]#011Speed: 2056.61 samples/sec#011loss=8.326452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[20] avg_epoch_loss=8.373706\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=20 train loss <loss>=8.530523490905761\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [20]#011Speed: 893.66 samples/sec#011loss=8.530523\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[25] avg_epoch_loss=8.282444\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=25 train loss <loss>=7.899145317077637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [25]#011Speed: 2040.08 samples/sec#011loss=7.899145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[30] avg_epoch_loss=8.261158\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=30 train loss <loss>=8.150472736358642\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [30]#011Speed: 905.23 samples/sec#011loss=8.150473\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[35] avg_epoch_loss=8.199341\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=35 train loss <loss>=7.816077136993409\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [35]#011Speed: 1682.65 samples/sec#011loss=7.816077\u001b[0m\n",
      "\n",
      "2021-04-13 05:01:14 Training - Training image download completed. Training in progress.\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[40] avg_epoch_loss=8.128860\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=40 train loss <loss>=7.621391010284424\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [40]#011Speed: 848.55 samples/sec#011loss=7.621391\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch[45] avg_epoch_loss=8.097052\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, batch=45 train loss <loss>=7.836231803894043\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:24 INFO 140529284842880] Epoch[23] Batch [45]#011Speed: 1620.06 samples/sec#011loss=7.836232\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] processed a total of 1594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290083.7055876, \"EndTime\": 1618290085.0425308, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1336.437463760376, \"count\": 1, \"min\": 1336.437463760376, \"max\": 1336.437463760376}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1192.6132264657751 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=23, train loss <loss>=8.108010482788085\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[0] avg_epoch_loss=8.231328\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=8.231328010559082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[5] avg_epoch_loss=8.271811\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=8.271810611089071\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [5]#011Speed: 2026.09 samples/sec#011loss=8.271811\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[10] avg_epoch_loss=8.316644\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=8.370443630218507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [10]#011Speed: 928.25 samples/sec#011loss=8.370444\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[15] avg_epoch_loss=8.371859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=15 train loss <loss>=8.493333435058593\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [15]#011Speed: 2068.54 samples/sec#011loss=8.493333\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[20] avg_epoch_loss=8.316719\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=20 train loss <loss>=8.140268039703368\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [20]#011Speed: 863.42 samples/sec#011loss=8.140268\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[25] avg_epoch_loss=8.279155\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=25 train loss <loss>=8.12139015197754\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [25]#011Speed: 2074.31 samples/sec#011loss=8.121390\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch[30] avg_epoch_loss=8.272363\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=30 train loss <loss>=8.237042999267578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:25 INFO 140529284842880] Epoch[24] Batch [30]#011Speed: 927.63 samples/sec#011loss=8.237043\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch[35] avg_epoch_loss=8.232013\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=35 train loss <loss>=7.981839561462403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch [35]#011Speed: 1672.49 samples/sec#011loss=7.981840\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch[40] avg_epoch_loss=8.221851\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=40 train loss <loss>=8.148687648773194\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch [40]#011Speed: 828.59 samples/sec#011loss=8.148688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch[45] avg_epoch_loss=8.188061\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=45 train loss <loss>=7.910984992980957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch [45]#011Speed: 2003.29 samples/sec#011loss=7.910985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch[50] avg_epoch_loss=8.127435\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, batch=50 train loss <loss>=7.569673633575439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[24] Batch [50]#011Speed: 1368.13 samples/sec#011loss=7.569674\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] processed a total of 1683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290085.0426173, \"EndTime\": 1618290086.4506953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1407.5496196746826, \"count\": 1, \"min\": 1407.5496196746826, \"max\": 1407.5496196746826}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1195.580751847608 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=24, train loss <loss>=8.1449532148973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch[0] avg_epoch_loss=8.130219\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=8.130219459533691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch[5] avg_epoch_loss=8.152511\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=8.152511358261108\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch [5]#011Speed: 1704.73 samples/sec#011loss=8.152511\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch[10] avg_epoch_loss=8.154419\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=8.156708431243896\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch [10]#011Speed: 839.90 samples/sec#011loss=8.156708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch[15] avg_epoch_loss=8.205218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=15 train loss <loss>=8.316975975036621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:26 INFO 140529284842880] Epoch[25] Batch [15]#011Speed: 2028.06 samples/sec#011loss=8.316976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[20] avg_epoch_loss=8.353924\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=20 train loss <loss>=8.829783058166504\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [20]#011Speed: 908.40 samples/sec#011loss=8.829783\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[25] avg_epoch_loss=8.349946\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=25 train loss <loss>=8.33323631286621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [25]#011Speed: 2018.45 samples/sec#011loss=8.333236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[30] avg_epoch_loss=8.324466\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=30 train loss <loss>=8.191972923278808\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [30]#011Speed: 941.39 samples/sec#011loss=8.191973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[35] avg_epoch_loss=8.276452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=35 train loss <loss>=7.9787623405456545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [35]#011Speed: 2068.13 samples/sec#011loss=7.978762\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[40] avg_epoch_loss=8.238396\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=40 train loss <loss>=7.964390850067138\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [40]#011Speed: 941.19 samples/sec#011loss=7.964391\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[45] avg_epoch_loss=8.194739\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=45 train loss <loss>=7.8367541313171385\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [45]#011Speed: 1936.40 samples/sec#011loss=7.836754\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch[50] avg_epoch_loss=8.172224\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, batch=50 train loss <loss>=7.965090084075928\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[25] Batch [50]#011Speed: 1590.87 samples/sec#011loss=7.965090\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] processed a total of 1673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290086.450791, \"EndTime\": 1618290087.8107476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1359.3904972076416, \"count\": 1, \"min\": 1359.3904972076416, \"max\": 1359.3904972076416}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1230.5991156330638 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=25, train loss <loss>=8.164332524785456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[26] Batch[0] avg_epoch_loss=7.979658\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=7.979658126831055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[26] Batch[5] avg_epoch_loss=8.061490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=8.061489820480347\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:27 INFO 140529284842880] Epoch[26] Batch [5]#011Speed: 2068.98 samples/sec#011loss=8.061490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[10] avg_epoch_loss=8.177532\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=8.31678352355957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [10]#011Speed: 930.96 samples/sec#011loss=8.316784\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[15] avg_epoch_loss=8.369839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=15 train loss <loss>=8.792913436889648\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [15]#011Speed: 1843.52 samples/sec#011loss=8.792913\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[20] avg_epoch_loss=8.456754\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=20 train loss <loss>=8.734880065917968\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [20]#011Speed: 943.78 samples/sec#011loss=8.734880\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[25] avg_epoch_loss=8.406501\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=25 train loss <loss>=8.195441627502442\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [25]#011Speed: 2064.04 samples/sec#011loss=8.195442\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[30] avg_epoch_loss=8.354770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=30 train loss <loss>=8.085766220092774\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [30]#011Speed: 913.74 samples/sec#011loss=8.085766\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[35] avg_epoch_loss=8.282779\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=35 train loss <loss>=7.836438941955566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [35]#011Speed: 2043.55 samples/sec#011loss=7.836439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch[40] avg_epoch_loss=8.236507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=40 train loss <loss>=7.903349781036377\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:28 INFO 140529284842880] Epoch[26] Batch [40]#011Speed: 867.34 samples/sec#011loss=7.903350\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[26] Batch[45] avg_epoch_loss=8.229157\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=45 train loss <loss>=8.16888723373413\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[26] Batch [45]#011Speed: 1829.18 samples/sec#011loss=8.168887\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[26] Batch[50] avg_epoch_loss=8.163220\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, batch=50 train loss <loss>=7.5565940856933596\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[26] Batch [50]#011Speed: 1592.81 samples/sec#011loss=7.556594\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] processed a total of 1630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290087.810827, \"EndTime\": 1618290089.127347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1315.9112930297852, \"count\": 1, \"min\": 1315.9112930297852, \"max\": 1315.9112930297852}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1238.5648137664136 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=26, train loss <loss>=8.163219872642966\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[0] avg_epoch_loss=7.753619\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=7.7536187171936035\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[5] avg_epoch_loss=8.223142\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=8.223142067591349\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch [5]#011Speed: 2080.90 samples/sec#011loss=8.223142\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[10] avg_epoch_loss=8.336395\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=8.472297763824463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch [10]#011Speed: 873.95 samples/sec#011loss=8.472298\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[15] avg_epoch_loss=8.379076\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=15 train loss <loss>=8.472975921630859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch [15]#011Speed: 1866.06 samples/sec#011loss=8.472976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[20] avg_epoch_loss=8.431530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=20 train loss <loss>=8.599381637573241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch [20]#011Speed: 728.93 samples/sec#011loss=8.599382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch[25] avg_epoch_loss=8.377276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=25 train loss <loss>=8.149409294128418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:29 INFO 140529284842880] Epoch[27] Batch [25]#011Speed: 1921.06 samples/sec#011loss=8.149409\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch[30] avg_epoch_loss=8.340508\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=30 train loss <loss>=8.149316024780273\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch [30]#011Speed: 748.37 samples/sec#011loss=8.149316\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch[35] avg_epoch_loss=8.278702\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=35 train loss <loss>=7.895503520965576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch [35]#011Speed: 1912.14 samples/sec#011loss=7.895504\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch[40] avg_epoch_loss=8.253696\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=40 train loss <loss>=8.0736554145813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch [40]#011Speed: 803.89 samples/sec#011loss=8.073655\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch[45] avg_epoch_loss=8.263078\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=45 train loss <loss>=8.340010738372802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch [45]#011Speed: 2053.75 samples/sec#011loss=8.340011\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch[50] avg_epoch_loss=8.223137\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, batch=50 train loss <loss>=7.855677604675293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[27] Batch [50]#011Speed: 1638.22 samples/sec#011loss=7.855678\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] processed a total of 1643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290089.1274385, \"EndTime\": 1618290090.585157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1457.2198390960693, \"count\": 1, \"min\": 1457.2198390960693, \"max\": 1457.2198390960693}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1127.3809951263054 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=27, train loss <loss>=8.189822637117826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[28] Batch[0] avg_epoch_loss=8.314121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=8.31412124633789\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[28] Batch[5] avg_epoch_loss=8.291545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=8.291544993718466\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:30 INFO 140529284842880] Epoch[28] Batch [5]#011Speed: 1743.56 samples/sec#011loss=8.291545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[10] avg_epoch_loss=8.377655\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=8.480986022949219\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [10]#011Speed: 834.42 samples/sec#011loss=8.480986\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[15] avg_epoch_loss=8.353117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=15 train loss <loss>=8.299134445190429\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [15]#011Speed: 1842.24 samples/sec#011loss=8.299134\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[20] avg_epoch_loss=8.436803\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=20 train loss <loss>=8.704599571228027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [20]#011Speed: 777.22 samples/sec#011loss=8.704600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[25] avg_epoch_loss=8.339096\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=25 train loss <loss>=7.928727626800537\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [25]#011Speed: 1739.12 samples/sec#011loss=7.928728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[30] avg_epoch_loss=8.316078\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=30 train loss <loss>=8.196381855010987\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [30]#011Speed: 832.99 samples/sec#011loss=8.196382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[35] avg_epoch_loss=8.248324\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=35 train loss <loss>=7.8282524108886715\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [35]#011Speed: 1773.12 samples/sec#011loss=7.828252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[40] avg_epoch_loss=8.201990\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=40 train loss <loss>=7.868378829956055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [40]#011Speed: 841.94 samples/sec#011loss=7.868379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch[45] avg_epoch_loss=8.170711\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=45 train loss <loss>=7.91422815322876\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:31 INFO 140529284842880] Epoch[28] Batch [45]#011Speed: 2043.85 samples/sec#011loss=7.914228\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[28] Batch[50] avg_epoch_loss=8.110944\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, batch=50 train loss <loss>=7.561083602905273\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[28] Batch [50]#011Speed: 1684.76 samples/sec#011loss=7.561084\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] processed a total of 1639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290090.5852587, \"EndTime\": 1618290092.060873, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1475.1009941101074, \"count\": 1, \"min\": 1475.1009941101074, \"max\": 1475.1009941101074}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1111.014830092796 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=28, train loss <loss>=8.089676251778236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_4a7a4128-8a13-4d92-a894-8450cfada650-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290092.0609615, \"EndTime\": 1618290092.0772638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.770912170410156, \"count\": 1, \"min\": 15.770912170410156, \"max\": 15.770912170410156}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[0] avg_epoch_loss=8.272713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=8.272712707519531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[5] avg_epoch_loss=8.291497\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=8.291496753692627\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch [5]#011Speed: 1941.15 samples/sec#011loss=8.291497\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[10] avg_epoch_loss=8.286617\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=8.280761337280273\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch [10]#011Speed: 898.36 samples/sec#011loss=8.280761\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[15] avg_epoch_loss=8.373970\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=15 train loss <loss>=8.566145420074463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch [15]#011Speed: 1899.52 samples/sec#011loss=8.566145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[20] avg_epoch_loss=8.440475\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=20 train loss <loss>=8.653290176391602\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch [20]#011Speed: 767.53 samples/sec#011loss=8.653290\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch[25] avg_epoch_loss=8.400992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=25 train loss <loss>=8.235164165496826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:32 INFO 140529284842880] Epoch[29] Batch [25]#011Speed: 1741.26 samples/sec#011loss=8.235164\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch[30] avg_epoch_loss=8.379076\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=30 train loss <loss>=8.265116691589355\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch [30]#011Speed: 911.26 samples/sec#011loss=8.265117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch[35] avg_epoch_loss=8.300496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=35 train loss <loss>=7.8132976531982425\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch [35]#011Speed: 1737.83 samples/sec#011loss=7.813298\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch[40] avg_epoch_loss=8.252077\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=40 train loss <loss>=7.903462886810303\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch [40]#011Speed: 864.85 samples/sec#011loss=7.903463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch[45] avg_epoch_loss=8.274930\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=45 train loss <loss>=8.46232204437256\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch [45]#011Speed: 1725.90 samples/sec#011loss=8.462322\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch[50] avg_epoch_loss=8.236453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, batch=50 train loss <loss>=7.882467555999756\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[29] Batch [50]#011Speed: 1486.73 samples/sec#011loss=7.882468\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] processed a total of 1667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290092.077333, \"EndTime\": 1618290093.532371, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1454.9860954284668, \"count\": 1, \"min\": 1454.9860954284668, \"max\": 1454.9860954284668}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1145.6301884220302 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=29, train loss <loss>=8.183399344390294\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[30] Batch[0] avg_epoch_loss=7.921986\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=7.921986103057861\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[30] Batch[5] avg_epoch_loss=8.215290\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=8.215289990107218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[30] Batch [5]#011Speed: 1810.00 samples/sec#011loss=8.215290\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[30] Batch[10] avg_epoch_loss=8.185841\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=8.150501918792724\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:33 INFO 140529284842880] Epoch[30] Batch [10]#011Speed: 815.13 samples/sec#011loss=8.150502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[15] avg_epoch_loss=8.343760\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=15 train loss <loss>=8.691182613372803\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [15]#011Speed: 1840.64 samples/sec#011loss=8.691183\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[20] avg_epoch_loss=8.408776\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=20 train loss <loss>=8.616826438903809\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [20]#011Speed: 911.10 samples/sec#011loss=8.616826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[25] avg_epoch_loss=8.352369\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=25 train loss <loss>=8.115457725524902\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [25]#011Speed: 2079.91 samples/sec#011loss=8.115458\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[30] avg_epoch_loss=8.251660\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=30 train loss <loss>=7.7279764175415036\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [30]#011Speed: 880.16 samples/sec#011loss=7.727976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[35] avg_epoch_loss=8.210955\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=35 train loss <loss>=7.958583354949951\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [35]#011Speed: 2048.18 samples/sec#011loss=7.958583\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[40] avg_epoch_loss=8.119701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=40 train loss <loss>=7.462674999237061\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [40]#011Speed: 862.52 samples/sec#011loss=7.462675\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[45] avg_epoch_loss=8.135578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=45 train loss <loss>=8.265769577026367\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [45]#011Speed: 1625.00 samples/sec#011loss=8.265770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch[50] avg_epoch_loss=8.136246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, batch=50 train loss <loss>=8.142384910583496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] Epoch[30] Batch [50]#011Speed: 1435.47 samples/sec#011loss=8.142385\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] processed a total of 1674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290093.53245, \"EndTime\": 1618290094.9643888, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1431.3697814941406, \"count\": 1, \"min\": 1431.3697814941406, \"max\": 1431.3697814941406}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1169.3825033934297 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=30, train loss <loss>=8.154895809461486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:34 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[0] avg_epoch_loss=8.185942\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=8.185941696166992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[5] avg_epoch_loss=8.304650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=8.304649670918783\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [5]#011Speed: 1881.34 samples/sec#011loss=8.304650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[10] avg_epoch_loss=8.435679\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=8.592914199829101\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [10]#011Speed: 943.06 samples/sec#011loss=8.592914\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[15] avg_epoch_loss=8.420974\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=15 train loss <loss>=8.388624000549317\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [15]#011Speed: 1953.59 samples/sec#011loss=8.388624\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[20] avg_epoch_loss=8.476471\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=20 train loss <loss>=8.654060554504394\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [20]#011Speed: 920.70 samples/sec#011loss=8.654061\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[25] avg_epoch_loss=8.353780\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=25 train loss <loss>=7.8384778022766115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [25]#011Speed: 2044.62 samples/sec#011loss=7.838478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[30] avg_epoch_loss=8.299575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=30 train loss <loss>=8.01771125793457\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [30]#011Speed: 887.30 samples/sec#011loss=8.017711\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch[35] avg_epoch_loss=8.240568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=35 train loss <loss>=7.874722862243653\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:35 INFO 140529284842880] Epoch[31] Batch [35]#011Speed: 1910.05 samples/sec#011loss=7.874723\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[31] Batch[40] avg_epoch_loss=8.202552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=40 train loss <loss>=7.928838443756104\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[31] Batch [40]#011Speed: 963.53 samples/sec#011loss=7.928838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[31] Batch[45] avg_epoch_loss=8.175709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, batch=45 train loss <loss>=7.955595397949219\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[31] Batch [45]#011Speed: 2022.62 samples/sec#011loss=7.955595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] processed a total of 1574 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290094.9645, \"EndTime\": 1618290096.254722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1289.6347045898438, \"count\": 1, \"min\": 1289.6347045898438, \"max\": 1289.6347045898438}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1220.3869429742251 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=31, train loss <loss>=8.15182029724121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[0] avg_epoch_loss=8.232708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=8.232707977294922\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[5] avg_epoch_loss=8.536435\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=8.536434650421143\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch [5]#011Speed: 1896.54 samples/sec#011loss=8.536435\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[10] avg_epoch_loss=8.523390\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=8.507736206054688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch [10]#011Speed: 877.35 samples/sec#011loss=8.507736\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[15] avg_epoch_loss=8.510218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=15 train loss <loss>=8.481240367889404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch [15]#011Speed: 2017.23 samples/sec#011loss=8.481240\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[20] avg_epoch_loss=8.444095\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=20 train loss <loss>=8.232501411437989\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch [20]#011Speed: 926.01 samples/sec#011loss=8.232501\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch[25] avg_epoch_loss=8.380288\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=25 train loss <loss>=8.112296295166015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:36 INFO 140529284842880] Epoch[32] Batch [25]#011Speed: 2049.70 samples/sec#011loss=8.112296\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch[30] avg_epoch_loss=8.327415\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=30 train loss <loss>=8.05247983932495\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch [30]#011Speed: 876.59 samples/sec#011loss=8.052480\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch[35] avg_epoch_loss=8.244438\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=35 train loss <loss>=7.729975795745849\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch [35]#011Speed: 1844.24 samples/sec#011loss=7.729976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch[40] avg_epoch_loss=8.178421\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=40 train loss <loss>=7.703101634979248\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch [40]#011Speed: 913.48 samples/sec#011loss=7.703102\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch[45] avg_epoch_loss=8.158334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=45 train loss <loss>=7.993621158599853\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch [45]#011Speed: 2047.42 samples/sec#011loss=7.993621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch[50] avg_epoch_loss=8.082000\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, batch=50 train loss <loss>=7.379726886749268\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[32] Batch [50]#011Speed: 1777.86 samples/sec#011loss=7.379727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] processed a total of 1617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290096.2547987, \"EndTime\": 1618290097.5855827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1330.186367034912, \"count\": 1, \"min\": 1330.186367034912, \"max\": 1330.186367034912}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1215.5045382107958 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=32, train loss <loss>=8.082000115338493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_8654430f-3bce-4dc6-b57e-35ca78e7435f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290097.5856705, \"EndTime\": 1618290097.5964186, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.224580764770508, \"count\": 1, \"min\": 10.224580764770508, \"max\": 10.224580764770508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[33] Batch[0] avg_epoch_loss=8.202101\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=8.20210075378418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[33] Batch[5] avg_epoch_loss=8.287370\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=8.287370363871256\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[33] Batch [5]#011Speed: 1790.19 samples/sec#011loss=8.287370\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[33] Batch[10] avg_epoch_loss=8.379089\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=8.489151573181152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:37 INFO 140529284842880] Epoch[33] Batch [10]#011Speed: 815.50 samples/sec#011loss=8.489152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[15] avg_epoch_loss=8.467478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=15 train loss <loss>=8.661933326721192\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [15]#011Speed: 2030.33 samples/sec#011loss=8.661933\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[20] avg_epoch_loss=8.481053\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=20 train loss <loss>=8.524493598937989\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [20]#011Speed: 885.20 samples/sec#011loss=8.524494\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[25] avg_epoch_loss=8.473865\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=25 train loss <loss>=8.44367504119873\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [25]#011Speed: 1996.44 samples/sec#011loss=8.443675\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[30] avg_epoch_loss=8.408578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=30 train loss <loss>=8.06908655166626\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [30]#011Speed: 904.42 samples/sec#011loss=8.069087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[35] avg_epoch_loss=8.331646\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=35 train loss <loss>=7.854666137695313\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [35]#011Speed: 2084.17 samples/sec#011loss=7.854666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[40] avg_epoch_loss=8.271251\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=40 train loss <loss>=7.8364053726196286\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [40]#011Speed: 924.68 samples/sec#011loss=7.836405\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[45] avg_epoch_loss=8.239199\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=45 train loss <loss>=7.976379299163819\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [45]#011Speed: 1940.17 samples/sec#011loss=7.976379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch[50] avg_epoch_loss=8.216825\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, batch=50 train loss <loss>=8.010976028442382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] Epoch[33] Batch [50]#011Speed: 1430.08 samples/sec#011loss=8.010976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] processed a total of 1677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290097.5964868, \"EndTime\": 1618290098.9987118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1402.165174484253, \"count\": 1, \"min\": 1402.165174484253, \"max\": 1402.165174484253}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1195.9002902934797 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=33, train loss <loss>=8.180699042554172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:38 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[0] avg_epoch_loss=8.726840\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=8.726840019226074\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[5] avg_epoch_loss=8.119287\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=8.119287490844727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [5]#011Speed: 2082.06 samples/sec#011loss=8.119287\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[10] avg_epoch_loss=8.226411\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=8.354960250854493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [10]#011Speed: 906.03 samples/sec#011loss=8.354960\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[15] avg_epoch_loss=8.454678\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=15 train loss <loss>=8.956862831115723\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [15]#011Speed: 2012.76 samples/sec#011loss=8.956863\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[20] avg_epoch_loss=8.496448\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=20 train loss <loss>=8.630112838745116\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [20]#011Speed: 925.42 samples/sec#011loss=8.630113\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[25] avg_epoch_loss=8.424139\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=25 train loss <loss>=8.120442199707032\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [25]#011Speed: 2123.90 samples/sec#011loss=8.120442\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[30] avg_epoch_loss=8.392247\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=30 train loss <loss>=8.226407623291015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [30]#011Speed: 848.35 samples/sec#011loss=8.226408\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch[35] avg_epoch_loss=8.306568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=35 train loss <loss>=7.775361061096191\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:39 INFO 140529284842880] Epoch[34] Batch [35]#011Speed: 1970.41 samples/sec#011loss=7.775361\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch[40] avg_epoch_loss=8.264207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=40 train loss <loss>=7.959207344055176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch [40]#011Speed: 889.85 samples/sec#011loss=7.959207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch[45] avg_epoch_loss=8.211952\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=45 train loss <loss>=7.783459568023682\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch [45]#011Speed: 1779.66 samples/sec#011loss=7.783460\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch[50] avg_epoch_loss=8.159253\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, batch=50 train loss <loss>=7.674425888061523\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[34] Batch [50]#011Speed: 1365.09 samples/sec#011loss=7.674426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] processed a total of 1737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290098.9988015, \"EndTime\": 1618290100.4183977, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1419.0559387207031, \"count\": 1, \"min\": 1419.0559387207031, \"max\": 1419.0559387207031}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1223.9413597253993 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=34, train loss <loss>=8.146238864551892\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch[0] avg_epoch_loss=8.843530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=8.84352970123291\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch[5] avg_epoch_loss=8.402601\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=8.40260108311971\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch [5]#011Speed: 2047.49 samples/sec#011loss=8.402601\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch[10] avg_epoch_loss=8.367104\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=8.324507141113282\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch [10]#011Speed: 851.45 samples/sec#011loss=8.324507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch[15] avg_epoch_loss=8.514436\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=15 train loss <loss>=8.838567352294922\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:40 INFO 140529284842880] Epoch[35] Batch [15]#011Speed: 1656.04 samples/sec#011loss=8.838567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[20] avg_epoch_loss=8.543351\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=20 train loss <loss>=8.635878562927246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [20]#011Speed: 906.69 samples/sec#011loss=8.635879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[25] avg_epoch_loss=8.448599\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=25 train loss <loss>=8.050638961791993\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [25]#011Speed: 2050.40 samples/sec#011loss=8.050639\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[30] avg_epoch_loss=8.365874\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=30 train loss <loss>=7.9357084274292\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [30]#011Speed: 890.41 samples/sec#011loss=7.935708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[35] avg_epoch_loss=8.309895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=35 train loss <loss>=7.962825012207031\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [35]#011Speed: 1847.33 samples/sec#011loss=7.962825\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[40] avg_epoch_loss=8.261284\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=40 train loss <loss>=7.911284446716309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [40]#011Speed: 797.87 samples/sec#011loss=7.911284\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[45] avg_epoch_loss=8.245637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=45 train loss <loss>=8.117330932617188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [45]#011Speed: 2078.29 samples/sec#011loss=8.117331\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch[50] avg_epoch_loss=8.220931\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, batch=50 train loss <loss>=7.993633651733399\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[35] Batch [50]#011Speed: 1633.79 samples/sec#011loss=7.993634\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] processed a total of 1661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290100.4184885, \"EndTime\": 1618290101.8239222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1404.8991203308105, \"count\": 1, \"min\": 1404.8991203308105, \"max\": 1404.8991203308105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1182.1789465503414 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=35, train loss <loss>=8.197987492267902\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] Epoch[36] Batch[0] avg_epoch_loss=8.428645\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=8.428645133972168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[5] avg_epoch_loss=8.114976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=8.114975770314535\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [5]#011Speed: 2155.26 samples/sec#011loss=8.114976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[10] avg_epoch_loss=8.221228\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=8.348730850219727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [10]#011Speed: 939.18 samples/sec#011loss=8.348731\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[15] avg_epoch_loss=8.315518\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=15 train loss <loss>=8.522955894470215\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [15]#011Speed: 1960.74 samples/sec#011loss=8.522956\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[20] avg_epoch_loss=8.388703\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=20 train loss <loss>=8.622896862030029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [20]#011Speed: 912.39 samples/sec#011loss=8.622897\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[25] avg_epoch_loss=8.301041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=25 train loss <loss>=7.932858657836914\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [25]#011Speed: 1980.10 samples/sec#011loss=7.932859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[30] avg_epoch_loss=8.248398\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=30 train loss <loss>=7.974656105041504\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [30]#011Speed: 915.39 samples/sec#011loss=7.974656\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[35] avg_epoch_loss=8.184229\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=35 train loss <loss>=7.786379623413086\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [35]#011Speed: 2105.40 samples/sec#011loss=7.786380\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch[40] avg_epoch_loss=8.152159\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=40 train loss <loss>=7.921254444122314\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:42 INFO 140529284842880] Epoch[36] Batch [40]#011Speed: 879.24 samples/sec#011loss=7.921254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[36] Batch[45] avg_epoch_loss=8.156169\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=45 train loss <loss>=8.189055252075196\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[36] Batch [45]#011Speed: 2052.57 samples/sec#011loss=8.189055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[36] Batch[50] avg_epoch_loss=8.130551\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, batch=50 train loss <loss>=7.894858074188233\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[36] Batch [50]#011Speed: 1583.21 samples/sec#011loss=7.894858\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] processed a total of 1638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290101.8240108, \"EndTime\": 1618290103.1611266, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1336.5767002105713, \"count\": 1, \"min\": 1336.5767002105713, \"max\": 1336.5767002105713}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1225.395830464676 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=36, train loss <loss>=8.110598334899315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[0] avg_epoch_loss=8.265315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=8.265315055847168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[5] avg_epoch_loss=8.385434\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=8.38543430964152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch [5]#011Speed: 2046.23 samples/sec#011loss=8.385434\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[10] avg_epoch_loss=8.367699\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=8.346417236328126\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch [10]#011Speed: 814.33 samples/sec#011loss=8.346417\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[15] avg_epoch_loss=8.408515\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=15 train loss <loss>=8.498308181762695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch [15]#011Speed: 2159.81 samples/sec#011loss=8.498308\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[20] avg_epoch_loss=8.385480\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=20 train loss <loss>=8.311771202087403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch [20]#011Speed: 915.28 samples/sec#011loss=8.311771\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch[25] avg_epoch_loss=8.309522\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=25 train loss <loss>=7.9904961585998535\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:43 INFO 140529284842880] Epoch[37] Batch [25]#011Speed: 1624.39 samples/sec#011loss=7.990496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch[30] avg_epoch_loss=8.250729\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=30 train loss <loss>=7.945005321502686\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch [30]#011Speed: 816.87 samples/sec#011loss=7.945005\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch[35] avg_epoch_loss=8.178112\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=35 train loss <loss>=7.727883529663086\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch [35]#011Speed: 2034.49 samples/sec#011loss=7.727884\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch[40] avg_epoch_loss=8.142496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=40 train loss <loss>=7.886066722869873\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch [40]#011Speed: 853.12 samples/sec#011loss=7.886067\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch[45] avg_epoch_loss=8.139799\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, batch=45 train loss <loss>=8.117683219909669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[37] Batch [45]#011Speed: 2028.38 samples/sec#011loss=8.117683\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] processed a total of 1594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290103.1612134, \"EndTime\": 1618290104.5182295, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1356.5037250518799, \"count\": 1, \"min\": 1356.5037250518799, \"max\": 1356.5037250518799}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1174.9597245776488 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=37, train loss <loss>=8.107045803070069\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch[0] avg_epoch_loss=8.087141\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=8.087141036987305\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch[5] avg_epoch_loss=8.523325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=8.523325284322103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch [5]#011Speed: 2091.95 samples/sec#011loss=8.523325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch[10] avg_epoch_loss=8.541935\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=8.564266681671143\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch [10]#011Speed: 860.42 samples/sec#011loss=8.564267\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch[15] avg_epoch_loss=8.505516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=15 train loss <loss>=8.425394630432129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:44 INFO 140529284842880] Epoch[38] Batch [15]#011Speed: 1888.32 samples/sec#011loss=8.425395\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[20] avg_epoch_loss=8.415583\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=20 train loss <loss>=8.127798080444336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [20]#011Speed: 853.13 samples/sec#011loss=8.127798\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[25] avg_epoch_loss=8.381074\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=25 train loss <loss>=8.236134910583496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [25]#011Speed: 1504.03 samples/sec#011loss=8.236135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[30] avg_epoch_loss=8.322240\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=30 train loss <loss>=8.016301727294922\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [30]#011Speed: 777.15 samples/sec#011loss=8.016302\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[35] avg_epoch_loss=8.250709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=35 train loss <loss>=7.807216358184815\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [35]#011Speed: 2077.24 samples/sec#011loss=7.807216\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[40] avg_epoch_loss=8.171317\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=40 train loss <loss>=7.59969539642334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [40]#011Speed: 784.04 samples/sec#011loss=7.599695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[45] avg_epoch_loss=8.154976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=45 train loss <loss>=8.020977401733399\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [45]#011Speed: 1831.28 samples/sec#011loss=8.020977\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch[50] avg_epoch_loss=8.060832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, batch=50 train loss <loss>=7.19470853805542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Epoch[38] Batch [50]#011Speed: 1531.53 samples/sec#011loss=7.194709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] processed a total of 1618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290104.5183196, \"EndTime\": 1618290105.9576373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1438.7946128845215, \"count\": 1, \"min\": 1438.7946128845215, \"max\": 1438.7946128845215}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1124.452944985449 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=38, train loss <loss>=8.060831771177405\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:45 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_295ec9b2-91be-415e-8919-3723d45ceb36-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290105.9577255, \"EndTime\": 1618290105.9743621, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 16.116857528686523, \"count\": 1, \"min\": 16.116857528686523, \"max\": 16.116857528686523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[0] avg_epoch_loss=8.502427\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=8.502427101135254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[5] avg_epoch_loss=8.439584\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=8.439583937327066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [5]#011Speed: 1865.73 samples/sec#011loss=8.439584\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[10] avg_epoch_loss=8.333382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=8.205940628051758\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [10]#011Speed: 868.16 samples/sec#011loss=8.205941\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[15] avg_epoch_loss=8.398676\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=15 train loss <loss>=8.542321968078614\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [15]#011Speed: 1925.17 samples/sec#011loss=8.542322\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[20] avg_epoch_loss=8.428242\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=20 train loss <loss>=8.522854232788086\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [20]#011Speed: 842.11 samples/sec#011loss=8.522854\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[25] avg_epoch_loss=8.365970\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=25 train loss <loss>=8.104428386688232\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [25]#011Speed: 2153.53 samples/sec#011loss=8.104428\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch[30] avg_epoch_loss=8.320976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=30 train loss <loss>=8.08700351715088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:46 INFO 140529284842880] Epoch[39] Batch [30]#011Speed: 830.96 samples/sec#011loss=8.087004\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch[35] avg_epoch_loss=8.246790\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=35 train loss <loss>=7.786839294433594\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch [35]#011Speed: 1814.58 samples/sec#011loss=7.786839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch[40] avg_epoch_loss=8.189861\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=40 train loss <loss>=7.779971981048584\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch [40]#011Speed: 881.49 samples/sec#011loss=7.779972\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch[45] avg_epoch_loss=8.130846\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=45 train loss <loss>=7.646918678283692\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch [45]#011Speed: 1851.55 samples/sec#011loss=7.646919\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch[50] avg_epoch_loss=8.117833\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, batch=50 train loss <loss>=7.998113059997559\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[39] Batch [50]#011Speed: 1778.72 samples/sec#011loss=7.998113\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] processed a total of 1619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290105.9744291, \"EndTime\": 1618290107.3716676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1397.1824645996094, \"count\": 1, \"min\": 1397.1824645996094, \"max\": 1397.1824645996094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1158.661154934956 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=39, train loss <loss>=8.117832595226812\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch[0] avg_epoch_loss=8.052127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=8.05212688446045\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch[5] avg_epoch_loss=8.013802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=8.01380205154419\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch [5]#011Speed: 1759.65 samples/sec#011loss=8.013802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch[10] avg_epoch_loss=8.231180\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=8.492034339904786\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch [10]#011Speed: 878.73 samples/sec#011loss=8.492034\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch[15] avg_epoch_loss=8.347876\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=15 train loss <loss>=8.604607200622558\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:47 INFO 140529284842880] Epoch[40] Batch [15]#011Speed: 1966.20 samples/sec#011loss=8.604607\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[20] avg_epoch_loss=8.438475\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=20 train loss <loss>=8.728391647338867\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [20]#011Speed: 793.49 samples/sec#011loss=8.728392\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[25] avg_epoch_loss=8.364478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=25 train loss <loss>=8.053691005706787\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [25]#011Speed: 1954.63 samples/sec#011loss=8.053691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[30] avg_epoch_loss=8.309882\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=30 train loss <loss>=8.02598295211792\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [30]#011Speed: 927.47 samples/sec#011loss=8.025983\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[35] avg_epoch_loss=8.257414\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=35 train loss <loss>=7.932110118865967\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [35]#011Speed: 2052.96 samples/sec#011loss=7.932110\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[40] avg_epoch_loss=8.222160\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=40 train loss <loss>=7.9683349609375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [40]#011Speed: 904.53 samples/sec#011loss=7.968335\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[45] avg_epoch_loss=8.202834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=45 train loss <loss>=8.044360446929932\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [45]#011Speed: 2083.41 samples/sec#011loss=8.044360\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch[50] avg_epoch_loss=8.169530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, batch=50 train loss <loss>=7.863134765625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[40] Batch [50]#011Speed: 1867.52 samples/sec#011loss=7.863135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] processed a total of 1610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290107.3717537, \"EndTime\": 1618290108.7292414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1357.0005893707275, \"count\": 1, \"min\": 1357.0005893707275, \"max\": 1357.0005893707275}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1186.3282345521336 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=40, train loss <loss>=8.169530382343368\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[41] Batch[0] avg_epoch_loss=8.143828\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=8.143828392028809\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[41] Batch[5] avg_epoch_loss=8.228306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=8.228305896123251\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:48 INFO 140529284842880] Epoch[41] Batch [5]#011Speed: 2085.15 samples/sec#011loss=8.228306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[10] avg_epoch_loss=8.285204\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=8.353481101989747\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [10]#011Speed: 897.86 samples/sec#011loss=8.353481\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[15] avg_epoch_loss=8.312963\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=15 train loss <loss>=8.374031829833985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [15]#011Speed: 1878.89 samples/sec#011loss=8.374032\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[20] avg_epoch_loss=8.348773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=20 train loss <loss>=8.463365364074708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [20]#011Speed: 801.10 samples/sec#011loss=8.463365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[25] avg_epoch_loss=8.271327\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=25 train loss <loss>=7.946054744720459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [25]#011Speed: 2119.25 samples/sec#011loss=7.946055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[30] avg_epoch_loss=8.203092\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=30 train loss <loss>=7.848270034790039\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [30]#011Speed: 904.98 samples/sec#011loss=7.848270\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[35] avg_epoch_loss=8.101174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=35 train loss <loss>=7.469281196594238\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [35]#011Speed: 2099.49 samples/sec#011loss=7.469281\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[40] avg_epoch_loss=8.097546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=40 train loss <loss>=8.071423053741455\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [40]#011Speed: 857.38 samples/sec#011loss=8.071423\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch[45] avg_epoch_loss=8.089014\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, batch=45 train loss <loss>=8.01905336380005\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:49 INFO 140529284842880] Epoch[41] Batch [45]#011Speed: 2049.11 samples/sec#011loss=8.019053\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] processed a total of 1580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290108.7293277, \"EndTime\": 1618290110.0519216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1322.042465209961, \"count\": 1, \"min\": 1322.042465209961, \"max\": 1322.042465209961}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1194.9928808938193 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=41, train loss <loss>=8.088629512786865\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[0] avg_epoch_loss=7.967197\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=7.967197418212891\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[5] avg_epoch_loss=8.482595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=8.482595125834147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch [5]#011Speed: 1692.16 samples/sec#011loss=8.482595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[10] avg_epoch_loss=8.386042\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=8.270178699493409\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch [10]#011Speed: 871.55 samples/sec#011loss=8.270179\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[15] avg_epoch_loss=8.374614\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=15 train loss <loss>=8.349473476409912\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch [15]#011Speed: 1713.23 samples/sec#011loss=8.349473\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[20] avg_epoch_loss=8.434173\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=20 train loss <loss>=8.62476043701172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch [20]#011Speed: 901.20 samples/sec#011loss=8.624760\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch[25] avg_epoch_loss=8.347241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=25 train loss <loss>=7.982124614715576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:50 INFO 140529284842880] Epoch[42] Batch [25]#011Speed: 2047.91 samples/sec#011loss=7.982125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch[30] avg_epoch_loss=8.302880\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=30 train loss <loss>=8.072205448150635\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch [30]#011Speed: 847.77 samples/sec#011loss=8.072205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch[35] avg_epoch_loss=8.224598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=35 train loss <loss>=7.73924560546875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch [35]#011Speed: 1879.87 samples/sec#011loss=7.739246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch[40] avg_epoch_loss=8.198280\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=40 train loss <loss>=8.008795738220215\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch [40]#011Speed: 912.64 samples/sec#011loss=8.008796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch[45] avg_epoch_loss=8.191543\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=45 train loss <loss>=8.136299133300781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch [45]#011Speed: 2061.13 samples/sec#011loss=8.136299\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch[50] avg_epoch_loss=8.110357\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, batch=50 train loss <loss>=7.3634490966796875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[42] Batch [50]#011Speed: 1827.06 samples/sec#011loss=7.363449\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] processed a total of 1618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290110.052022, \"EndTime\": 1618290111.446298, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1393.7444686889648, \"count\": 1, \"min\": 1393.7444686889648, \"max\": 1393.7444686889648}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1160.8144933158587 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=42, train loss <loss>=8.110357490240359\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch[0] avg_epoch_loss=8.247254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=8.247254371643066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch[5] avg_epoch_loss=8.088558\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=8.088558197021484\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch [5]#011Speed: 1966.68 samples/sec#011loss=8.088558\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch[10] avg_epoch_loss=8.126546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=8.172132301330567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch [10]#011Speed: 936.63 samples/sec#011loss=8.172132\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch[15] avg_epoch_loss=8.330301\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=15 train loss <loss>=8.778560447692872\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:51 INFO 140529284842880] Epoch[43] Batch [15]#011Speed: 2058.64 samples/sec#011loss=8.778560\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[20] avg_epoch_loss=8.334783\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=20 train loss <loss>=8.349127864837646\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [20]#011Speed: 879.91 samples/sec#011loss=8.349128\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[25] avg_epoch_loss=8.307624\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=25 train loss <loss>=8.193556690216065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [25]#011Speed: 2061.48 samples/sec#011loss=8.193557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[30] avg_epoch_loss=8.292264\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=30 train loss <loss>=8.212387466430664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [30]#011Speed: 906.02 samples/sec#011loss=8.212387\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[35] avg_epoch_loss=8.203199\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=35 train loss <loss>=7.651001262664795\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [35]#011Speed: 1850.45 samples/sec#011loss=7.651001\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[40] avg_epoch_loss=8.170605\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=40 train loss <loss>=7.935921287536621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [40]#011Speed: 853.72 samples/sec#011loss=7.935921\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[45] avg_epoch_loss=8.172364\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=45 train loss <loss>=8.186788272857665\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [45]#011Speed: 2082.34 samples/sec#011loss=8.186788\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch[50] avg_epoch_loss=8.128545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, batch=50 train loss <loss>=7.725416278839111\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[43] Batch [50]#011Speed: 1585.35 samples/sec#011loss=7.725416\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] processed a total of 1644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290111.4463642, \"EndTime\": 1618290112.8028474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1355.9625148773193, \"count\": 1, \"min\": 1355.9625148773193, \"max\": 1355.9625148773193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1212.308642889103 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=43, train loss <loss>=8.11585835310129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] Epoch[44] Batch[0] avg_epoch_loss=7.637818\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=7.637818336486816\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[5] avg_epoch_loss=8.234977\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=8.234976847966513\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [5]#011Speed: 2092.27 samples/sec#011loss=8.234977\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[10] avg_epoch_loss=8.365408\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=8.521925354003907\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [10]#011Speed: 775.92 samples/sec#011loss=8.521925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[15] avg_epoch_loss=8.450585\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=15 train loss <loss>=8.637975311279297\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [15]#011Speed: 2057.88 samples/sec#011loss=8.637975\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[20] avg_epoch_loss=8.501628\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=20 train loss <loss>=8.664966773986816\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [20]#011Speed: 917.61 samples/sec#011loss=8.664967\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[25] avg_epoch_loss=8.364560\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=25 train loss <loss>=7.7888744354248045\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [25]#011Speed: 1983.04 samples/sec#011loss=7.788874\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[30] avg_epoch_loss=8.344620\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=30 train loss <loss>=8.240927791595459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [30]#011Speed: 916.19 samples/sec#011loss=8.240928\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[35] avg_epoch_loss=8.271247\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=35 train loss <loss>=7.81633996963501\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [35]#011Speed: 1894.50 samples/sec#011loss=7.816340\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch[40] avg_epoch_loss=8.220665\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=40 train loss <loss>=7.856471252441406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:53 INFO 140529284842880] Epoch[44] Batch [40]#011Speed: 932.49 samples/sec#011loss=7.856471\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[44] Batch[45] avg_epoch_loss=8.211889\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=45 train loss <loss>=8.139926528930664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[44] Batch [45]#011Speed: 1981.86 samples/sec#011loss=8.139927\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[44] Batch[50] avg_epoch_loss=8.166842\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, batch=50 train loss <loss>=7.752403926849365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[44] Batch [50]#011Speed: 1298.78 samples/sec#011loss=7.752404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] processed a total of 1662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290112.8029363, \"EndTime\": 1618290114.1987624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1395.2827453613281, \"count\": 1, \"min\": 1395.2827453613281, \"max\": 1395.2827453613281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1191.047740438978 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=44, train loss <loss>=8.14017732326801\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[0] avg_epoch_loss=8.336084\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=8.336084365844727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[5] avg_epoch_loss=8.360117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=8.360117435455322\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch [5]#011Speed: 2081.81 samples/sec#011loss=8.360117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[10] avg_epoch_loss=8.295087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=8.217051124572754\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch [10]#011Speed: 900.06 samples/sec#011loss=8.217051\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[15] avg_epoch_loss=8.403651\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=15 train loss <loss>=8.642489624023437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch [15]#011Speed: 1732.05 samples/sec#011loss=8.642490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[20] avg_epoch_loss=8.401642\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=20 train loss <loss>=8.395212936401368\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch [20]#011Speed: 881.34 samples/sec#011loss=8.395213\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch[25] avg_epoch_loss=8.329913\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=25 train loss <loss>=8.028652667999268\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:54 INFO 140529284842880] Epoch[45] Batch [25]#011Speed: 2058.48 samples/sec#011loss=8.028653\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch[30] avg_epoch_loss=8.282689\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=30 train loss <loss>=8.037126541137695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch [30]#011Speed: 860.46 samples/sec#011loss=8.037127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch[35] avg_epoch_loss=8.214226\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=35 train loss <loss>=7.789755535125733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch [35]#011Speed: 1940.74 samples/sec#011loss=7.789756\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch[40] avg_epoch_loss=8.172106\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=40 train loss <loss>=7.868835926055908\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch [40]#011Speed: 835.49 samples/sec#011loss=7.868836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch[45] avg_epoch_loss=8.153313\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=45 train loss <loss>=7.999212074279785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch [45]#011Speed: 1946.57 samples/sec#011loss=7.999212\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch[50] avg_epoch_loss=8.127878\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, batch=50 train loss <loss>=7.893876743316651\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[45] Batch [50]#011Speed: 1618.44 samples/sec#011loss=7.893877\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] processed a total of 1639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290114.1988535, \"EndTime\": 1618290115.5859833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1386.594533920288, \"count\": 1, \"min\": 1386.594533920288, \"max\": 1386.594533920288}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1181.9298087015725 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=45, train loss <loss>=8.104611305090097\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[46] Batch[0] avg_epoch_loss=8.050027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=8.050026893615723\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[46] Batch[5] avg_epoch_loss=8.115833\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=8.115832726160685\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[46] Batch [5]#011Speed: 1733.40 samples/sec#011loss=8.115833\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[46] Batch[10] avg_epoch_loss=8.171186\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=8.237610816955566\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:55 INFO 140529284842880] Epoch[46] Batch [10]#011Speed: 812.42 samples/sec#011loss=8.237611\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[15] avg_epoch_loss=8.280319\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=15 train loss <loss>=8.52041130065918\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [15]#011Speed: 2007.82 samples/sec#011loss=8.520411\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[20] avg_epoch_loss=8.346188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=20 train loss <loss>=8.556968116760254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [20]#011Speed: 891.17 samples/sec#011loss=8.556968\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[25] avg_epoch_loss=8.298781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=25 train loss <loss>=8.099671936035156\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [25]#011Speed: 1863.48 samples/sec#011loss=8.099672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[30] avg_epoch_loss=8.282678\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=30 train loss <loss>=8.198939800262451\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [30]#011Speed: 828.85 samples/sec#011loss=8.198940\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[35] avg_epoch_loss=8.206271\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=35 train loss <loss>=7.7325481414794925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [35]#011Speed: 1808.13 samples/sec#011loss=7.732548\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[40] avg_epoch_loss=8.161012\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=40 train loss <loss>=7.835148715972901\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [40]#011Speed: 912.49 samples/sec#011loss=7.835149\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[45] avg_epoch_loss=8.189453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=45 train loss <loss>=8.422671699523926\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [45]#011Speed: 2065.10 samples/sec#011loss=8.422672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch[50] avg_epoch_loss=8.128696\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, batch=50 train loss <loss>=7.56973295211792\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] Epoch[46] Batch [50]#011Speed: 1785.46 samples/sec#011loss=7.569733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] processed a total of 1633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290115.586068, \"EndTime\": 1618290116.9808824, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1394.3243026733398, \"count\": 1, \"min\": 1394.3243026733398, \"max\": 1394.3243026733398}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1171.0708739716601 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=46, train loss <loss>=8.108004056490385\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:56 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[0] avg_epoch_loss=8.316072\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=8.316072463989258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[5] avg_epoch_loss=8.271293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=8.271292686462402\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [5]#011Speed: 2076.12 samples/sec#011loss=8.271293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[10] avg_epoch_loss=8.346448\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=8.436633491516114\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [10]#011Speed: 878.92 samples/sec#011loss=8.436633\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[15] avg_epoch_loss=8.402948\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=15 train loss <loss>=8.527248668670655\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [15]#011Speed: 1966.41 samples/sec#011loss=8.527249\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[20] avg_epoch_loss=8.417263\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=20 train loss <loss>=8.463072586059571\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [20]#011Speed: 947.32 samples/sec#011loss=8.463073\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[25] avg_epoch_loss=8.350486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=25 train loss <loss>=8.070022010803223\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [25]#011Speed: 1961.84 samples/sec#011loss=8.070022\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[30] avg_epoch_loss=8.286750\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=30 train loss <loss>=7.955321025848389\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [30]#011Speed: 910.30 samples/sec#011loss=7.955321\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch[35] avg_epoch_loss=8.236234\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=35 train loss <loss>=7.923038673400879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:57 INFO 140529284842880] Epoch[47] Batch [35]#011Speed: 2046.53 samples/sec#011loss=7.923039\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch[40] avg_epoch_loss=8.175480\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=40 train loss <loss>=7.738049030303955\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch [40]#011Speed: 831.66 samples/sec#011loss=7.738049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch[45] avg_epoch_loss=8.134822\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=45 train loss <loss>=7.801426696777344\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch [45]#011Speed: 1942.04 samples/sec#011loss=7.801427\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch[50] avg_epoch_loss=8.109172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, batch=50 train loss <loss>=7.873192310333252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[47] Batch [50]#011Speed: 1258.79 samples/sec#011loss=7.873192\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] processed a total of 1720 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290116.9809718, \"EndTime\": 1618290118.4072623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1425.7798194885254, \"count\": 1, \"min\": 1425.7798194885254, \"max\": 1425.7798194885254}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1206.2391744165438 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=47, train loss <loss>=8.105863173802694\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch[0] avg_epoch_loss=8.455322\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=8.455322265625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch[5] avg_epoch_loss=8.312464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=8.312463839848837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch [5]#011Speed: 2068.61 samples/sec#011loss=8.312464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch[10] avg_epoch_loss=8.216113\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=8.100492763519288\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch [10]#011Speed: 884.59 samples/sec#011loss=8.100493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch[15] avg_epoch_loss=8.383993\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=15 train loss <loss>=8.753327178955079\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:58 INFO 140529284842880] Epoch[48] Batch [15]#011Speed: 2061.45 samples/sec#011loss=8.753327\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[20] avg_epoch_loss=8.424817\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=20 train loss <loss>=8.555453109741212\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [20]#011Speed: 926.91 samples/sec#011loss=8.555453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[25] avg_epoch_loss=8.315154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=25 train loss <loss>=7.854571533203125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [25]#011Speed: 2052.94 samples/sec#011loss=7.854572\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[30] avg_epoch_loss=8.239284\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=30 train loss <loss>=7.84476146697998\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [30]#011Speed: 900.69 samples/sec#011loss=7.844761\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[35] avg_epoch_loss=8.161466\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=35 train loss <loss>=7.678995990753174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [35]#011Speed: 2151.22 samples/sec#011loss=7.678996\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[40] avg_epoch_loss=8.127398\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=40 train loss <loss>=7.882107448577881\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [40]#011Speed: 789.09 samples/sec#011loss=7.882107\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[45] avg_epoch_loss=8.125464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=45 train loss <loss>=8.109600353240968\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [45]#011Speed: 1984.03 samples/sec#011loss=8.109600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch[50] avg_epoch_loss=8.101269\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, batch=50 train loss <loss>=7.878678703308106\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[48] Batch [50]#011Speed: 1971.11 samples/sec#011loss=7.878679\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] processed a total of 1612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290118.4073539, \"EndTime\": 1618290119.7295053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1321.634292602539, \"count\": 1, \"min\": 1321.634292602539, \"max\": 1321.634292602539}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1219.5871625860548 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=48, train loss <loss>=8.101269132950726\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[49] Batch[0] avg_epoch_loss=8.267441\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=8.267440795898438\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[49] Batch[5] avg_epoch_loss=8.281439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=8.281438827514648\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:01:59 INFO 140529284842880] Epoch[49] Batch [5]#011Speed: 2045.63 samples/sec#011loss=8.281439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[10] avg_epoch_loss=8.223606\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=8.154206371307373\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [10]#011Speed: 916.85 samples/sec#011loss=8.154206\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[15] avg_epoch_loss=8.352491\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=15 train loss <loss>=8.63603916168213\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [15]#011Speed: 1723.88 samples/sec#011loss=8.636039\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[20] avg_epoch_loss=8.346733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=20 train loss <loss>=8.3283052444458\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [20]#011Speed: 714.05 samples/sec#011loss=8.328305\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[25] avg_epoch_loss=8.278651\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=25 train loss <loss>=7.9927081108093265\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [25]#011Speed: 2129.30 samples/sec#011loss=7.992708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[30] avg_epoch_loss=8.231894\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=30 train loss <loss>=7.9887566566467285\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [30]#011Speed: 883.52 samples/sec#011loss=7.988757\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[35] avg_epoch_loss=8.186126\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=35 train loss <loss>=7.902365016937256\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [35]#011Speed: 1779.14 samples/sec#011loss=7.902365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch[40] avg_epoch_loss=8.144363\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=40 train loss <loss>=7.843667697906494\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:00 INFO 140529284842880] Epoch[49] Batch [40]#011Speed: 891.43 samples/sec#011loss=7.843668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[49] Batch[45] avg_epoch_loss=8.073838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=45 train loss <loss>=7.495530891418457\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[49] Batch [45]#011Speed: 2007.36 samples/sec#011loss=7.495531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[49] Batch[50] avg_epoch_loss=8.079701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, batch=50 train loss <loss>=8.133647060394287\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[49] Batch [50]#011Speed: 1316.14 samples/sec#011loss=8.133647\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] processed a total of 1703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290119.729592, \"EndTime\": 1618290121.2214441, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1491.353988647461, \"count\": 1, \"min\": 1491.353988647461, \"max\": 1491.353988647461}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1141.8098350802538 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=49, train loss <loss>=8.057098609429818\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_2358a1bb-ea34-49a7-b467-cf470bc6fa75-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290121.2215393, \"EndTime\": 1618290121.2340016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.690616607666016, \"count\": 1, \"min\": 11.690616607666016, \"max\": 11.690616607666016}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[0] avg_epoch_loss=8.474287\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=8.474287033081055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[5] avg_epoch_loss=8.137020\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=8.137019713719686\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch [5]#011Speed: 1974.20 samples/sec#011loss=8.137020\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[10] avg_epoch_loss=8.272342\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=8.434728813171386\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch [10]#011Speed: 896.71 samples/sec#011loss=8.434729\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[15] avg_epoch_loss=8.314958\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=15 train loss <loss>=8.408712673187257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch [15]#011Speed: 1781.07 samples/sec#011loss=8.408713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[20] avg_epoch_loss=8.316332\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=20 train loss <loss>=8.320728302001953\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch [20]#011Speed: 870.87 samples/sec#011loss=8.320728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch[25] avg_epoch_loss=8.235606\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=25 train loss <loss>=7.896556568145752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:01 INFO 140529284842880] Epoch[50] Batch [25]#011Speed: 2063.76 samples/sec#011loss=7.896557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch[30] avg_epoch_loss=8.182395\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=30 train loss <loss>=7.905699253082275\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch [30]#011Speed: 927.73 samples/sec#011loss=7.905699\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch[35] avg_epoch_loss=8.105980\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=35 train loss <loss>=7.632205867767334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch [35]#011Speed: 1903.57 samples/sec#011loss=7.632206\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch[40] avg_epoch_loss=8.050925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=40 train loss <loss>=7.654528999328614\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch [40]#011Speed: 801.64 samples/sec#011loss=7.654529\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch[45] avg_epoch_loss=8.083898\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=45 train loss <loss>=8.354275894165038\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch [45]#011Speed: 2050.05 samples/sec#011loss=8.354276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch[50] avg_epoch_loss=8.036836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, batch=50 train loss <loss>=7.603865146636963\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[50] Batch [50]#011Speed: 1946.02 samples/sec#011loss=7.603865\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] processed a total of 1607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290121.2340765, \"EndTime\": 1618290122.5840087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1349.8659133911133, \"count\": 1, \"min\": 1349.8659133911133, \"max\": 1349.8659133911133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1190.377812451709 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=50, train loss <loss>=8.036835801367666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_d5a973de-6bae-4f95-a8d1-49b6c6fe96ff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290122.584098, \"EndTime\": 1618290122.600375, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.75922966003418, \"count\": 1, \"min\": 15.75922966003418, \"max\": 15.75922966003418}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[51] Batch[0] avg_epoch_loss=8.251866\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=8.251866340637207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[51] Batch[5] avg_epoch_loss=8.153024\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=8.153024037679037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[51] Batch [5]#011Speed: 2056.12 samples/sec#011loss=8.153024\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[51] Batch[10] avg_epoch_loss=8.215344\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=8.290128326416015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:02 INFO 140529284842880] Epoch[51] Batch [10]#011Speed: 904.47 samples/sec#011loss=8.290128\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[15] avg_epoch_loss=8.302701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=15 train loss <loss>=8.494884490966797\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [15]#011Speed: 2063.10 samples/sec#011loss=8.494884\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[20] avg_epoch_loss=8.268006\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=20 train loss <loss>=8.156983661651612\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [20]#011Speed: 796.01 samples/sec#011loss=8.156984\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[25] avg_epoch_loss=8.199225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=25 train loss <loss>=7.910346126556396\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [25]#011Speed: 2046.03 samples/sec#011loss=7.910346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[30] avg_epoch_loss=8.156732\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=30 train loss <loss>=7.9357682228088375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [30]#011Speed: 954.53 samples/sec#011loss=7.935768\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[35] avg_epoch_loss=8.138116\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=35 train loss <loss>=8.02269458770752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [35]#011Speed: 2042.57 samples/sec#011loss=8.022695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[40] avg_epoch_loss=8.101098\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=40 train loss <loss>=7.834572601318359\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [40]#011Speed: 950.16 samples/sec#011loss=7.834573\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[45] avg_epoch_loss=8.115091\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=45 train loss <loss>=8.229826164245605\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [45]#011Speed: 2143.17 samples/sec#011loss=8.229826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch[50] avg_epoch_loss=8.085004\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, batch=50 train loss <loss>=7.808207321166992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] Epoch[51] Batch [50]#011Speed: 1569.14 samples/sec#011loss=7.808207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] processed a total of 1654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290122.6004484, \"EndTime\": 1618290123.9483576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1347.851276397705, \"count\": 1, \"min\": 1347.851276397705, \"max\": 1347.851276397705}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1227.0391652981073 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=51, train loss <loss>=8.065392879339365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:03 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[0] avg_epoch_loss=8.023638\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=8.023637771606445\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[5] avg_epoch_loss=8.259781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=8.259780724843344\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [5]#011Speed: 2010.37 samples/sec#011loss=8.259781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[10] avg_epoch_loss=8.391311\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=8.549147796630859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [10]#011Speed: 908.99 samples/sec#011loss=8.549148\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[15] avg_epoch_loss=8.368693\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=15 train loss <loss>=8.318932437896729\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [15]#011Speed: 2162.28 samples/sec#011loss=8.318932\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[20] avg_epoch_loss=8.375490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=20 train loss <loss>=8.397242641448974\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [20]#011Speed: 808.08 samples/sec#011loss=8.397243\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[25] avg_epoch_loss=8.313773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=25 train loss <loss>=8.054560661315918\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [25]#011Speed: 2006.53 samples/sec#011loss=8.054561\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[30] avg_epoch_loss=8.253447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=30 train loss <loss>=7.939750862121582\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [30]#011Speed: 855.79 samples/sec#011loss=7.939751\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch[35] avg_epoch_loss=8.190668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=35 train loss <loss>=7.801436519622802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:04 INFO 140529284842880] Epoch[52] Batch [35]#011Speed: 1917.43 samples/sec#011loss=7.801437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch[40] avg_epoch_loss=8.131109\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=40 train loss <loss>=7.702289485931397\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch [40]#011Speed: 900.73 samples/sec#011loss=7.702289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch[45] avg_epoch_loss=8.132425\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=45 train loss <loss>=8.143209838867188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch [45]#011Speed: 1794.88 samples/sec#011loss=8.143210\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch[50] avg_epoch_loss=8.101429\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, batch=50 train loss <loss>=7.816272163391114\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[52] Batch [50]#011Speed: 1497.54 samples/sec#011loss=7.816272\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] processed a total of 1668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290123.9484348, \"EndTime\": 1618290125.33641, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1387.4189853668213, \"count\": 1, \"min\": 1387.4189853668213, \"max\": 1387.4189853668213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1202.1117195469774 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=52, train loss <loss>=8.040292038107818\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch[0] avg_epoch_loss=7.633741\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=7.6337409019470215\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch[5] avg_epoch_loss=8.222256\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=8.22225578625997\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch [5]#011Speed: 1944.70 samples/sec#011loss=8.222256\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch[10] avg_epoch_loss=8.254080\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=8.292269897460937\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch [10]#011Speed: 920.99 samples/sec#011loss=8.292270\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch[15] avg_epoch_loss=8.387966\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=15 train loss <loss>=8.682515144348145\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch [15]#011Speed: 2004.97 samples/sec#011loss=8.682515\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch[20] avg_epoch_loss=8.347666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=20 train loss <loss>=8.218706417083741\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:05 INFO 140529284842880] Epoch[53] Batch [20]#011Speed: 858.29 samples/sec#011loss=8.218706\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[25] avg_epoch_loss=8.294123\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=25 train loss <loss>=8.069242763519288\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [25]#011Speed: 1964.63 samples/sec#011loss=8.069243\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[30] avg_epoch_loss=8.220070\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=30 train loss <loss>=7.834993076324463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [30]#011Speed: 864.26 samples/sec#011loss=7.834993\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[35] avg_epoch_loss=8.160881\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=35 train loss <loss>=7.793905925750733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [35]#011Speed: 2054.97 samples/sec#011loss=7.793906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[40] avg_epoch_loss=8.130270\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=40 train loss <loss>=7.909874725341797\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [40]#011Speed: 892.07 samples/sec#011loss=7.909875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[45] avg_epoch_loss=8.114027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=45 train loss <loss>=7.980836582183838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [45]#011Speed: 1814.04 samples/sec#011loss=7.980837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch[50] avg_epoch_loss=8.096868\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, batch=50 train loss <loss>=7.938999080657959\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[53] Batch [50]#011Speed: 1510.73 samples/sec#011loss=7.938999\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] processed a total of 1668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290125.3365104, \"EndTime\": 1618290126.7445064, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1407.4881076812744, \"count\": 1, \"min\": 1407.4881076812744, \"max\": 1407.4881076812744}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1184.9837570092761 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=53, train loss <loss>=8.05652170361213\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[54] Batch[0] avg_epoch_loss=7.672276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=7.672275543212891\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[54] Batch[5] avg_epoch_loss=8.156837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=8.156837383906046\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:06 INFO 140529284842880] Epoch[54] Batch [5]#011Speed: 1957.38 samples/sec#011loss=8.156837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[10] avg_epoch_loss=8.364054\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=8.612714767456055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [10]#011Speed: 844.63 samples/sec#011loss=8.612715\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[15] avg_epoch_loss=8.495992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=15 train loss <loss>=8.786253929138184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [15]#011Speed: 2107.70 samples/sec#011loss=8.786254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[20] avg_epoch_loss=8.555115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=20 train loss <loss>=8.744308471679688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [20]#011Speed: 828.43 samples/sec#011loss=8.744308\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[25] avg_epoch_loss=8.450281\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=25 train loss <loss>=8.00998134613037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [25]#011Speed: 1720.35 samples/sec#011loss=8.009981\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[30] avg_epoch_loss=8.356610\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=30 train loss <loss>=7.869520759582519\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [30]#011Speed: 912.09 samples/sec#011loss=7.869521\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[35] avg_epoch_loss=8.256531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=35 train loss <loss>=7.636040687561035\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [35]#011Speed: 1902.22 samples/sec#011loss=7.636041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch[40] avg_epoch_loss=8.200019\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=40 train loss <loss>=7.793133735656738\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:07 INFO 140529284842880] Epoch[54] Batch [40]#011Speed: 882.27 samples/sec#011loss=7.793134\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[54] Batch[45] avg_epoch_loss=8.199973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, batch=45 train loss <loss>=8.199591064453125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[54] Batch [45]#011Speed: 1717.09 samples/sec#011loss=8.199591\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] processed a total of 1588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290126.744594, \"EndTime\": 1618290128.0975852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1352.5102138519287, \"count\": 1, \"min\": 1352.5102138519287, \"max\": 1352.5102138519287}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1174.0028244225832 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=54, train loss <loss>=8.16857162475586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[0] avg_epoch_loss=8.545447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=8.54544734954834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[5] avg_epoch_loss=8.551456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=8.551455974578857\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [5]#011Speed: 1749.88 samples/sec#011loss=8.551456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[10] avg_epoch_loss=8.439945\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=8.306132316589355\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [10]#011Speed: 918.47 samples/sec#011loss=8.306132\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[15] avg_epoch_loss=8.415348\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=15 train loss <loss>=8.361233139038086\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [15]#011Speed: 1976.48 samples/sec#011loss=8.361233\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[20] avg_epoch_loss=8.408391\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=20 train loss <loss>=8.386130332946777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [20]#011Speed: 939.62 samples/sec#011loss=8.386130\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[25] avg_epoch_loss=8.319534\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=25 train loss <loss>=7.946335506439209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [25]#011Speed: 2060.37 samples/sec#011loss=7.946336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch[30] avg_epoch_loss=8.241258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=30 train loss <loss>=7.834218311309814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:08 INFO 140529284842880] Epoch[55] Batch [30]#011Speed: 869.61 samples/sec#011loss=7.834218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch[35] avg_epoch_loss=8.154503\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=35 train loss <loss>=7.616625499725342\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch [35]#011Speed: 2072.96 samples/sec#011loss=7.616625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch[40] avg_epoch_loss=8.138716\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=40 train loss <loss>=8.025048160552979\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch [40]#011Speed: 958.93 samples/sec#011loss=8.025048\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch[45] avg_epoch_loss=8.134244\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=45 train loss <loss>=8.097574806213379\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch [45]#011Speed: 1797.37 samples/sec#011loss=8.097575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch[50] avg_epoch_loss=8.098081\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, batch=50 train loss <loss>=7.7653850555419925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[55] Batch [50]#011Speed: 1792.68 samples/sec#011loss=7.765385\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] processed a total of 1612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290128.097679, \"EndTime\": 1618290129.416199, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1317.9640769958496, \"count\": 1, \"min\": 1317.9640769958496, \"max\": 1317.9640769958496}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1222.9835437052836 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=55, train loss <loss>=8.098081401750154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch[0] avg_epoch_loss=7.788630\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=7.78863000869751\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch[5] avg_epoch_loss=7.891579\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=7.8915791511535645\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch [5]#011Speed: 1718.91 samples/sec#011loss=7.891579\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch[10] avg_epoch_loss=8.155983\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=8.473267745971679\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch [10]#011Speed: 825.99 samples/sec#011loss=8.473268\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch[15] avg_epoch_loss=8.162510\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=15 train loss <loss>=8.176868915557861\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:09 INFO 140529284842880] Epoch[56] Batch [15]#011Speed: 1933.78 samples/sec#011loss=8.176869\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[20] avg_epoch_loss=8.250130\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=20 train loss <loss>=8.530513572692872\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [20]#011Speed: 877.81 samples/sec#011loss=8.530514\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[25] avg_epoch_loss=8.170083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=25 train loss <loss>=7.833884239196777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [25]#011Speed: 2064.65 samples/sec#011loss=7.833884\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[30] avg_epoch_loss=8.134787\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=30 train loss <loss>=7.9512526512146\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [30]#011Speed: 894.53 samples/sec#011loss=7.951253\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[35] avg_epoch_loss=8.088602\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=35 train loss <loss>=7.802252674102784\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [35]#011Speed: 2004.96 samples/sec#011loss=7.802253\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[40] avg_epoch_loss=8.050463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=40 train loss <loss>=7.775860118865967\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [40]#011Speed: 754.96 samples/sec#011loss=7.775860\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[45] avg_epoch_loss=8.058557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=45 train loss <loss>=8.12493143081665\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [45]#011Speed: 2082.06 samples/sec#011loss=8.124931\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch[50] avg_epoch_loss=8.029505\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, batch=50 train loss <loss>=7.762222003936768\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[56] Batch [50]#011Speed: 1784.73 samples/sec#011loss=7.762222\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] processed a total of 1652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290129.4162858, \"EndTime\": 1618290130.8254647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1408.6668491363525, \"count\": 1, \"min\": 1408.6668491363525, \"max\": 1408.6668491363525}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1172.6443646734506 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=56, train loss <loss>=8.007974477914663\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_6449689e-31d1-41fa-a662-f594cfa05d85-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290130.8255403, \"EndTime\": 1618290130.8357358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.627819061279297, \"count\": 1, \"min\": 9.627819061279297, \"max\": 9.627819061279297}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] Epoch[57] Batch[0] avg_epoch_loss=8.135106\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=8.135106086730957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[5] avg_epoch_loss=8.277852\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=8.277852137883505\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [5]#011Speed: 2144.77 samples/sec#011loss=8.277852\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[10] avg_epoch_loss=8.221802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=8.154542922973633\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [10]#011Speed: 928.67 samples/sec#011loss=8.154543\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[15] avg_epoch_loss=8.324859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=15 train loss <loss>=8.551581954956054\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [15]#011Speed: 2150.62 samples/sec#011loss=8.551582\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[20] avg_epoch_loss=8.374879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=20 train loss <loss>=8.534944534301758\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [20]#011Speed: 818.14 samples/sec#011loss=8.534945\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[25] avg_epoch_loss=8.290439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=25 train loss <loss>=7.935793113708496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [25]#011Speed: 2014.04 samples/sec#011loss=7.935793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[30] avg_epoch_loss=8.215570\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=30 train loss <loss>=7.826251983642578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [30]#011Speed: 828.59 samples/sec#011loss=7.826252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch[35] avg_epoch_loss=8.136649\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=35 train loss <loss>=7.6473386764526365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:11 INFO 140529284842880] Epoch[57] Batch [35]#011Speed: 1939.10 samples/sec#011loss=7.647339\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch[40] avg_epoch_loss=8.116444\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=40 train loss <loss>=7.970962238311768\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch [40]#011Speed: 811.24 samples/sec#011loss=7.970962\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch[45] avg_epoch_loss=8.104906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=45 train loss <loss>=8.010294914245605\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch [45]#011Speed: 1848.31 samples/sec#011loss=8.010295\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch[50] avg_epoch_loss=8.003839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, batch=50 train loss <loss>=7.074027442932129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[57] Batch [50]#011Speed: 1919.66 samples/sec#011loss=7.074027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] processed a total of 1604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290130.8358057, \"EndTime\": 1618290132.1913507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1355.4885387420654, \"count\": 1, \"min\": 1355.4885387420654, \"max\": 1355.4885387420654}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1183.2239744982082 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=57, train loss <loss>=8.003839249704399\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_68df0c18-085c-47bc-a0c2-1bc0921669d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290132.1914473, \"EndTime\": 1618290132.2017603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.772300720214844, \"count\": 1, \"min\": 9.772300720214844, \"max\": 9.772300720214844}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[0] avg_epoch_loss=8.451407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=8.451407432556152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[5] avg_epoch_loss=8.080187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=8.080186525980631\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch [5]#011Speed: 1936.65 samples/sec#011loss=8.080187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[10] avg_epoch_loss=8.239058\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=8.429704093933106\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch [10]#011Speed: 815.59 samples/sec#011loss=8.429704\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[15] avg_epoch_loss=8.252970\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=15 train loss <loss>=8.283574676513672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch [15]#011Speed: 1976.77 samples/sec#011loss=8.283575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[20] avg_epoch_loss=8.317448\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=20 train loss <loss>=8.523780250549317\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch [20]#011Speed: 959.02 samples/sec#011loss=8.523780\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch[25] avg_epoch_loss=8.233357\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=25 train loss <loss>=7.880172252655029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:12 INFO 140529284842880] Epoch[58] Batch [25]#011Speed: 2064.38 samples/sec#011loss=7.880172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch[30] avg_epoch_loss=8.168141\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=30 train loss <loss>=7.8290197372436525\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch [30]#011Speed: 883.26 samples/sec#011loss=7.829020\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch[35] avg_epoch_loss=8.119083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=35 train loss <loss>=7.814924907684326\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch [35]#011Speed: 2061.25 samples/sec#011loss=7.814925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch[40] avg_epoch_loss=8.074757\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=40 train loss <loss>=7.755611705780029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch [40]#011Speed: 942.05 samples/sec#011loss=7.755612\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch[45] avg_epoch_loss=8.021942\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, batch=45 train loss <loss>=7.588851547241211\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[58] Batch [45]#011Speed: 2012.46 samples/sec#011loss=7.588852\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] processed a total of 1600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290132.20183, \"EndTime\": 1618290133.4877582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1285.8741283416748, \"count\": 1, \"min\": 1285.8741283416748, \"max\": 1285.8741283416748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1244.1859417401138 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=58, train loss <loss>=8.008195714950562\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch[0] avg_epoch_loss=8.018170\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=8.018170356750488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch[5] avg_epoch_loss=8.179352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=8.179352362950643\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch [5]#011Speed: 1942.39 samples/sec#011loss=8.179352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch[10] avg_epoch_loss=8.290935\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=8.424834442138671\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch [10]#011Speed: 902.67 samples/sec#011loss=8.424834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch[15] avg_epoch_loss=8.423882\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=15 train loss <loss>=8.716363906860352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:13 INFO 140529284842880] Epoch[59] Batch [15]#011Speed: 2045.33 samples/sec#011loss=8.716364\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[20] avg_epoch_loss=8.376189\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=20 train loss <loss>=8.223574161529541\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [20]#011Speed: 925.60 samples/sec#011loss=8.223574\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[25] avg_epoch_loss=8.260264\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=25 train loss <loss>=7.773377132415772\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [25]#011Speed: 1837.49 samples/sec#011loss=7.773377\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[30] avg_epoch_loss=8.192525\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=30 train loss <loss>=7.840280246734619\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [30]#011Speed: 885.71 samples/sec#011loss=7.840280\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[35] avg_epoch_loss=8.117592\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=35 train loss <loss>=7.65300931930542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [35]#011Speed: 2027.67 samples/sec#011loss=7.653009\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[40] avg_epoch_loss=8.051075\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=40 train loss <loss>=7.572152042388916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [40]#011Speed: 802.78 samples/sec#011loss=7.572152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch[45] avg_epoch_loss=8.027886\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, batch=45 train loss <loss>=7.837741661071777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[59] Batch [45]#011Speed: 1707.74 samples/sec#011loss=7.837742\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] processed a total of 1590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290133.487837, \"EndTime\": 1618290134.8127708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1324.4023323059082, \"count\": 1, \"min\": 1324.4023323059082, \"max\": 1324.4023323059082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1200.414462408114 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=59, train loss <loss>=8.018995037078858\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] Epoch[60] Batch[0] avg_epoch_loss=7.851533\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=7.85153341293335\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[5] avg_epoch_loss=8.164105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=8.16410493850708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [5]#011Speed: 1892.86 samples/sec#011loss=8.164105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[10] avg_epoch_loss=8.325152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=8.518408966064452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [10]#011Speed: 859.21 samples/sec#011loss=8.518409\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[15] avg_epoch_loss=8.283370\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=15 train loss <loss>=8.191449737548828\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [15]#011Speed: 2072.35 samples/sec#011loss=8.191450\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[20] avg_epoch_loss=8.309133\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=20 train loss <loss>=8.391575241088868\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [20]#011Speed: 875.59 samples/sec#011loss=8.391575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[25] avg_epoch_loss=8.185896\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=25 train loss <loss>=7.668299007415771\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [25]#011Speed: 1851.16 samples/sec#011loss=7.668299\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[30] avg_epoch_loss=8.131508\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=30 train loss <loss>=7.848690795898437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [30]#011Speed: 836.53 samples/sec#011loss=7.848691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[35] avg_epoch_loss=8.040531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=35 train loss <loss>=7.476473808288574\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [35]#011Speed: 1707.63 samples/sec#011loss=7.476474\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch[40] avg_epoch_loss=7.975943\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=40 train loss <loss>=7.51091194152832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:15 INFO 140529284842880] Epoch[60] Batch [40]#011Speed: 1044.08 samples/sec#011loss=7.510912\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[60] Batch[45] avg_epoch_loss=7.989550\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, batch=45 train loss <loss>=8.101125240325928\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[60] Batch [45]#011Speed: 1712.53 samples/sec#011loss=8.101125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] processed a total of 1518 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290134.8128726, \"EndTime\": 1618290136.1242113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1310.8367919921875, \"count\": 1, \"min\": 1310.8367919921875, \"max\": 1310.8367919921875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1157.926227902716 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=60, train loss <loss>=8.04651254415512\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[0] avg_epoch_loss=9.302232\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.302231788635254\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[5] avg_epoch_loss=8.236600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=8.236599842707315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch [5]#011Speed: 1963.69 samples/sec#011loss=8.236600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[10] avg_epoch_loss=8.361394\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=8.51114730834961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch [10]#011Speed: 899.95 samples/sec#011loss=8.511147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[15] avg_epoch_loss=8.476652\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=15 train loss <loss>=8.730218315124512\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch [15]#011Speed: 1996.17 samples/sec#011loss=8.730218\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[20] avg_epoch_loss=8.496863\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=20 train loss <loss>=8.561538982391358\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch [20]#011Speed: 914.24 samples/sec#011loss=8.561539\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch[25] avg_epoch_loss=8.424121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=25 train loss <loss>=8.11860294342041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:16 INFO 140529284842880] Epoch[61] Batch [25]#011Speed: 1771.40 samples/sec#011loss=8.118603\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch[30] avg_epoch_loss=8.305354\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=30 train loss <loss>=7.687767219543457\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch [30]#011Speed: 797.49 samples/sec#011loss=7.687767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch[35] avg_epoch_loss=8.221277\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=35 train loss <loss>=7.699999618530273\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch [35]#011Speed: 1982.45 samples/sec#011loss=7.700000\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch[40] avg_epoch_loss=8.173459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=40 train loss <loss>=7.829172611236572\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch [40]#011Speed: 898.44 samples/sec#011loss=7.829173\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch[45] avg_epoch_loss=8.166848\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=45 train loss <loss>=8.112634181976318\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch [45]#011Speed: 2153.28 samples/sec#011loss=8.112634\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch[50] avg_epoch_loss=8.130087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, batch=50 train loss <loss>=7.7918905258178714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[61] Batch [50]#011Speed: 1572.98 samples/sec#011loss=7.791891\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] processed a total of 1626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290136.1243017, \"EndTime\": 1618290137.4802496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1355.3757667541504, \"count\": 1, \"min\": 1355.3757667541504, \"max\": 1355.3757667541504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1199.5561081615406 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=61, train loss <loss>=8.130087403690114\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch[0] avg_epoch_loss=7.978017\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=7.978017330169678\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch[5] avg_epoch_loss=8.168369\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=8.16836929321289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch [5]#011Speed: 1532.35 samples/sec#011loss=8.168369\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch[10] avg_epoch_loss=8.147950\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=8.12344732284546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch [10]#011Speed: 863.58 samples/sec#011loss=8.123447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch[15] avg_epoch_loss=8.263961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=15 train loss <loss>=8.519184398651124\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:17 INFO 140529284842880] Epoch[62] Batch [15]#011Speed: 2039.78 samples/sec#011loss=8.519184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[20] avg_epoch_loss=8.278712\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=20 train loss <loss>=8.32591724395752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [20]#011Speed: 944.24 samples/sec#011loss=8.325917\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[25] avg_epoch_loss=8.222850\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=25 train loss <loss>=7.988226318359375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [25]#011Speed: 1934.81 samples/sec#011loss=7.988226\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[30] avg_epoch_loss=8.133502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=30 train loss <loss>=7.668893814086914\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [30]#011Speed: 914.27 samples/sec#011loss=7.668894\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[35] avg_epoch_loss=8.095444\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=35 train loss <loss>=7.859483337402343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [35]#011Speed: 1687.75 samples/sec#011loss=7.859483\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[40] avg_epoch_loss=8.046134\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=40 train loss <loss>=7.69110336303711\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [40]#011Speed: 856.78 samples/sec#011loss=7.691103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[45] avg_epoch_loss=8.084643\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=45 train loss <loss>=8.400417804718018\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [45]#011Speed: 1715.45 samples/sec#011loss=8.400418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch[50] avg_epoch_loss=8.058867\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, batch=50 train loss <loss>=7.821726894378662\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] Epoch[62] Batch [50]#011Speed: 1495.62 samples/sec#011loss=7.821727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] processed a total of 1668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290137.4803374, \"EndTime\": 1618290138.9237416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1442.8863525390625, \"count\": 1, \"min\": 1442.8863525390625, \"max\": 1442.8863525390625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1155.9300589605202 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=62, train loss <loss>=8.011801872613296\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:18 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[0] avg_epoch_loss=8.595860\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=8.59585952758789\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[5] avg_epoch_loss=8.094209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=8.094209432601929\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [5]#011Speed: 2158.90 samples/sec#011loss=8.094209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[10] avg_epoch_loss=8.204375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=8.336572647094727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [10]#011Speed: 943.85 samples/sec#011loss=8.336573\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[15] avg_epoch_loss=8.322499\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=15 train loss <loss>=8.582372093200684\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [15]#011Speed: 2052.11 samples/sec#011loss=8.582372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[20] avg_epoch_loss=8.266329\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=20 train loss <loss>=8.086585807800294\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [20]#011Speed: 912.47 samples/sec#011loss=8.086586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[25] avg_epoch_loss=8.197859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=25 train loss <loss>=7.910285186767578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [25]#011Speed: 1979.03 samples/sec#011loss=7.910285\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[30] avg_epoch_loss=8.126730\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=30 train loss <loss>=7.75685749053955\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [30]#011Speed: 846.72 samples/sec#011loss=7.756857\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch[35] avg_epoch_loss=8.073243\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=35 train loss <loss>=7.741621494293213\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:19 INFO 140529284842880] Epoch[63] Batch [35]#011Speed: 2131.10 samples/sec#011loss=7.741621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch[40] avg_epoch_loss=8.053333\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=40 train loss <loss>=7.909982490539551\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch [40]#011Speed: 844.87 samples/sec#011loss=7.909982\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch[45] avg_epoch_loss=8.013673\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=45 train loss <loss>=7.688465023040772\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch [45]#011Speed: 1973.06 samples/sec#011loss=7.688465\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch[50] avg_epoch_loss=7.990638\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, batch=50 train loss <loss>=7.778717422485352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[63] Batch [50]#011Speed: 1506.46 samples/sec#011loss=7.778717\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] processed a total of 1677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290138.923815, \"EndTime\": 1618290140.2987459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1374.3488788604736, \"count\": 1, \"min\": 1374.3488788604736, \"max\": 1374.3488788604736}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1220.0950230702515 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=63, train loss <loss>=7.974797563732795\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_42c88f7a-94d4-4941-b2d0-16152c0ac8ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290140.2988327, \"EndTime\": 1618290140.314641, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.260934829711914, \"count\": 1, \"min\": 15.260934829711914, \"max\": 15.260934829711914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch[0] avg_epoch_loss=7.793945\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=7.7939453125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch[5] avg_epoch_loss=8.127804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=8.127804279327393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch [5]#011Speed: 1698.27 samples/sec#011loss=8.127804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch[10] avg_epoch_loss=8.213328\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=8.315956783294677\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch [10]#011Speed: 779.83 samples/sec#011loss=8.315957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch[15] avg_epoch_loss=8.336436\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=15 train loss <loss>=8.607274436950684\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch [15]#011Speed: 2062.17 samples/sec#011loss=8.607274\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch[20] avg_epoch_loss=8.347316\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=20 train loss <loss>=8.382129287719726\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:20 INFO 140529284842880] Epoch[64] Batch [20]#011Speed: 901.63 samples/sec#011loss=8.382129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[25] avg_epoch_loss=8.302810\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=25 train loss <loss>=8.115887928009034\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [25]#011Speed: 2060.10 samples/sec#011loss=8.115888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[30] avg_epoch_loss=8.248339\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=30 train loss <loss>=7.9650876998901365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [30]#011Speed: 850.15 samples/sec#011loss=7.965088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[35] avg_epoch_loss=8.165661\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=35 train loss <loss>=7.653057003021241\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [35]#011Speed: 1919.39 samples/sec#011loss=7.653057\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[40] avg_epoch_loss=8.111451\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=40 train loss <loss>=7.7211432456970215\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [40]#011Speed: 866.19 samples/sec#011loss=7.721143\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[45] avg_epoch_loss=8.110446\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=45 train loss <loss>=8.102202701568604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [45]#011Speed: 1986.53 samples/sec#011loss=8.102203\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch[50] avg_epoch_loss=8.082706\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, batch=50 train loss <loss>=7.827493476867676\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[64] Batch [50]#011Speed: 1466.81 samples/sec#011loss=7.827493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] processed a total of 1653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290140.3147109, \"EndTime\": 1618290141.7248924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1410.125970840454, \"count\": 1, \"min\": 1410.125970840454, \"max\": 1410.125970840454}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1172.1413626063888 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=64, train loss <loss>=8.070243982168344\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[65] Batch[0] avg_epoch_loss=7.531420\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=7.5314202308654785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[65] Batch[5] avg_epoch_loss=7.949373\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=7.9493727684021\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:21 INFO 140529284842880] Epoch[65] Batch [5]#011Speed: 1847.39 samples/sec#011loss=7.949373\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[10] avg_epoch_loss=8.038028\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=8.14441499710083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [10]#011Speed: 789.77 samples/sec#011loss=8.144415\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[15] avg_epoch_loss=8.196970\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=15 train loss <loss>=8.54664134979248\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [15]#011Speed: 1759.11 samples/sec#011loss=8.546641\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[20] avg_epoch_loss=8.239125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=20 train loss <loss>=8.374021434783936\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [20]#011Speed: 809.05 samples/sec#011loss=8.374021\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[25] avg_epoch_loss=8.182472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=25 train loss <loss>=7.944526958465576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [25]#011Speed: 1920.38 samples/sec#011loss=7.944527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[30] avg_epoch_loss=8.125551\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=30 train loss <loss>=7.829566764831543\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [30]#011Speed: 874.49 samples/sec#011loss=7.829567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[35] avg_epoch_loss=8.056092\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=35 train loss <loss>=7.62544355392456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [35]#011Speed: 1948.58 samples/sec#011loss=7.625444\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch[40] avg_epoch_loss=8.032923\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=40 train loss <loss>=7.866102600097657\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:22 INFO 140529284842880] Epoch[65] Batch [40]#011Speed: 860.22 samples/sec#011loss=7.866103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[65] Batch[45] avg_epoch_loss=8.020500\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=45 train loss <loss>=7.918634128570557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[65] Batch [45]#011Speed: 2043.95 samples/sec#011loss=7.918634\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[65] Batch[50] avg_epoch_loss=8.001228\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, batch=50 train loss <loss>=7.823922538757325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[65] Batch [50]#011Speed: 1651.50 samples/sec#011loss=7.823923\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] processed a total of 1625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290141.7249694, \"EndTime\": 1618290143.1255543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1400.0434875488281, \"count\": 1, \"min\": 1400.0434875488281, \"max\": 1400.0434875488281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1160.5734837821813 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=65, train loss <loss>=8.00122761258892\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[0] avg_epoch_loss=7.845552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=7.845552444458008\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[5] avg_epoch_loss=8.101493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=8.1014932791392\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch [5]#011Speed: 2054.31 samples/sec#011loss=8.101493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[10] avg_epoch_loss=8.161248\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=8.232954502105713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch [10]#011Speed: 886.11 samples/sec#011loss=8.232955\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[15] avg_epoch_loss=8.339577\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=15 train loss <loss>=8.731901359558105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch [15]#011Speed: 1819.60 samples/sec#011loss=8.731901\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[20] avg_epoch_loss=8.367773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=20 train loss <loss>=8.457998466491699\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch [20]#011Speed: 902.22 samples/sec#011loss=8.457998\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch[25] avg_epoch_loss=8.315462\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=25 train loss <loss>=8.095755290985107\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:23 INFO 140529284842880] Epoch[66] Batch [25]#011Speed: 2082.04 samples/sec#011loss=8.095755\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch[30] avg_epoch_loss=8.261324\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=30 train loss <loss>=7.979804420471192\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch [30]#011Speed: 841.55 samples/sec#011loss=7.979804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch[35] avg_epoch_loss=8.190455\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=35 train loss <loss>=7.751071834564209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch [35]#011Speed: 1767.40 samples/sec#011loss=7.751072\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch[40] avg_epoch_loss=8.155452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=40 train loss <loss>=7.903426933288574\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch [40]#011Speed: 897.00 samples/sec#011loss=7.903427\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch[45] avg_epoch_loss=8.132238\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=45 train loss <loss>=7.941885185241699\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch [45]#011Speed: 1948.77 samples/sec#011loss=7.941885\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch[50] avg_epoch_loss=8.048957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, batch=50 train loss <loss>=7.282767200469971\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[66] Batch [50]#011Speed: 1587.03 samples/sec#011loss=7.282767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] processed a total of 1641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290143.1256435, \"EndTime\": 1618290144.50563, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1379.5182704925537, \"count\": 1, \"min\": 1379.5182704925537, \"max\": 1379.5182704925537}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1189.4391965488762 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=66, train loss <loss>=7.990188580292922\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch[0] avg_epoch_loss=8.351964\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=8.351963996887207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch[5] avg_epoch_loss=8.187376\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=8.187375863393148\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch [5]#011Speed: 2013.73 samples/sec#011loss=8.187376\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch[10] avg_epoch_loss=8.165157\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=8.138494682312011\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch [10]#011Speed: 877.87 samples/sec#011loss=8.138495\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch[15] avg_epoch_loss=8.328813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=15 train loss <loss>=8.68885498046875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:24 INFO 140529284842880] Epoch[67] Batch [15]#011Speed: 1740.54 samples/sec#011loss=8.688855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[20] avg_epoch_loss=8.396015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=20 train loss <loss>=8.611063766479493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [20]#011Speed: 812.72 samples/sec#011loss=8.611064\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[25] avg_epoch_loss=8.295169\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=25 train loss <loss>=7.871616172790527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [25]#011Speed: 2061.06 samples/sec#011loss=7.871616\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[30] avg_epoch_loss=8.200148\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=30 train loss <loss>=7.706037807464599\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [30]#011Speed: 862.34 samples/sec#011loss=7.706038\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[35] avg_epoch_loss=8.157059\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=35 train loss <loss>=7.889907455444336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [35]#011Speed: 1963.20 samples/sec#011loss=7.889907\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[40] avg_epoch_loss=8.108267\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=40 train loss <loss>=7.756961822509766\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [40]#011Speed: 846.48 samples/sec#011loss=7.756962\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[45] avg_epoch_loss=8.061309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=45 train loss <loss>=7.676251983642578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [45]#011Speed: 1855.54 samples/sec#011loss=7.676252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch[50] avg_epoch_loss=8.033316\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, batch=50 train loss <loss>=7.775784206390381\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] Epoch[67] Batch [50]#011Speed: 1567.82 samples/sec#011loss=7.775784\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] processed a total of 1687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290144.5057173, \"EndTime\": 1618290145.933578, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1427.2716045379639, \"count\": 1, \"min\": 1427.2716045379639, \"max\": 1427.2716045379639}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1181.8704216826518 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=67, train loss <loss>=8.027588304483666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:25 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[0] avg_epoch_loss=8.527336\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=8.527336120605469\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[5] avg_epoch_loss=8.109059\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=8.10905933380127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [5]#011Speed: 1954.62 samples/sec#011loss=8.109059\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[10] avg_epoch_loss=8.205982\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=8.322288703918456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [10]#011Speed: 944.74 samples/sec#011loss=8.322289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[15] avg_epoch_loss=8.238115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=15 train loss <loss>=8.308807754516602\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [15]#011Speed: 2060.58 samples/sec#011loss=8.308808\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[20] avg_epoch_loss=8.293166\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=20 train loss <loss>=8.469330024719238\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [20]#011Speed: 871.46 samples/sec#011loss=8.469330\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[25] avg_epoch_loss=8.285153\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=25 train loss <loss>=8.251499557495118\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [25]#011Speed: 2008.19 samples/sec#011loss=8.251500\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[30] avg_epoch_loss=8.232530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=30 train loss <loss>=7.958888244628906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [30]#011Speed: 925.29 samples/sec#011loss=7.958888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch[35] avg_epoch_loss=8.182049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=35 train loss <loss>=7.869065284729004\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:26 INFO 140529284842880] Epoch[68] Batch [35]#011Speed: 2051.97 samples/sec#011loss=7.869065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch[40] avg_epoch_loss=8.123509\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=40 train loss <loss>=7.702023887634278\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch [40]#011Speed: 839.14 samples/sec#011loss=7.702024\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch[45] avg_epoch_loss=8.094801\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=45 train loss <loss>=7.859391880035401\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch [45]#011Speed: 1700.49 samples/sec#011loss=7.859392\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch[50] avg_epoch_loss=8.063021\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, batch=50 train loss <loss>=7.7706451416015625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[68] Batch [50]#011Speed: 1514.80 samples/sec#011loss=7.770645\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] processed a total of 1653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290145.933666, \"EndTime\": 1618290147.3116703, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1377.5067329406738, \"count\": 1, \"min\": 1377.5067329406738, \"max\": 1377.5067329406738}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1199.8807440620203 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=68, train loss <loss>=8.01834736420558\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch[0] avg_epoch_loss=8.283494\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=8.283493995666504\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch[5] avg_epoch_loss=8.287257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=8.287256956100464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch [5]#011Speed: 1975.10 samples/sec#011loss=8.287257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch[10] avg_epoch_loss=8.349119\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=8.42335386276245\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch [10]#011Speed: 904.34 samples/sec#011loss=8.423354\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch[15] avg_epoch_loss=8.407007\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=15 train loss <loss>=8.534359169006347\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch [15]#011Speed: 2054.98 samples/sec#011loss=8.534359\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch[20] avg_epoch_loss=8.368242\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=20 train loss <loss>=8.244194316864014\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:27 INFO 140529284842880] Epoch[69] Batch [20]#011Speed: 951.45 samples/sec#011loss=8.244194\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[25] avg_epoch_loss=8.290986\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=25 train loss <loss>=7.966508960723877\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [25]#011Speed: 1925.79 samples/sec#011loss=7.966509\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[30] avg_epoch_loss=8.250328\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=30 train loss <loss>=8.038910484313964\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [30]#011Speed: 868.14 samples/sec#011loss=8.038910\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[35] avg_epoch_loss=8.190564\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=35 train loss <loss>=7.820025062561035\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [35]#011Speed: 2055.74 samples/sec#011loss=7.820025\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[40] avg_epoch_loss=8.143403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=40 train loss <loss>=7.8038427352905275\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [40]#011Speed: 930.76 samples/sec#011loss=7.803843\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[45] avg_epoch_loss=8.076088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=45 train loss <loss>=7.52410306930542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [45]#011Speed: 2050.82 samples/sec#011loss=7.524103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch[50] avg_epoch_loss=8.067249\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, batch=50 train loss <loss>=7.9859375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[69] Batch [50]#011Speed: 1331.12 samples/sec#011loss=7.985938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] processed a total of 1683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290147.3117619, \"EndTime\": 1618290148.686729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1374.4347095489502, \"count\": 1, \"min\": 1374.4347095489502, \"max\": 1374.4347095489502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1224.3798078148895 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=69, train loss <loss>=8.068271358058137\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[70] Batch[0] avg_epoch_loss=8.325301\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=8.325301170349121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[70] Batch[5] avg_epoch_loss=8.327742\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=8.327742099761963\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:28 INFO 140529284842880] Epoch[70] Batch [5]#011Speed: 2065.02 samples/sec#011loss=8.327742\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[10] avg_epoch_loss=8.235393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=8.124574756622314\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [10]#011Speed: 825.81 samples/sec#011loss=8.124575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[15] avg_epoch_loss=8.294108\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=15 train loss <loss>=8.423279190063477\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [15]#011Speed: 2055.97 samples/sec#011loss=8.423279\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[20] avg_epoch_loss=8.383597\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=20 train loss <loss>=8.669960975646973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [20]#011Speed: 931.39 samples/sec#011loss=8.669961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[25] avg_epoch_loss=8.322754\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=25 train loss <loss>=8.067214965820312\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [25]#011Speed: 1862.47 samples/sec#011loss=8.067215\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[30] avg_epoch_loss=8.228567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=30 train loss <loss>=7.7387939453125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [30]#011Speed: 792.99 samples/sec#011loss=7.738794\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[35] avg_epoch_loss=8.168758\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=35 train loss <loss>=7.797944164276123\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [35]#011Speed: 1900.73 samples/sec#011loss=7.797944\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[40] avg_epoch_loss=8.131799\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=40 train loss <loss>=7.865690517425537\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [40]#011Speed: 838.00 samples/sec#011loss=7.865691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch[45] avg_epoch_loss=8.135578\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=45 train loss <loss>=8.166566753387452\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:29 INFO 140529284842880] Epoch[70] Batch [45]#011Speed: 1776.75 samples/sec#011loss=8.166567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[70] Batch[50] avg_epoch_loss=8.099371\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, batch=50 train loss <loss>=7.766265106201172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[70] Batch [50]#011Speed: 1836.61 samples/sec#011loss=7.766265\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] processed a total of 1608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290148.6868286, \"EndTime\": 1618290150.0742495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1386.8694305419922, \"count\": 1, \"min\": 1386.8694305419922, \"max\": 1386.8694305419922}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1159.3428056084183 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=70, train loss <loss>=8.099370675928453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[0] avg_epoch_loss=8.313695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=8.313694953918457\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[5] avg_epoch_loss=8.335175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=8.335174878438314\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch [5]#011Speed: 1955.61 samples/sec#011loss=8.335175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[10] avg_epoch_loss=8.301123\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=8.260260772705077\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch [10]#011Speed: 915.32 samples/sec#011loss=8.260261\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[15] avg_epoch_loss=8.351214\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=15 train loss <loss>=8.461415481567382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch [15]#011Speed: 2123.05 samples/sec#011loss=8.461415\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[20] avg_epoch_loss=8.400025\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=20 train loss <loss>=8.556218910217286\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch [20]#011Speed: 772.45 samples/sec#011loss=8.556219\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch[25] avg_epoch_loss=8.308083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=25 train loss <loss>=7.921927356719971\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:30 INFO 140529284842880] Epoch[71] Batch [25]#011Speed: 1731.59 samples/sec#011loss=7.921927\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch[30] avg_epoch_loss=8.240324\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=30 train loss <loss>=7.887976455688476\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch [30]#011Speed: 877.39 samples/sec#011loss=7.887976\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch[35] avg_epoch_loss=8.155956\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=35 train loss <loss>=7.632875156402588\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch [35]#011Speed: 1993.64 samples/sec#011loss=7.632875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch[40] avg_epoch_loss=8.085236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=40 train loss <loss>=7.576051425933838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch [40]#011Speed: 915.53 samples/sec#011loss=7.576051\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch[45] avg_epoch_loss=8.080009\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=45 train loss <loss>=8.037150669097901\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch [45]#011Speed: 2052.78 samples/sec#011loss=8.037151\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch[50] avg_epoch_loss=8.045563\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, batch=50 train loss <loss>=7.728651714324951\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[71] Batch [50]#011Speed: 1565.32 samples/sec#011loss=7.728652\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] processed a total of 1666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290150.0743356, \"EndTime\": 1618290151.4857206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1410.8836650848389, \"count\": 1, \"min\": 1410.8836650848389, \"max\": 1410.8836650848389}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1180.7101119980696 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=71, train loss <loss>=7.926837885154868\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/state_4d84c069-559a-4ab9-b9c6-d4ff5bff312f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290151.4858105, \"EndTime\": 1618290151.4963467, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.940624237060547, \"count\": 1, \"min\": 9.940624237060547, \"max\": 9.940624237060547}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch[0] avg_epoch_loss=8.411853\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=8.411852836608887\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch[5] avg_epoch_loss=8.489721\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=8.489720503489176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch [5]#011Speed: 1647.79 samples/sec#011loss=8.489721\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch[10] avg_epoch_loss=8.311312\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=8.09722080230713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch [10]#011Speed: 799.37 samples/sec#011loss=8.097221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch[15] avg_epoch_loss=8.339122\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=15 train loss <loss>=8.40030460357666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:31 INFO 140529284842880] Epoch[72] Batch [15]#011Speed: 2003.87 samples/sec#011loss=8.400305\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[20] avg_epoch_loss=8.375122\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=20 train loss <loss>=8.490323066711426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [20]#011Speed: 832.53 samples/sec#011loss=8.490323\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[25] avg_epoch_loss=8.307547\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=25 train loss <loss>=8.02373218536377\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [25]#011Speed: 2052.82 samples/sec#011loss=8.023732\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[30] avg_epoch_loss=8.235049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=30 train loss <loss>=7.858058166503906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [30]#011Speed: 906.42 samples/sec#011loss=7.858058\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[35] avg_epoch_loss=8.148593\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=35 train loss <loss>=7.612564945220948\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [35]#011Speed: 2076.09 samples/sec#011loss=7.612565\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[40] avg_epoch_loss=8.079117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=40 train loss <loss>=7.578894805908203\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [40]#011Speed: 871.05 samples/sec#011loss=7.578895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[45] avg_epoch_loss=8.070511\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=45 train loss <loss>=7.9999415397644045\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [45]#011Speed: 2048.32 samples/sec#011loss=7.999942\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch[50] avg_epoch_loss=8.049687\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, batch=50 train loss <loss>=7.858098316192627\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[72] Batch [50]#011Speed: 1968.04 samples/sec#011loss=7.858098\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] processed a total of 1607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290151.496397, \"EndTime\": 1618290152.8579617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1361.5059852600098, \"count\": 1, \"min\": 1361.5059852600098, \"max\": 1361.5059852600098}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1180.2023444194756 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=72, train loss <loss>=8.04968657213099\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] Epoch[73] Batch[0] avg_epoch_loss=8.357273\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:32 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=8.35727310180664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[5] avg_epoch_loss=8.221506\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=8.221505959828695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [5]#011Speed: 1968.12 samples/sec#011loss=8.221506\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[10] avg_epoch_loss=8.136383\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=8.034235858917237\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [10]#011Speed: 843.36 samples/sec#011loss=8.034236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[15] avg_epoch_loss=8.340039\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=15 train loss <loss>=8.788081932067872\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [15]#011Speed: 1944.98 samples/sec#011loss=8.788082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[20] avg_epoch_loss=8.329884\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=20 train loss <loss>=8.297387790679931\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [20]#011Speed: 871.01 samples/sec#011loss=8.297388\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[25] avg_epoch_loss=8.282838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=25 train loss <loss>=8.085245990753174\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [25]#011Speed: 2076.80 samples/sec#011loss=8.085246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[30] avg_epoch_loss=8.206483\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=30 train loss <loss>=7.809433650970459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [30]#011Speed: 866.99 samples/sec#011loss=7.809434\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch[35] avg_epoch_loss=8.122239\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=35 train loss <loss>=7.599928569793701\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:33 INFO 140529284842880] Epoch[73] Batch [35]#011Speed: 1878.17 samples/sec#011loss=7.599929\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[73] Batch[40] avg_epoch_loss=8.100483\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=40 train loss <loss>=7.943841171264649\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[73] Batch [40]#011Speed: 833.23 samples/sec#011loss=7.943841\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[73] Batch[45] avg_epoch_loss=8.084375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, batch=45 train loss <loss>=7.952290153503418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[73] Batch [45]#011Speed: 1759.57 samples/sec#011loss=7.952290\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] processed a total of 1590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290152.8580499, \"EndTime\": 1618290154.216748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1358.2000732421875, \"count\": 1, \"min\": 1358.2000732421875, \"max\": 1358.2000732421875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1170.5511062495218 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=73, train loss <loss>=8.066692199707031\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[0] avg_epoch_loss=8.647535\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=8.64753532409668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[5] avg_epoch_loss=8.174285\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=8.17428461710612\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch [5]#011Speed: 1924.70 samples/sec#011loss=8.174285\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[10] avg_epoch_loss=8.244542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=8.328850746154785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch [10]#011Speed: 869.96 samples/sec#011loss=8.328851\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[15] avg_epoch_loss=8.412812\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=15 train loss <loss>=8.783007621765137\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch [15]#011Speed: 1932.05 samples/sec#011loss=8.783008\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[20] avg_epoch_loss=8.407804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=20 train loss <loss>=8.39177770614624\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch [20]#011Speed: 878.83 samples/sec#011loss=8.391778\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch[25] avg_epoch_loss=8.270839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=25 train loss <loss>=7.695585823059082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:34 INFO 140529284842880] Epoch[74] Batch [25]#011Speed: 2026.75 samples/sec#011loss=7.695586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch[30] avg_epoch_loss=8.202472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=30 train loss <loss>=7.846963787078858\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch [30]#011Speed: 857.27 samples/sec#011loss=7.846964\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch[35] avg_epoch_loss=8.138911\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=35 train loss <loss>=7.744832229614258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch [35]#011Speed: 2069.33 samples/sec#011loss=7.744832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch[40] avg_epoch_loss=8.117909\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=40 train loss <loss>=7.96669750213623\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch [40]#011Speed: 889.63 samples/sec#011loss=7.966698\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch[45] avg_epoch_loss=8.117713\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=45 train loss <loss>=8.116105079650879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch [45]#011Speed: 2061.24 samples/sec#011loss=8.116105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch[50] avg_epoch_loss=8.085352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, batch=50 train loss <loss>=7.787624740600586\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[74] Batch [50]#011Speed: 1582.43 samples/sec#011loss=7.787625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] processed a total of 1644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290154.216843, \"EndTime\": 1618290155.579147, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1361.75537109375, \"count\": 1, \"min\": 1361.75537109375, \"max\": 1361.75537109375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1207.152222691592 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=74, train loss <loss>=8.041743837870085\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[75] Batch[0] avg_epoch_loss=8.396224\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=8.396224021911621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[75] Batch[5] avg_epoch_loss=8.223472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=8.223471641540527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[75] Batch [5]#011Speed: 1899.65 samples/sec#011loss=8.223472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[75] Batch[10] avg_epoch_loss=8.216507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=8.20814905166626\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:35 INFO 140529284842880] Epoch[75] Batch [10]#011Speed: 776.59 samples/sec#011loss=8.208149\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[15] avg_epoch_loss=8.306796\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=15 train loss <loss>=8.505433273315429\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [15]#011Speed: 2031.67 samples/sec#011loss=8.505433\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[20] avg_epoch_loss=8.379589\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=20 train loss <loss>=8.612526512145996\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [20]#011Speed: 907.18 samples/sec#011loss=8.612527\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[25] avg_epoch_loss=8.302560\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=25 train loss <loss>=7.979036903381347\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [25]#011Speed: 1925.22 samples/sec#011loss=7.979037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[30] avg_epoch_loss=8.228607\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=30 train loss <loss>=7.844048976898193\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [30]#011Speed: 877.36 samples/sec#011loss=7.844049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[35] avg_epoch_loss=8.120198\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=35 train loss <loss>=7.448067760467529\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [35]#011Speed: 2040.56 samples/sec#011loss=7.448068\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[40] avg_epoch_loss=8.058337\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=40 train loss <loss>=7.612932586669922\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [40]#011Speed: 971.89 samples/sec#011loss=7.612933\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch[45] avg_epoch_loss=8.031978\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, batch=45 train loss <loss>=7.815834426879883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] Epoch[75] Batch [45]#011Speed: 1906.95 samples/sec#011loss=7.815834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] processed a total of 1544 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290155.5792346, \"EndTime\": 1618290156.891022, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1311.2685680389404, \"count\": 1, \"min\": 1311.2685680389404, \"max\": 1311.2685680389404}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1177.3538423499788 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] #quality_metric: host=algo-1, epoch=75, train loss <loss>=8.145474618794967\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:36 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[0] avg_epoch_loss=8.242468\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=8.242467880249023\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[5] avg_epoch_loss=8.244855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=8.244855403900146\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [5]#011Speed: 1921.01 samples/sec#011loss=8.244855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[10] avg_epoch_loss=8.278465\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=8.318796730041504\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [10]#011Speed: 836.79 samples/sec#011loss=8.318797\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[15] avg_epoch_loss=8.312689\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=15 train loss <loss>=8.387982940673828\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [15]#011Speed: 2020.97 samples/sec#011loss=8.387983\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[20] avg_epoch_loss=8.330413\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=20 train loss <loss>=8.387129878997802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [20]#011Speed: 921.99 samples/sec#011loss=8.387130\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[25] avg_epoch_loss=8.238650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=25 train loss <loss>=7.853244781494141\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [25]#011Speed: 1955.48 samples/sec#011loss=7.853245\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[30] avg_epoch_loss=8.150348\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=30 train loss <loss>=7.691176128387451\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [30]#011Speed: 912.27 samples/sec#011loss=7.691176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch[35] avg_epoch_loss=8.121797\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=35 train loss <loss>=7.944779968261718\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:37 INFO 140529284842880] Epoch[76] Batch [35]#011Speed: 1898.11 samples/sec#011loss=7.944780\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[76] Batch[40] avg_epoch_loss=8.077077\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=40 train loss <loss>=7.755097579956055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[76] Batch [40]#011Speed: 984.06 samples/sec#011loss=7.755098\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[76] Batch[45] avg_epoch_loss=8.074030\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, batch=45 train loss <loss>=8.049036979675293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[76] Batch [45]#011Speed: 2077.22 samples/sec#011loss=8.049037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] processed a total of 1570 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290156.8911314, \"EndTime\": 1618290158.2003822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1308.744192123413, \"count\": 1, \"min\": 1308.744192123413, \"max\": 1308.744192123413}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1199.4971955507694 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=76, train loss <loss>=8.111190090179443\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[0] avg_epoch_loss=7.894733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=7.89473295211792\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[5] avg_epoch_loss=8.015655\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=8.015655358632406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch [5]#011Speed: 1966.44 samples/sec#011loss=8.015655\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[10] avg_epoch_loss=8.152546\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=8.316814136505126\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch [10]#011Speed: 762.55 samples/sec#011loss=8.316814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[15] avg_epoch_loss=8.235538\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=15 train loss <loss>=8.418120574951171\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch [15]#011Speed: 2072.65 samples/sec#011loss=8.418121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[20] avg_epoch_loss=8.285576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=20 train loss <loss>=8.445699882507324\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch [20]#011Speed: 924.65 samples/sec#011loss=8.445700\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch[25] avg_epoch_loss=8.250737\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=25 train loss <loss>=8.10441026687622\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:38 INFO 140529284842880] Epoch[77] Batch [25]#011Speed: 1958.05 samples/sec#011loss=8.104410\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch[30] avg_epoch_loss=8.221110\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=30 train loss <loss>=8.067048931121827\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch [30]#011Speed: 916.27 samples/sec#011loss=8.067049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch[35] avg_epoch_loss=8.141477\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=35 train loss <loss>=7.647756767272949\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch [35]#011Speed: 2000.38 samples/sec#011loss=7.647757\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch[40] avg_epoch_loss=8.068230\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=40 train loss <loss>=7.54084882736206\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch [40]#011Speed: 947.97 samples/sec#011loss=7.540849\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch[45] avg_epoch_loss=8.036846\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=45 train loss <loss>=7.779499530792236\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch [45]#011Speed: 2078.50 samples/sec#011loss=7.779500\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch[50] avg_epoch_loss=8.011643\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, batch=50 train loss <loss>=7.779772567749023\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[77] Batch [50]#011Speed: 1400.50 samples/sec#011loss=7.779773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] processed a total of 1717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290158.2004821, \"EndTime\": 1618290159.6045659, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1403.5732746124268, \"count\": 1, \"min\": 1403.5732746124268, \"max\": 1403.5732746124268}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1223.1957408393482 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=77, train loss <loss>=8.007709988841304\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[78] Batch[0] avg_epoch_loss=8.652139\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=8.652138710021973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[78] Batch[5] avg_epoch_loss=7.957430\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=7.957429567972819\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[78] Batch [5]#011Speed: 1759.52 samples/sec#011loss=7.957430\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[78] Batch[10] avg_epoch_loss=8.121097\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=8.317497634887696\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:39 INFO 140529284842880] Epoch[78] Batch [10]#011Speed: 895.81 samples/sec#011loss=8.317498\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[15] avg_epoch_loss=8.294040\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=15 train loss <loss>=8.674515914916991\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [15]#011Speed: 1927.76 samples/sec#011loss=8.674516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[20] avg_epoch_loss=8.272110\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=20 train loss <loss>=8.201933002471923\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [20]#011Speed: 820.52 samples/sec#011loss=8.201933\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[25] avg_epoch_loss=8.185132\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=25 train loss <loss>=7.819825077056885\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [25]#011Speed: 1983.25 samples/sec#011loss=7.819825\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[30] avg_epoch_loss=8.134858\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=30 train loss <loss>=7.8734314918518065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [30]#011Speed: 954.27 samples/sec#011loss=7.873431\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[35] avg_epoch_loss=8.089895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=35 train loss <loss>=7.811125755310059\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [35]#011Speed: 2061.87 samples/sec#011loss=7.811126\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[40] avg_epoch_loss=8.049133\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=40 train loss <loss>=7.7556455612182615\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [40]#011Speed: 896.62 samples/sec#011loss=7.755646\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[45] avg_epoch_loss=8.025557\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=45 train loss <loss>=7.832231903076172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [45]#011Speed: 1787.40 samples/sec#011loss=7.832232\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch[50] avg_epoch_loss=7.987099\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, batch=50 train loss <loss>=7.633285903930664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] Epoch[78] Batch [50]#011Speed: 1438.76 samples/sec#011loss=7.633286\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] processed a total of 1651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290159.6046553, \"EndTime\": 1618290160.9957497, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1390.568733215332, \"count\": 1, \"min\": 1390.568733215332, \"max\": 1390.568733215332}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1187.1663456794272 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] #quality_metric: host=algo-1, epoch=78, train loss <loss>=7.969569206237793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:40 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[0] avg_epoch_loss=8.070341\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=8.070341110229492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[5] avg_epoch_loss=8.136594\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=8.136593659718832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [5]#011Speed: 1836.21 samples/sec#011loss=8.136594\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[10] avg_epoch_loss=8.297083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=8.489670181274414\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [10]#011Speed: 909.21 samples/sec#011loss=8.489670\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[15] avg_epoch_loss=8.440081\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=15 train loss <loss>=8.75467529296875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [15]#011Speed: 2043.90 samples/sec#011loss=8.754675\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[20] avg_epoch_loss=8.477652\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=20 train loss <loss>=8.597882270812988\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [20]#011Speed: 927.84 samples/sec#011loss=8.597882\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[25] avg_epoch_loss=8.425105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=25 train loss <loss>=8.204404354095459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [25]#011Speed: 2115.26 samples/sec#011loss=8.204404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[30] avg_epoch_loss=8.366910\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=30 train loss <loss>=8.064296436309814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [30]#011Speed: 804.29 samples/sec#011loss=8.064296\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch[35] avg_epoch_loss=8.271949\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=35 train loss <loss>=7.683193016052246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:41 INFO 140529284842880] Epoch[79] Batch [35]#011Speed: 1930.64 samples/sec#011loss=7.683193\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch[40] avg_epoch_loss=8.227007\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=40 train loss <loss>=7.903425025939941\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch [40]#011Speed: 896.98 samples/sec#011loss=7.903425\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch[45] avg_epoch_loss=8.162747\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=45 train loss <loss>=7.635815620422363\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch [45]#011Speed: 1711.30 samples/sec#011loss=7.635816\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch[50] avg_epoch_loss=8.112824\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, batch=50 train loss <loss>=7.653535079956055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[79] Batch [50]#011Speed: 1250.30 samples/sec#011loss=7.653535\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] processed a total of 1722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290160.99585, \"EndTime\": 1618290162.4403796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1444.0279006958008, \"count\": 1, \"min\": 1444.0279006958008, \"max\": 1444.0279006958008}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1192.400872748789 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=79, train loss <loss>=8.1148730913798\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch[0] avg_epoch_loss=7.806864\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=7.806864261627197\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch[5] avg_epoch_loss=8.199178\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=8.199177662531534\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch [5]#011Speed: 1969.69 samples/sec#011loss=8.199178\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch[10] avg_epoch_loss=8.236257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=8.280752277374267\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch [10]#011Speed: 835.65 samples/sec#011loss=8.280752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch[15] avg_epoch_loss=8.296447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=15 train loss <loss>=8.428864097595214\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:42 INFO 140529284842880] Epoch[80] Batch [15]#011Speed: 1743.08 samples/sec#011loss=8.428864\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[20] avg_epoch_loss=8.283547\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=20 train loss <loss>=8.242269897460938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [20]#011Speed: 858.08 samples/sec#011loss=8.242270\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[25] avg_epoch_loss=8.210580\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=25 train loss <loss>=7.904118537902832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [25]#011Speed: 1925.26 samples/sec#011loss=7.904119\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[30] avg_epoch_loss=8.159233\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=30 train loss <loss>=7.892226696014404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [30]#011Speed: 883.98 samples/sec#011loss=7.892227\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[35] avg_epoch_loss=8.111327\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=35 train loss <loss>=7.814312839508057\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [35]#011Speed: 2071.29 samples/sec#011loss=7.814313\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[40] avg_epoch_loss=8.075180\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=40 train loss <loss>=7.81491641998291\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [40]#011Speed: 817.32 samples/sec#011loss=7.814916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[45] avg_epoch_loss=8.080938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=45 train loss <loss>=8.128157138824463\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [45]#011Speed: 2049.90 samples/sec#011loss=8.128157\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch[50] avg_epoch_loss=8.045954\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, batch=50 train loss <loss>=7.724103355407715\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[80] Batch [50]#011Speed: 1715.12 samples/sec#011loss=7.724103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] processed a total of 1625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290162.4404624, \"EndTime\": 1618290163.8121533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1371.1309432983398, \"count\": 1, \"min\": 1371.1309432983398, \"max\": 1371.1309432983398}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1185.0366121562965 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=80, train loss <loss>=8.045954358343984\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] Epoch[81] Batch[0] avg_epoch_loss=8.438420\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:43 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=8.438420295715332\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[5] avg_epoch_loss=8.399640\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=8.39963960647583\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [5]#011Speed: 2129.73 samples/sec#011loss=8.399640\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[10] avg_epoch_loss=8.279480\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=8.135289192199707\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [10]#011Speed: 931.46 samples/sec#011loss=8.135289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[15] avg_epoch_loss=8.286573\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=15 train loss <loss>=8.302177333831787\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [15]#011Speed: 2037.53 samples/sec#011loss=8.302177\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[20] avg_epoch_loss=8.254478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=20 train loss <loss>=8.151772117614746\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [20]#011Speed: 814.81 samples/sec#011loss=8.151772\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[25] avg_epoch_loss=8.179227\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=25 train loss <loss>=7.86317663192749\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [25]#011Speed: 2058.71 samples/sec#011loss=7.863177\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[30] avg_epoch_loss=8.117334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=30 train loss <loss>=7.795487689971924\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [30]#011Speed: 948.03 samples/sec#011loss=7.795488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[35] avg_epoch_loss=8.082493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=35 train loss <loss>=7.866478443145752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [35]#011Speed: 1942.30 samples/sec#011loss=7.866478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch[40] avg_epoch_loss=8.046129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=40 train loss <loss>=7.784305667877197\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:44 INFO 140529284842880] Epoch[81] Batch [40]#011Speed: 881.95 samples/sec#011loss=7.784306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[81] Batch[45] avg_epoch_loss=8.018988\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, batch=45 train loss <loss>=7.796431255340576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[81] Batch [45]#011Speed: 2008.44 samples/sec#011loss=7.796431\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] processed a total of 1573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290163.8122437, \"EndTime\": 1618290165.109997, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1297.109842300415, \"count\": 1, \"min\": 1297.109842300415, \"max\": 1297.109842300415}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1212.5750396617102 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=81, train loss <loss>=8.013391790390015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[0] avg_epoch_loss=7.707407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=7.707406997680664\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[5] avg_epoch_loss=8.126375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=8.126375198364258\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch [5]#011Speed: 1931.01 samples/sec#011loss=8.126375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[10] avg_epoch_loss=8.213445\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=8.317927837371826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch [10]#011Speed: 895.42 samples/sec#011loss=8.317928\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[15] avg_epoch_loss=8.216654\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=15 train loss <loss>=8.223713874816895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch [15]#011Speed: 1915.45 samples/sec#011loss=8.223714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[20] avg_epoch_loss=8.250996\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=20 train loss <loss>=8.360891723632813\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch [20]#011Speed: 847.23 samples/sec#011loss=8.360892\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch[25] avg_epoch_loss=8.179878\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=25 train loss <loss>=7.881183528900147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:45 INFO 140529284842880] Epoch[82] Batch [25]#011Speed: 1982.06 samples/sec#011loss=7.881184\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch[30] avg_epoch_loss=8.118771\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=30 train loss <loss>=7.801009941101074\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch [30]#011Speed: 753.44 samples/sec#011loss=7.801010\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch[35] avg_epoch_loss=8.049947\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=35 train loss <loss>=7.623239231109619\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch [35]#011Speed: 1741.38 samples/sec#011loss=7.623239\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch[40] avg_epoch_loss=8.027696\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=40 train loss <loss>=7.867487621307373\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch [40]#011Speed: 891.40 samples/sec#011loss=7.867488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch[45] avg_epoch_loss=8.017166\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, batch=45 train loss <loss>=7.930823707580567\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[82] Batch [45]#011Speed: 1847.15 samples/sec#011loss=7.930824\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] processed a total of 1592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290165.110088, \"EndTime\": 1618290166.4699912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1359.3125343322754, \"count\": 1, \"min\": 1359.3125343322754, \"max\": 1359.3125343322754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1171.0584526767361 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=82, train loss <loss>=8.053560009002686\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch[0] avg_epoch_loss=8.025435\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=8.025435447692871\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch[5] avg_epoch_loss=8.174879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=8.17487907409668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch [5]#011Speed: 2051.14 samples/sec#011loss=8.174879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch[10] avg_epoch_loss=8.106354\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=8.0241229057312\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch [10]#011Speed: 868.52 samples/sec#011loss=8.024123\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch[15] avg_epoch_loss=8.256124\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=15 train loss <loss>=8.585617733001708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:46 INFO 140529284842880] Epoch[83] Batch [15]#011Speed: 1732.72 samples/sec#011loss=8.585618\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[20] avg_epoch_loss=8.365878\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=20 train loss <loss>=8.717094039916992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [20]#011Speed: 880.09 samples/sec#011loss=8.717094\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[25] avg_epoch_loss=8.260476\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=25 train loss <loss>=7.81778564453125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [25]#011Speed: 2055.48 samples/sec#011loss=7.817786\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[30] avg_epoch_loss=8.181167\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=30 train loss <loss>=7.768761825561524\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [30]#011Speed: 924.17 samples/sec#011loss=7.768762\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[35] avg_epoch_loss=8.097899\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=35 train loss <loss>=7.581633472442627\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [35]#011Speed: 1988.87 samples/sec#011loss=7.581633\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[40] avg_epoch_loss=8.027691\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=40 train loss <loss>=7.522194480895996\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [40]#011Speed: 913.67 samples/sec#011loss=7.522194\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[45] avg_epoch_loss=8.033237\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=45 train loss <loss>=8.078713989257812\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [45]#011Speed: 2007.76 samples/sec#011loss=8.078714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch[50] avg_epoch_loss=8.010404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, batch=50 train loss <loss>=7.800345802307129\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[83] Batch [50]#011Speed: 1426.42 samples/sec#011loss=7.800346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] processed a total of 1616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290166.4700828, \"EndTime\": 1618290167.8254206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1354.8328876495361, \"count\": 1, \"min\": 1354.8328876495361, \"max\": 1354.8328876495361}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1192.657263354674 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=83, train loss <loss>=8.01040439044728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] Epoch[84] Batch[0] avg_epoch_loss=7.921240\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:47 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=7.921240329742432\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[5] avg_epoch_loss=8.262722\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=8.26272209485372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [5]#011Speed: 1840.51 samples/sec#011loss=8.262722\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[10] avg_epoch_loss=8.269962\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=8.278650093078614\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [10]#011Speed: 840.81 samples/sec#011loss=8.278650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[15] avg_epoch_loss=8.293167\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=15 train loss <loss>=8.344216537475585\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [15]#011Speed: 2054.52 samples/sec#011loss=8.344217\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[20] avg_epoch_loss=8.295284\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=20 train loss <loss>=8.3020601272583\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [20]#011Speed: 709.38 samples/sec#011loss=8.302060\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[25] avg_epoch_loss=8.205475\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=25 train loss <loss>=7.828278923034668\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [25]#011Speed: 1966.85 samples/sec#011loss=7.828279\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[30] avg_epoch_loss=8.156658\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=30 train loss <loss>=7.9028076171875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [30]#011Speed: 918.94 samples/sec#011loss=7.902808\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch[35] avg_epoch_loss=8.080915\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=35 train loss <loss>=7.611309337615967\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:48 INFO 140529284842880] Epoch[84] Batch [35]#011Speed: 1825.84 samples/sec#011loss=7.611309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[84] Batch[40] avg_epoch_loss=8.008396\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=40 train loss <loss>=7.486254787445068\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[84] Batch [40]#011Speed: 886.59 samples/sec#011loss=7.486255\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[84] Batch[45] avg_epoch_loss=8.006411\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, batch=45 train loss <loss>=7.990139102935791\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[84] Batch [45]#011Speed: 1777.00 samples/sec#011loss=7.990139\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] processed a total of 1596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290167.8255076, \"EndTime\": 1618290169.2402418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1414.269208908081, \"count\": 1, \"min\": 1414.269208908081, \"max\": 1414.269208908081}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1128.39396523039 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=84, train loss <loss>=7.995165519714355\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[0] avg_epoch_loss=8.515616\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=8.515616416931152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[5] avg_epoch_loss=8.256329\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=8.256328662236532\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch [5]#011Speed: 1852.38 samples/sec#011loss=8.256329\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[10] avg_epoch_loss=8.239683\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=8.219707679748534\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch [10]#011Speed: 863.65 samples/sec#011loss=8.219708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[15] avg_epoch_loss=8.378149\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=15 train loss <loss>=8.682775115966797\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch [15]#011Speed: 1717.99 samples/sec#011loss=8.682775\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[20] avg_epoch_loss=8.376147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=20 train loss <loss>=8.369740867614746\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch [20]#011Speed: 875.12 samples/sec#011loss=8.369741\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch[25] avg_epoch_loss=8.254099\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=25 train loss <loss>=7.741494274139404\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:49 INFO 140529284842880] Epoch[85] Batch [25]#011Speed: 1872.97 samples/sec#011loss=7.741494\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch[30] avg_epoch_loss=8.191670\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=30 train loss <loss>=7.867040157318115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch [30]#011Speed: 782.01 samples/sec#011loss=7.867040\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch[35] avg_epoch_loss=8.098343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=35 train loss <loss>=7.519713592529297\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch [35]#011Speed: 1789.85 samples/sec#011loss=7.519714\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch[40] avg_epoch_loss=8.056456\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=40 train loss <loss>=7.754876613616943\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch [40]#011Speed: 933.63 samples/sec#011loss=7.754877\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch[45] avg_epoch_loss=8.070752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=45 train loss <loss>=8.18797779083252\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch [45]#011Speed: 1714.06 samples/sec#011loss=8.187978\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch[50] avg_epoch_loss=8.048814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, batch=50 train loss <loss>=7.846983814239502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[85] Batch [50]#011Speed: 1451.77 samples/sec#011loss=7.846984\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] processed a total of 1699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290169.2403264, \"EndTime\": 1618290170.7045891, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1463.6967182159424, \"count\": 1, \"min\": 1463.6967182159424, \"max\": 1463.6967182159424}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1160.6563631817985 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=85, train loss <loss>=7.998831731301767\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[86] Batch[0] avg_epoch_loss=7.588120\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=7.5881195068359375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[86] Batch[5] avg_epoch_loss=8.260801\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=8.260801315307617\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:50 INFO 140529284842880] Epoch[86] Batch [5]#011Speed: 2066.83 samples/sec#011loss=8.260801\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[10] avg_epoch_loss=8.313752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=8.377293682098388\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [10]#011Speed: 727.75 samples/sec#011loss=8.377294\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[15] avg_epoch_loss=8.411545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=15 train loss <loss>=8.626688385009766\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [15]#011Speed: 1910.20 samples/sec#011loss=8.626688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[20] avg_epoch_loss=8.425371\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=20 train loss <loss>=8.469613647460937\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [20]#011Speed: 822.56 samples/sec#011loss=8.469614\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[25] avg_epoch_loss=8.363946\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=25 train loss <loss>=8.105964469909669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [25]#011Speed: 2049.80 samples/sec#011loss=8.105964\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[30] avg_epoch_loss=8.277409\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=30 train loss <loss>=7.827416801452637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [30]#011Speed: 944.66 samples/sec#011loss=7.827417\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[35] avg_epoch_loss=8.163513\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=35 train loss <loss>=7.457351875305176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [35]#011Speed: 2044.69 samples/sec#011loss=7.457352\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[40] avg_epoch_loss=8.100845\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=40 train loss <loss>=7.649635314941406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [40]#011Speed: 928.33 samples/sec#011loss=7.649635\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch[45] avg_epoch_loss=8.096050\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, batch=45 train loss <loss>=8.056732654571533\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:51 INFO 140529284842880] Epoch[86] Batch [45]#011Speed: 1816.12 samples/sec#011loss=8.056733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] processed a total of 1591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290170.7046819, \"EndTime\": 1618290172.0694675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1364.2635345458984, \"count\": 1, \"min\": 1364.2635345458984, \"max\": 1364.2635345458984}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1166.0883521307976 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=86, train loss <loss>=8.079554386138916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[0] avg_epoch_loss=7.476383\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=7.476383209228516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[5] avg_epoch_loss=8.138915\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=8.138915300369263\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [5]#011Speed: 2037.76 samples/sec#011loss=8.138915\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[10] avg_epoch_loss=8.274533\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=8.437274932861328\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [10]#011Speed: 942.74 samples/sec#011loss=8.437275\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[15] avg_epoch_loss=8.290853\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=15 train loss <loss>=8.326756000518799\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [15]#011Speed: 1977.87 samples/sec#011loss=8.326756\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[20] avg_epoch_loss=8.211881\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=20 train loss <loss>=7.959169483184814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [20]#011Speed: 935.65 samples/sec#011loss=7.959169\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[25] avg_epoch_loss=8.138838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=25 train loss <loss>=7.832056713104248\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [25]#011Speed: 1883.32 samples/sec#011loss=7.832057\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch[30] avg_epoch_loss=8.110492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=30 train loss <loss>=7.9630937576293945\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:52 INFO 140529284842880] Epoch[87] Batch [30]#011Speed: 846.20 samples/sec#011loss=7.963094\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch[35] avg_epoch_loss=8.050689\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=35 train loss <loss>=7.679912948608399\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch [35]#011Speed: 2016.87 samples/sec#011loss=7.679913\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch[40] avg_epoch_loss=8.031512\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=40 train loss <loss>=7.893436527252197\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch [40]#011Speed: 939.40 samples/sec#011loss=7.893437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch[45] avg_epoch_loss=8.026905\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=45 train loss <loss>=7.9891256332397464\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch [45]#011Speed: 2052.55 samples/sec#011loss=7.989126\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch[50] avg_epoch_loss=7.983579\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, batch=50 train loss <loss>=7.584985160827637\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[87] Batch [50]#011Speed: 1564.53 samples/sec#011loss=7.584985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] processed a total of 1686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290172.069557, \"EndTime\": 1618290173.4370039, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1366.9402599334717, \"count\": 1, \"min\": 1366.9402599334717, \"max\": 1366.9402599334717}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1233.3006231906882 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=87, train loss <loss>=7.955530940361743\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch[0] avg_epoch_loss=8.625721\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=8.625720977783203\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch[5] avg_epoch_loss=8.381888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=8.381887833277384\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch [5]#011Speed: 2050.60 samples/sec#011loss=8.381888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch[10] avg_epoch_loss=8.488681\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=8.616831970214843\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch [10]#011Speed: 918.69 samples/sec#011loss=8.616832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch[15] avg_epoch_loss=8.511954\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=15 train loss <loss>=8.563154220581055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:53 INFO 140529284842880] Epoch[88] Batch [15]#011Speed: 1877.80 samples/sec#011loss=8.563154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[20] avg_epoch_loss=8.495744\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=20 train loss <loss>=8.443874168395997\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [20]#011Speed: 817.37 samples/sec#011loss=8.443874\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[25] avg_epoch_loss=8.391205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=25 train loss <loss>=7.952139663696289\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [25]#011Speed: 1948.60 samples/sec#011loss=7.952140\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[30] avg_epoch_loss=8.262936\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=30 train loss <loss>=7.595937252044678\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [30]#011Speed: 913.83 samples/sec#011loss=7.595937\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[35] avg_epoch_loss=8.199040\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=35 train loss <loss>=7.802884578704834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [35]#011Speed: 2097.56 samples/sec#011loss=7.802885\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[40] avg_epoch_loss=8.122775\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=40 train loss <loss>=7.573668670654297\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [40]#011Speed: 935.57 samples/sec#011loss=7.573669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch[45] avg_epoch_loss=8.103805\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, batch=45 train loss <loss>=7.948251152038575\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[88] Batch [45]#011Speed: 2004.02 samples/sec#011loss=7.948251\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] processed a total of 1573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290173.4370875, \"EndTime\": 1618290174.7406714, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1302.983283996582, \"count\": 1, \"min\": 1302.983283996582, \"max\": 1302.983283996582}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1207.0990722321153 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=88, train loss <loss>=8.056635160446167\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[89] Batch[0] avg_epoch_loss=7.541365\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=7.541365146636963\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[89] Batch[5] avg_epoch_loss=8.088274\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=8.088274319966635\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:54 INFO 140529284842880] Epoch[89] Batch [5]#011Speed: 2050.73 samples/sec#011loss=8.088274\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[10] avg_epoch_loss=8.132823\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=8.186280441284179\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [10]#011Speed: 879.50 samples/sec#011loss=8.186280\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[15] avg_epoch_loss=8.204011\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=15 train loss <loss>=8.360624313354492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [15]#011Speed: 1705.41 samples/sec#011loss=8.360624\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[20] avg_epoch_loss=8.256198\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=20 train loss <loss>=8.423196411132812\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [20]#011Speed: 894.48 samples/sec#011loss=8.423196\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[25] avg_epoch_loss=8.228237\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=25 train loss <loss>=8.110801601409912\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [25]#011Speed: 1888.38 samples/sec#011loss=8.110802\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[30] avg_epoch_loss=8.171293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=30 train loss <loss>=7.875186634063721\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [30]#011Speed: 899.23 samples/sec#011loss=7.875187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[35] avg_epoch_loss=8.098541\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=35 train loss <loss>=7.647475147247315\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [35]#011Speed: 2045.13 samples/sec#011loss=7.647475\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch[40] avg_epoch_loss=8.059794\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=40 train loss <loss>=7.780814456939697\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:55 INFO 140529284842880] Epoch[89] Batch [40]#011Speed: 942.08 samples/sec#011loss=7.780814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[89] Batch[45] avg_epoch_loss=8.025793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=45 train loss <loss>=7.74698486328125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[89] Batch [45]#011Speed: 1706.53 samples/sec#011loss=7.746985\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[89] Batch[50] avg_epoch_loss=8.035621\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, batch=50 train loss <loss>=8.126040077209472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[89] Batch [50]#011Speed: 1215.01 samples/sec#011loss=8.126040\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] processed a total of 1688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290174.7407625, \"EndTime\": 1618290176.182082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1440.7899379730225, \"count\": 1, \"min\": 1440.7899379730225, \"max\": 1440.7899379730225}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1171.4771765818573 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=89, train loss <loss>=8.011551182225066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[0] avg_epoch_loss=8.404266\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=8.404266357421875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[5] avg_epoch_loss=8.352372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=8.35237201054891\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch [5]#011Speed: 1950.94 samples/sec#011loss=8.352372\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[10] avg_epoch_loss=8.193514\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=8.002883338928223\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch [10]#011Speed: 936.26 samples/sec#011loss=8.002883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[15] avg_epoch_loss=8.158433\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=15 train loss <loss>=8.081256580352782\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch [15]#011Speed: 2059.07 samples/sec#011loss=8.081257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[20] avg_epoch_loss=8.194175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=20 train loss <loss>=8.308550262451172\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch [20]#011Speed: 929.81 samples/sec#011loss=8.308550\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch[25] avg_epoch_loss=8.159380\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=25 train loss <loss>=8.013241958618163\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:56 INFO 140529284842880] Epoch[90] Batch [25]#011Speed: 2020.58 samples/sec#011loss=8.013242\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch[30] avg_epoch_loss=8.113921\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=30 train loss <loss>=7.87753438949585\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch [30]#011Speed: 900.17 samples/sec#011loss=7.877534\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch[35] avg_epoch_loss=8.037905\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=35 train loss <loss>=7.566600227355957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch [35]#011Speed: 1911.83 samples/sec#011loss=7.566600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch[40] avg_epoch_loss=8.006068\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=40 train loss <loss>=7.776844120025634\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch [40]#011Speed: 904.09 samples/sec#011loss=7.776844\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch[45] avg_epoch_loss=8.021487\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, batch=45 train loss <loss>=8.147919464111329\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[90] Batch [45]#011Speed: 1997.14 samples/sec#011loss=8.147919\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] processed a total of 1557 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290176.1821706, \"EndTime\": 1618290177.4648235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1282.1564674377441, \"count\": 1, \"min\": 1282.1564674377441, \"max\": 1282.1564674377441}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1214.23821638642 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=90, train loss <loss>=8.019792547031324\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch[0] avg_epoch_loss=7.621140\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=7.621140003204346\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch[5] avg_epoch_loss=8.020490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=8.020490010579428\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch [5]#011Speed: 2088.10 samples/sec#011loss=8.020490\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch[10] avg_epoch_loss=8.124430\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=8.249156951904297\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch [10]#011Speed: 915.99 samples/sec#011loss=8.249157\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch[15] avg_epoch_loss=8.209115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=15 train loss <loss>=8.39542179107666\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:57 INFO 140529284842880] Epoch[91] Batch [15]#011Speed: 1736.35 samples/sec#011loss=8.395422\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[20] avg_epoch_loss=8.256113\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=20 train loss <loss>=8.40650634765625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [20]#011Speed: 889.79 samples/sec#011loss=8.406506\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[25] avg_epoch_loss=8.221826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=25 train loss <loss>=8.077824592590332\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [25]#011Speed: 1797.14 samples/sec#011loss=8.077825\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[30] avg_epoch_loss=8.159282\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=30 train loss <loss>=7.834049224853516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [30]#011Speed: 913.24 samples/sec#011loss=7.834049\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[35] avg_epoch_loss=8.103320\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=35 train loss <loss>=7.756358337402344\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [35]#011Speed: 2013.25 samples/sec#011loss=7.756358\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[40] avg_epoch_loss=8.065112\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=40 train loss <loss>=7.790013599395752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [40]#011Speed: 812.24 samples/sec#011loss=7.790014\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[45] avg_epoch_loss=8.019515\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=45 train loss <loss>=7.645614814758301\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [45]#011Speed: 2094.59 samples/sec#011loss=7.645615\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch[50] avg_epoch_loss=7.999056\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, batch=50 train loss <loss>=7.81083574295044\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[91] Batch [50]#011Speed: 1721.64 samples/sec#011loss=7.810836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] processed a total of 1650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290177.464915, \"EndTime\": 1618290178.827861, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1362.3924255371094, \"count\": 1, \"min\": 1362.3924255371094, \"max\": 1362.3924255371094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1210.989085597237 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=91, train loss <loss>=7.964961281189551\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] Epoch[92] Batch[0] avg_epoch_loss=7.680354\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:58 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=7.680354118347168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[5] avg_epoch_loss=8.182351\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=8.182351112365723\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [5]#011Speed: 1955.31 samples/sec#011loss=8.182351\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[10] avg_epoch_loss=8.226163\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=8.27873706817627\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [10]#011Speed: 785.61 samples/sec#011loss=8.278737\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[15] avg_epoch_loss=8.327377\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=15 train loss <loss>=8.550046730041505\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [15]#011Speed: 1964.91 samples/sec#011loss=8.550047\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[20] avg_epoch_loss=8.297911\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=20 train loss <loss>=8.203621578216552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [20]#011Speed: 935.41 samples/sec#011loss=8.203622\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[25] avg_epoch_loss=8.202689\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=25 train loss <loss>=7.8027571678161625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [25]#011Speed: 2097.66 samples/sec#011loss=7.802757\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[30] avg_epoch_loss=8.137888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=30 train loss <loss>=7.800924587249756\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [30]#011Speed: 866.87 samples/sec#011loss=7.800925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch[35] avg_epoch_loss=8.104901\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=35 train loss <loss>=7.900380706787109\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:02:59 INFO 140529284842880] Epoch[92] Batch [35]#011Speed: 1830.18 samples/sec#011loss=7.900381\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch[40] avg_epoch_loss=8.077672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=40 train loss <loss>=7.88162260055542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch [40]#011Speed: 937.69 samples/sec#011loss=7.881623\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch[45] avg_epoch_loss=8.031148\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=45 train loss <loss>=7.64964542388916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch [45]#011Speed: 1726.66 samples/sec#011loss=7.649645\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch[50] avg_epoch_loss=7.992753\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, batch=50 train loss <loss>=7.639520359039307\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[92] Batch [50]#011Speed: 1511.59 samples/sec#011loss=7.639520\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] processed a total of 1627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290178.8279533, \"EndTime\": 1618290180.2044225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1375.9589195251465, \"count\": 1, \"min\": 1375.9589195251465, \"max\": 1375.9589195251465}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1182.3636848140256 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=92, train loss <loss>=7.992752701628442\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[0] avg_epoch_loss=7.628538\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=7.628537654876709\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[5] avg_epoch_loss=7.960839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=7.960838874181111\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch [5]#011Speed: 1749.99 samples/sec#011loss=7.960839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[10] avg_epoch_loss=8.137757\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=8.350057792663574\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch [10]#011Speed: 853.47 samples/sec#011loss=8.350058\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[15] avg_epoch_loss=8.222037\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=15 train loss <loss>=8.407455444335938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch [15]#011Speed: 1841.06 samples/sec#011loss=8.407455\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[20] avg_epoch_loss=8.258545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=20 train loss <loss>=8.37536792755127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch [20]#011Speed: 928.30 samples/sec#011loss=8.375368\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch[25] avg_epoch_loss=8.179187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=25 train loss <loss>=7.845887184143066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:00 INFO 140529284842880] Epoch[93] Batch [25]#011Speed: 2097.75 samples/sec#011loss=7.845887\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch[30] avg_epoch_loss=8.104345\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=30 train loss <loss>=7.715162754058838\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch [30]#011Speed: 767.39 samples/sec#011loss=7.715163\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch[35] avg_epoch_loss=8.039977\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=35 train loss <loss>=7.640897369384765\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch [35]#011Speed: 1754.34 samples/sec#011loss=7.640897\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch[40] avg_epoch_loss=7.990226\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=40 train loss <loss>=7.632017803192139\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch [40]#011Speed: 711.61 samples/sec#011loss=7.632018\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch[45] avg_epoch_loss=7.982044\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, batch=45 train loss <loss>=7.914948654174805\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[93] Batch [45]#011Speed: 1924.40 samples/sec#011loss=7.914949\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] processed a total of 1586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290180.2044873, \"EndTime\": 1618290181.6037505, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1398.7622261047363, \"count\": 1, \"min\": 1398.7622261047363, \"max\": 1398.7622261047363}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1133.7459865678657 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=93, train loss <loss>=7.974884748458862\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[94] Batch[0] avg_epoch_loss=8.446302\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=8.44630241394043\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[94] Batch[5] avg_epoch_loss=8.204041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=8.204041322072348\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[94] Batch [5]#011Speed: 2052.66 samples/sec#011loss=8.204041\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[94] Batch[10] avg_epoch_loss=8.217883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=8.234493255615234\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:01 INFO 140529284842880] Epoch[94] Batch [10]#011Speed: 923.70 samples/sec#011loss=8.234493\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[15] avg_epoch_loss=8.191592\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=15 train loss <loss>=8.133753108978272\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [15]#011Speed: 1833.94 samples/sec#011loss=8.133753\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[20] avg_epoch_loss=8.298839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=20 train loss <loss>=8.642027282714844\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [20]#011Speed: 876.49 samples/sec#011loss=8.642027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[25] avg_epoch_loss=8.225276\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=25 train loss <loss>=7.916310214996338\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [25]#011Speed: 1746.49 samples/sec#011loss=7.916310\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[30] avg_epoch_loss=8.166981\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=30 train loss <loss>=7.863846683502198\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [30]#011Speed: 851.49 samples/sec#011loss=7.863847\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[35] avg_epoch_loss=8.093840\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=35 train loss <loss>=7.640369606018067\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [35]#011Speed: 1831.40 samples/sec#011loss=7.640370\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[40] avg_epoch_loss=8.051092\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=40 train loss <loss>=7.743305015563965\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [40]#011Speed: 884.79 samples/sec#011loss=7.743305\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[45] avg_epoch_loss=8.052792\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=45 train loss <loss>=8.066727542877198\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [45]#011Speed: 2085.74 samples/sec#011loss=8.066728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch[50] avg_epoch_loss=7.968870\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, batch=50 train loss <loss>=7.196794223785401\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] Epoch[94] Batch [50]#011Speed: 1974.39 samples/sec#011loss=7.196794\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] processed a total of 1609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290181.603852, \"EndTime\": 1618290182.9501553, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1345.7438945770264, \"count\": 1, \"min\": 1345.7438945770264, \"max\": 1345.7438945770264}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1195.5138677540308 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] #quality_metric: host=algo-1, epoch=94, train loss <loss>=7.968870247111601\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:02 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[0] avg_epoch_loss=7.966335\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=7.966335296630859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[5] avg_epoch_loss=8.241500\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=8.241499741872152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [5]#011Speed: 1995.44 samples/sec#011loss=8.241500\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[10] avg_epoch_loss=8.162697\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=8.068132877349854\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [10]#011Speed: 816.65 samples/sec#011loss=8.068133\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[15] avg_epoch_loss=8.193554\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=15 train loss <loss>=8.26144027709961\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [15]#011Speed: 2075.77 samples/sec#011loss=8.261440\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[20] avg_epoch_loss=8.230047\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=20 train loss <loss>=8.346823120117188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [20]#011Speed: 970.09 samples/sec#011loss=8.346823\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[25] avg_epoch_loss=8.170094\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=25 train loss <loss>=7.918291282653809\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [25]#011Speed: 1986.09 samples/sec#011loss=7.918291\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[30] avg_epoch_loss=8.110179\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=30 train loss <loss>=7.7986249923706055\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [30]#011Speed: 955.82 samples/sec#011loss=7.798625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch[35] avg_epoch_loss=8.050779\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=35 train loss <loss>=7.682495975494385\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:03 INFO 140529284842880] Epoch[95] Batch [35]#011Speed: 2093.65 samples/sec#011loss=7.682496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch[40] avg_epoch_loss=8.020593\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=40 train loss <loss>=7.803256988525391\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch [40]#011Speed: 955.15 samples/sec#011loss=7.803257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch[45] avg_epoch_loss=7.998768\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=45 train loss <loss>=7.819804000854492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch [45]#011Speed: 1959.53 samples/sec#011loss=7.819804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch[50] avg_epoch_loss=7.985646\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, batch=50 train loss <loss>=7.864916515350342\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[95] Batch [50]#011Speed: 1872.91 samples/sec#011loss=7.864917\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] processed a total of 1603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290182.9502375, \"EndTime\": 1618290184.2357786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1284.9934101104736, \"count\": 1, \"min\": 1284.9934101104736, \"max\": 1284.9934101104736}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1247.3566367245287 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=95, train loss <loss>=7.9856456588296325\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[0] avg_epoch_loss=7.946496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=7.946495532989502\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[5] avg_epoch_loss=8.120688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=8.120688358942667\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch [5]#011Speed: 1912.06 samples/sec#011loss=8.120688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[10] avg_epoch_loss=8.131706\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=8.144926166534423\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch [10]#011Speed: 981.59 samples/sec#011loss=8.144926\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[15] avg_epoch_loss=8.291274\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=15 train loss <loss>=8.64232349395752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch [15]#011Speed: 2083.44 samples/sec#011loss=8.642323\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[20] avg_epoch_loss=8.346865\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=20 train loss <loss>=8.524757671356202\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch [20]#011Speed: 952.55 samples/sec#011loss=8.524758\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch[25] avg_epoch_loss=8.292293\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=25 train loss <loss>=8.063090133666993\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:04 INFO 140529284842880] Epoch[96] Batch [25]#011Speed: 1915.61 samples/sec#011loss=8.063090\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch[30] avg_epoch_loss=8.241373\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=30 train loss <loss>=7.976589488983154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch [30]#011Speed: 912.53 samples/sec#011loss=7.976589\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch[35] avg_epoch_loss=8.167622\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=35 train loss <loss>=7.710367202758789\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch [35]#011Speed: 1819.17 samples/sec#011loss=7.710367\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch[40] avg_epoch_loss=8.114266\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=40 train loss <loss>=7.730104541778564\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch [40]#011Speed: 842.44 samples/sec#011loss=7.730105\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch[45] avg_epoch_loss=8.105002\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=45 train loss <loss>=8.029030799865723\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch [45]#011Speed: 2099.81 samples/sec#011loss=8.029031\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch[50] avg_epoch_loss=8.059837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, batch=50 train loss <loss>=7.644320011138916\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[96] Batch [50]#011Speed: 1585.53 samples/sec#011loss=7.644320\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] processed a total of 1667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290184.235865, \"EndTime\": 1618290185.5987341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1362.3816967010498, \"count\": 1, \"min\": 1362.3816967010498, \"max\": 1362.3816967010498}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1223.4875659107784 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=96, train loss <loss>=8.016237978665334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[97] Batch[0] avg_epoch_loss=8.194407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=8.194406509399414\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[97] Batch[5] avg_epoch_loss=8.030770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=8.03076990445455\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[97] Batch [5]#011Speed: 2178.52 samples/sec#011loss=8.030770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[97] Batch[10] avg_epoch_loss=7.978959\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=7.916786003112793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:05 INFO 140529284842880] Epoch[97] Batch [10]#011Speed: 911.38 samples/sec#011loss=7.916786\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[15] avg_epoch_loss=8.116187\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=15 train loss <loss>=8.418089389801025\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [15]#011Speed: 2068.41 samples/sec#011loss=8.418089\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[20] avg_epoch_loss=8.142531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=20 train loss <loss>=8.226832485198974\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [20]#011Speed: 856.08 samples/sec#011loss=8.226832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[25] avg_epoch_loss=8.106445\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=25 train loss <loss>=7.95488338470459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [25]#011Speed: 1848.75 samples/sec#011loss=7.954883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[30] avg_epoch_loss=8.080789\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=30 train loss <loss>=7.947374248504639\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [30]#011Speed: 791.56 samples/sec#011loss=7.947374\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[35] avg_epoch_loss=8.063529\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=35 train loss <loss>=7.956517314910888\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [35]#011Speed: 2096.58 samples/sec#011loss=7.956517\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[40] avg_epoch_loss=8.018383\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=40 train loss <loss>=7.693331623077393\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [40]#011Speed: 999.68 samples/sec#011loss=7.693332\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[45] avg_epoch_loss=8.033478\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=45 train loss <loss>=8.157263851165771\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [45]#011Speed: 2091.68 samples/sec#011loss=8.157264\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch[50] avg_epoch_loss=8.029667\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, batch=50 train loss <loss>=7.994600486755371\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] Epoch[97] Batch [50]#011Speed: 1683.81 samples/sec#011loss=7.994600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] processed a total of 1661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290185.5988173, \"EndTime\": 1618290186.9339657, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1334.6378803253174, \"count\": 1, \"min\": 1334.6378803253174, \"max\": 1334.6378803253174}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1244.4107384506783 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] #quality_metric: host=algo-1, epoch=97, train loss <loss>=8.002109426718492\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:06 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[0] avg_epoch_loss=7.946870\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=7.94687032699585\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[5] avg_epoch_loss=8.147029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=8.147028684616089\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [5]#011Speed: 2167.27 samples/sec#011loss=8.147029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[10] avg_epoch_loss=8.261700\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=8.399306297302246\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [10]#011Speed: 766.90 samples/sec#011loss=8.399306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[15] avg_epoch_loss=8.160280\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=15 train loss <loss>=7.937155151367188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [15]#011Speed: 2102.49 samples/sec#011loss=7.937155\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[20] avg_epoch_loss=8.217142\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=20 train loss <loss>=8.39910135269165\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [20]#011Speed: 825.06 samples/sec#011loss=8.399101\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[25] avg_epoch_loss=8.148520\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=25 train loss <loss>=7.8603057861328125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [25]#011Speed: 1980.88 samples/sec#011loss=7.860306\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[30] avg_epoch_loss=8.097689\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=30 train loss <loss>=7.833366584777832\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [30]#011Speed: 971.32 samples/sec#011loss=7.833367\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch[35] avg_epoch_loss=8.050348\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=35 train loss <loss>=7.75683479309082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:07 INFO 140529284842880] Epoch[98] Batch [35]#011Speed: 2067.97 samples/sec#011loss=7.756835\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch[40] avg_epoch_loss=8.003663\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=40 train loss <loss>=7.667529773712158\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch [40]#011Speed: 941.54 samples/sec#011loss=7.667530\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch[45] avg_epoch_loss=8.012648\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=45 train loss <loss>=8.086327457427979\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch [45]#011Speed: 2039.46 samples/sec#011loss=8.086327\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch[50] avg_epoch_loss=7.988436\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, batch=50 train loss <loss>=7.7656806945800785\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[98] Batch [50]#011Speed: 1735.22 samples/sec#011loss=7.765681\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] processed a total of 1605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290186.9340596, \"EndTime\": 1618290188.26161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1327.0230293273926, \"count\": 1, \"min\": 1327.0230293273926, \"max\": 1327.0230293273926}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1209.3619959852358 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=98, train loss <loss>=7.988435520845301\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[0] avg_epoch_loss=8.303176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=8.303175926208496\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[5] avg_epoch_loss=8.355738\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=8.355738083521524\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch [5]#011Speed: 2001.67 samples/sec#011loss=8.355738\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[10] avg_epoch_loss=8.371532\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=8.390485572814942\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch [10]#011Speed: 962.04 samples/sec#011loss=8.390486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[15] avg_epoch_loss=8.210973\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=15 train loss <loss>=7.857743644714356\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch [15]#011Speed: 2082.12 samples/sec#011loss=7.857744\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[20] avg_epoch_loss=8.199600\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=20 train loss <loss>=8.163204956054688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch [20]#011Speed: 813.46 samples/sec#011loss=8.163205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch[25] avg_epoch_loss=8.147906\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=25 train loss <loss>=7.930790519714355\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:08 INFO 140529284842880] Epoch[99] Batch [25]#011Speed: 2057.74 samples/sec#011loss=7.930791\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch[30] avg_epoch_loss=8.078719\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=30 train loss <loss>=7.718947315216065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch [30]#011Speed: 873.43 samples/sec#011loss=7.718947\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch[35] avg_epoch_loss=8.040407\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=35 train loss <loss>=7.802874946594239\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch [35]#011Speed: 1991.55 samples/sec#011loss=7.802875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch[40] avg_epoch_loss=8.000101\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=40 train loss <loss>=7.709892463684082\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch [40]#011Speed: 814.20 samples/sec#011loss=7.709892\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch[45] avg_epoch_loss=7.999998\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=45 train loss <loss>=7.999157047271728\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch [45]#011Speed: 2061.89 samples/sec#011loss=7.999157\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch[50] avg_epoch_loss=8.052864\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, batch=50 train loss <loss>=8.539228153228759\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[99] Batch [50]#011Speed: 1924.69 samples/sec#011loss=8.539228\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] processed a total of 1602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290188.261696, \"EndTime\": 1618290189.6079392, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1345.7484245300293, \"count\": 1, \"min\": 1345.7484245300293, \"max\": 1345.7484245300293}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1190.310009656369 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=99, train loss <loss>=8.052863756815592\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[100] Batch[0] avg_epoch_loss=8.206899\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=8.20689868927002\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[100] Batch[5] avg_epoch_loss=8.140479\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=8.14047940572103\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[100] Batch [5]#011Speed: 2100.03 samples/sec#011loss=8.140479\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[100] Batch[10] avg_epoch_loss=8.204488\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=8.281297397613525\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:09 INFO 140529284842880] Epoch[100] Batch [10]#011Speed: 833.86 samples/sec#011loss=8.281297\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[15] avg_epoch_loss=8.309948\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=15 train loss <loss>=8.541960144042969\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [15]#011Speed: 1925.71 samples/sec#011loss=8.541960\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[20] avg_epoch_loss=8.327025\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=20 train loss <loss>=8.381671619415282\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [20]#011Speed: 842.05 samples/sec#011loss=8.381672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[25] avg_epoch_loss=8.223647\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=25 train loss <loss>=7.7894610404968265\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [25]#011Speed: 1809.23 samples/sec#011loss=7.789461\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[30] avg_epoch_loss=8.155188\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=30 train loss <loss>=7.799200630187988\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [30]#011Speed: 902.21 samples/sec#011loss=7.799201\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[35] avg_epoch_loss=8.072583\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=35 train loss <loss>=7.5604349136352536\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [35]#011Speed: 1668.18 samples/sec#011loss=7.560435\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[40] avg_epoch_loss=8.056528\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=40 train loss <loss>=7.940929889678955\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [40]#011Speed: 875.48 samples/sec#011loss=7.940930\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[45] avg_epoch_loss=8.017871\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=45 train loss <loss>=7.700878715515136\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [45]#011Speed: 2033.94 samples/sec#011loss=7.700879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch[50] avg_epoch_loss=8.019592\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, batch=50 train loss <loss>=8.035427856445313\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] Epoch[100] Batch [50]#011Speed: 1997.14 samples/sec#011loss=8.035428\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] processed a total of 1604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290189.6080222, \"EndTime\": 1618290190.9743934, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1365.8995628356934, \"count\": 1, \"min\": 1365.8995628356934, \"max\": 1365.8995628356934}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1174.2137612237009 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] #quality_metric: host=algo-1, epoch=100, train loss <loss>=8.019591911166321\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:10 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[0] avg_epoch_loss=8.083342\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=8.083341598510742\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[5] avg_epoch_loss=7.990260\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=7.9902604420979815\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [5]#011Speed: 2069.63 samples/sec#011loss=7.990260\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[10] avg_epoch_loss=8.052656\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=8.12753086090088\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [10]#011Speed: 792.65 samples/sec#011loss=8.127531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[15] avg_epoch_loss=8.058826\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=15 train loss <loss>=8.072399044036866\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [15]#011Speed: 2083.13 samples/sec#011loss=8.072399\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[20] avg_epoch_loss=8.130097\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=20 train loss <loss>=8.358164596557618\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [20]#011Speed: 931.11 samples/sec#011loss=8.358165\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[25] avg_epoch_loss=8.085112\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=25 train loss <loss>=7.896175289154053\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [25]#011Speed: 1998.43 samples/sec#011loss=7.896175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[30] avg_epoch_loss=8.067487\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=30 train loss <loss>=7.9758342742919925\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [30]#011Speed: 955.16 samples/sec#011loss=7.975834\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch[35] avg_epoch_loss=8.042538\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=35 train loss <loss>=7.887855529785156\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:11 INFO 140529284842880] Epoch[101] Batch [35]#011Speed: 1926.86 samples/sec#011loss=7.887856\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[101] Batch[40] avg_epoch_loss=7.961937\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=40 train loss <loss>=7.381613445281983\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[101] Batch [40]#011Speed: 967.18 samples/sec#011loss=7.381613\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[101] Batch[45] avg_epoch_loss=7.943046\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, batch=45 train loss <loss>=7.788135051727295\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[101] Batch [45]#011Speed: 2165.53 samples/sec#011loss=7.788135\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] processed a total of 1560 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290190.9744768, \"EndTime\": 1618290192.2277212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1252.732276916504, \"count\": 1, \"min\": 1252.732276916504, \"max\": 1252.732276916504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1245.1413105097567 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=101, train loss <loss>=7.936104122473269\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[0] avg_epoch_loss=8.237783\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=8.237783432006836\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[5] avg_epoch_loss=8.443839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=8.443839073181152\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch [5]#011Speed: 1910.03 samples/sec#011loss=8.443839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[10] avg_epoch_loss=8.491426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=8.548530960083008\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch [10]#011Speed: 856.92 samples/sec#011loss=8.548531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[15] avg_epoch_loss=8.514870\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=15 train loss <loss>=8.566447448730468\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch [15]#011Speed: 2067.06 samples/sec#011loss=8.566447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[20] avg_epoch_loss=8.426886\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=20 train loss <loss>=8.145334720611572\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch [20]#011Speed: 915.42 samples/sec#011loss=8.145335\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch[25] avg_epoch_loss=8.326693\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=25 train loss <loss>=7.905882930755615\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:12 INFO 140529284842880] Epoch[102] Batch [25]#011Speed: 2008.18 samples/sec#011loss=7.905883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch[30] avg_epoch_loss=8.239811\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=30 train loss <loss>=7.788027381896972\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch [30]#011Speed: 867.41 samples/sec#011loss=7.788027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch[35] avg_epoch_loss=8.156459\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=35 train loss <loss>=7.63967227935791\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch [35]#011Speed: 1915.44 samples/sec#011loss=7.639672\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch[40] avg_epoch_loss=8.107127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=40 train loss <loss>=7.7519347190856935\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch [40]#011Speed: 869.09 samples/sec#011loss=7.751935\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch[45] avg_epoch_loss=8.101079\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=45 train loss <loss>=8.051485347747803\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch [45]#011Speed: 2068.31 samples/sec#011loss=8.051485\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch[50] avg_epoch_loss=8.037366\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, batch=50 train loss <loss>=7.451211452484131\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[102] Batch [50]#011Speed: 1571.14 samples/sec#011loss=7.451211\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] processed a total of 1648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290192.2278197, \"EndTime\": 1618290193.599154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1370.8267211914062, \"count\": 1, \"min\": 1370.8267211914062, \"max\": 1370.8267211914062}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1202.0813508529436 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=102, train loss <loss>=8.0106696165525\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[103] Batch[0] avg_epoch_loss=8.074616\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=8.074616432189941\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[103] Batch[5] avg_epoch_loss=8.205733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=8.205733140309652\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[103] Batch [5]#011Speed: 1993.39 samples/sec#011loss=8.205733\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[103] Batch[10] avg_epoch_loss=8.242382\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=8.286361122131348\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:13 INFO 140529284842880] Epoch[103] Batch [10]#011Speed: 869.72 samples/sec#011loss=8.286361\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[15] avg_epoch_loss=8.285054\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=15 train loss <loss>=8.37893180847168\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [15]#011Speed: 1772.33 samples/sec#011loss=8.378932\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[20] avg_epoch_loss=8.249640\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=20 train loss <loss>=8.136317253112793\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [20]#011Speed: 911.40 samples/sec#011loss=8.136317\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[25] avg_epoch_loss=8.178159\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=25 train loss <loss>=7.877937507629395\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [25]#011Speed: 1697.19 samples/sec#011loss=7.877938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[30] avg_epoch_loss=8.100912\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=30 train loss <loss>=7.69922456741333\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [30]#011Speed: 901.85 samples/sec#011loss=7.699225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[35] avg_epoch_loss=8.032296\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=35 train loss <loss>=7.606878757476807\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [35]#011Speed: 2054.44 samples/sec#011loss=7.606879\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[40] avg_epoch_loss=8.002439\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=40 train loss <loss>=7.787466335296631\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [40]#011Speed: 989.71 samples/sec#011loss=7.787466\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[45] avg_epoch_loss=7.979859\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=45 train loss <loss>=7.794704151153565\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [45]#011Speed: 2094.38 samples/sec#011loss=7.794704\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch[50] avg_epoch_loss=7.954349\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, batch=50 train loss <loss>=7.719663333892822\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] Epoch[103] Batch [50]#011Speed: 1698.46 samples/sec#011loss=7.719663\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] processed a total of 1653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290193.599245, \"EndTime\": 1618290194.9458818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1346.1151123046875, \"count\": 1, \"min\": 1346.1151123046875, \"max\": 1346.1151123046875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1227.8779337983667 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] #quality_metric: host=algo-1, epoch=103, train loss <loss>=7.934520473847022\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:14 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[0] avg_epoch_loss=8.051170\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=8.051170349121094\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[5] avg_epoch_loss=8.211552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=8.211552460988363\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [5]#011Speed: 2052.09 samples/sec#011loss=8.211552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[10] avg_epoch_loss=8.225737\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=8.242758560180665\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [10]#011Speed: 955.29 samples/sec#011loss=8.242759\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[15] avg_epoch_loss=8.328154\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=15 train loss <loss>=8.553471946716309\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [15]#011Speed: 2170.12 samples/sec#011loss=8.553472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[20] avg_epoch_loss=8.380979\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=20 train loss <loss>=8.550020217895508\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [20]#011Speed: 898.31 samples/sec#011loss=8.550020\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[25] avg_epoch_loss=8.222539\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=25 train loss <loss>=7.55709056854248\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [25]#011Speed: 1796.47 samples/sec#011loss=7.557091\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[30] avg_epoch_loss=8.119131\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=30 train loss <loss>=7.581405830383301\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [30]#011Speed: 920.80 samples/sec#011loss=7.581406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch[35] avg_epoch_loss=8.060286\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=35 train loss <loss>=7.6954474449157715\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:15 INFO 140529284842880] Epoch[104] Batch [35]#011Speed: 2043.65 samples/sec#011loss=7.695447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[104] Batch[40] avg_epoch_loss=7.992531\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=40 train loss <loss>=7.504692649841308\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[104] Batch [40]#011Speed: 862.79 samples/sec#011loss=7.504693\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[104] Batch[45] avg_epoch_loss=7.987885\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, batch=45 train loss <loss>=7.949789714813233\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[104] Batch [45]#011Speed: 1856.76 samples/sec#011loss=7.949790\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] processed a total of 1590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290194.9459524, \"EndTime\": 1618290196.2256384, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1279.1633605957031, \"count\": 1, \"min\": 1279.1633605957031, \"max\": 1279.1633605957031}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1242.8757881491001 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=104, train loss <loss>=7.979477653503418\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[0] avg_epoch_loss=9.012268\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=9.01226806640625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[5] avg_epoch_loss=8.319939\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=8.319939136505127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch [5]#011Speed: 1981.49 samples/sec#011loss=8.319939\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[10] avg_epoch_loss=8.206032\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=8.069343185424804\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch [10]#011Speed: 875.45 samples/sec#011loss=8.069343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[15] avg_epoch_loss=8.244469\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=15 train loss <loss>=8.329031562805175\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch [15]#011Speed: 2169.08 samples/sec#011loss=8.329032\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[20] avg_epoch_loss=8.217083\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=20 train loss <loss>=8.129445838928223\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch [20]#011Speed: 947.34 samples/sec#011loss=8.129446\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch[25] avg_epoch_loss=8.179403\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=25 train loss <loss>=8.021147346496582\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:16 INFO 140529284842880] Epoch[105] Batch [25]#011Speed: 2160.22 samples/sec#011loss=8.021147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch[30] avg_epoch_loss=8.163029\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=30 train loss <loss>=8.077884006500245\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch [30]#011Speed: 936.38 samples/sec#011loss=8.077884\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch[35] avg_epoch_loss=8.100735\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=35 train loss <loss>=7.714515590667725\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch [35]#011Speed: 1873.71 samples/sec#011loss=7.714516\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch[40] avg_epoch_loss=8.073125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=40 train loss <loss>=7.874330615997314\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch [40]#011Speed: 832.95 samples/sec#011loss=7.874331\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch[45] avg_epoch_loss=8.114855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=45 train loss <loss>=8.457042121887207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch [45]#011Speed: 2077.31 samples/sec#011loss=8.457042\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch[50] avg_epoch_loss=8.064715\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, batch=50 train loss <loss>=7.6034258842468265\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[105] Batch [50]#011Speed: 1743.43 samples/sec#011loss=7.603426\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] processed a total of 1641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290196.225727, \"EndTime\": 1618290197.5837717, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1357.5279712677002, \"count\": 1, \"min\": 1357.5279712677002, \"max\": 1357.5279712677002}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1208.6913251055805 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=105, train loss <loss>=8.04750840480511\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[106] Batch[0] avg_epoch_loss=8.337221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=8.337221145629883\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[106] Batch[5] avg_epoch_loss=8.071568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=8.071568489074707\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[106] Batch [5]#011Speed: 2102.76 samples/sec#011loss=8.071568\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:17 INFO 140529284842880] Epoch[106] Batch[10] avg_epoch_loss=8.162650\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=8.27194881439209\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [10]#011Speed: 730.54 samples/sec#011loss=8.271949\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[15] avg_epoch_loss=8.285067\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=15 train loss <loss>=8.554382514953613\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [15]#011Speed: 1916.88 samples/sec#011loss=8.554383\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[20] avg_epoch_loss=8.264339\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=20 train loss <loss>=8.198008251190185\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [20]#011Speed: 927.60 samples/sec#011loss=8.198008\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[25] avg_epoch_loss=8.208595\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=25 train loss <loss>=7.974471473693848\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [25]#011Speed: 2068.11 samples/sec#011loss=7.974471\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[30] avg_epoch_loss=8.142619\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=30 train loss <loss>=7.799542236328125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [30]#011Speed: 930.47 samples/sec#011loss=7.799542\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[35] avg_epoch_loss=8.036987\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=35 train loss <loss>=7.382073497772216\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [35]#011Speed: 1920.21 samples/sec#011loss=7.382073\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[40] avg_epoch_loss=8.011569\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=40 train loss <loss>=7.82855806350708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [40]#011Speed: 727.23 samples/sec#011loss=7.828558\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[45] avg_epoch_loss=8.015045\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=45 train loss <loss>=8.043551540374756\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [45]#011Speed: 1975.30 samples/sec#011loss=8.043552\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch[50] avg_epoch_loss=7.975929\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, batch=50 train loss <loss>=7.616053199768066\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] Epoch[106] Batch [50]#011Speed: 1834.33 samples/sec#011loss=7.616053\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] processed a total of 1634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290197.5838706, \"EndTime\": 1618290198.9987428, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1414.3624305725098, \"count\": 1, \"min\": 1414.3624305725098, \"max\": 1414.3624305725098}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1155.197591671309 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] #quality_metric: host=algo-1, epoch=106, train loss <loss>=7.946578227556669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:18 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[0] avg_epoch_loss=7.879635\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=7.879634857177734\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[5] avg_epoch_loss=8.139739\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=8.139739433924357\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [5]#011Speed: 1721.18 samples/sec#011loss=8.139739\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[10] avg_epoch_loss=8.110370\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=8.075127220153808\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [10]#011Speed: 857.90 samples/sec#011loss=8.075127\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[15] avg_epoch_loss=8.212930\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=15 train loss <loss>=8.438562679290772\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [15]#011Speed: 1791.62 samples/sec#011loss=8.438563\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[20] avg_epoch_loss=8.219684\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=20 train loss <loss>=8.241294193267823\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [20]#011Speed: 868.83 samples/sec#011loss=8.241294\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[25] avg_epoch_loss=8.163752\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=25 train loss <loss>=7.928837490081787\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [25]#011Speed: 2039.88 samples/sec#011loss=7.928837\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[30] avg_epoch_loss=8.080562\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=30 train loss <loss>=7.647975158691406\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [30]#011Speed: 927.14 samples/sec#011loss=7.647975\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch[35] avg_epoch_loss=8.020894\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=35 train loss <loss>=7.65095157623291\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:19 INFO 140529284842880] Epoch[107] Batch [35]#011Speed: 1961.38 samples/sec#011loss=7.650952\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[107] Batch[40] avg_epoch_loss=7.986957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=40 train loss <loss>=7.742610168457031\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[107] Batch [40]#011Speed: 965.50 samples/sec#011loss=7.742610\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[107] Batch[45] avg_epoch_loss=7.979895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, batch=45 train loss <loss>=7.921992492675781\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[107] Batch [45]#011Speed: 1846.78 samples/sec#011loss=7.921992\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] processed a total of 1583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290198.9988182, \"EndTime\": 1618290200.313382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1313.990831375122, \"count\": 1, \"min\": 1313.990831375122, \"max\": 1313.990831375122}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1204.608357422477 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=107, train loss <loss>=7.969837646484375\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch[0] avg_epoch_loss=8.434207\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=8.43420696258545\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch[5] avg_epoch_loss=8.360447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=8.360446612040201\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch [5]#011Speed: 2061.34 samples/sec#011loss=8.360447\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch[10] avg_epoch_loss=8.252039\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=8.121949005126954\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch [10]#011Speed: 961.25 samples/sec#011loss=8.121949\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch[15] avg_epoch_loss=8.344219\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=15 train loss <loss>=8.547015190124512\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch [15]#011Speed: 2067.04 samples/sec#011loss=8.547015\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch[20] avg_epoch_loss=8.410657\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=20 train loss <loss>=8.623257255554199\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:20 INFO 140529284842880] Epoch[108] Batch [20]#011Speed: 895.51 samples/sec#011loss=8.623257\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[25] avg_epoch_loss=8.329539\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=25 train loss <loss>=7.988845920562744\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [25]#011Speed: 1786.87 samples/sec#011loss=7.988846\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[30] avg_epoch_loss=8.249855\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=30 train loss <loss>=7.835498809814453\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [30]#011Speed: 897.75 samples/sec#011loss=7.835499\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[35] avg_epoch_loss=8.190507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=35 train loss <loss>=7.8225480079650875\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [35]#011Speed: 1718.86 samples/sec#011loss=7.822548\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[40] avg_epoch_loss=8.143449\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=40 train loss <loss>=7.804633045196534\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [40]#011Speed: 828.34 samples/sec#011loss=7.804633\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[45] avg_epoch_loss=8.103895\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=45 train loss <loss>=7.779554653167724\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [45]#011Speed: 1700.17 samples/sec#011loss=7.779555\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch[50] avg_epoch_loss=8.063398\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, batch=50 train loss <loss>=7.690818977355957\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[108] Batch [50]#011Speed: 1601.44 samples/sec#011loss=7.690819\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] processed a total of 1665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290200.313472, \"EndTime\": 1618290201.7252364, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1411.2672805786133, \"count\": 1, \"min\": 1411.2672805786133, \"max\": 1411.2672805786133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1179.7041953687658 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=108, train loss <loss>=7.994270522639437\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[109] Batch[0] avg_epoch_loss=8.640716\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=8.640715599060059\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[109] Batch[5] avg_epoch_loss=8.592562\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=8.592562357584635\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:21 INFO 140529284842880] Epoch[109] Batch [5]#011Speed: 1704.44 samples/sec#011loss=8.592562\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[10] avg_epoch_loss=8.430915\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=8.236938190460204\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [10]#011Speed: 839.13 samples/sec#011loss=8.236938\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[15] avg_epoch_loss=8.325596\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=15 train loss <loss>=8.093892669677734\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [15]#011Speed: 1842.77 samples/sec#011loss=8.093893\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[20] avg_epoch_loss=8.343708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=20 train loss <loss>=8.40166940689087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [20]#011Speed: 883.99 samples/sec#011loss=8.401669\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[25] avg_epoch_loss=8.214983\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=25 train loss <loss>=7.674334144592285\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [25]#011Speed: 2003.61 samples/sec#011loss=7.674334\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[30] avg_epoch_loss=8.138340\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=30 train loss <loss>=7.739797878265381\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [30]#011Speed: 887.17 samples/sec#011loss=7.739798\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[35] avg_epoch_loss=8.081167\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=35 train loss <loss>=7.726694869995117\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [35]#011Speed: 1920.21 samples/sec#011loss=7.726695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[40] avg_epoch_loss=8.062149\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=40 train loss <loss>=7.9252214431762695\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [40]#011Speed: 977.90 samples/sec#011loss=7.925221\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch[45] avg_epoch_loss=8.070736\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, batch=45 train loss <loss>=8.14114694595337\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:22 INFO 140529284842880] Epoch[109] Batch [45]#011Speed: 2093.33 samples/sec#011loss=8.141147\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] processed a total of 1541 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290201.7253027, \"EndTime\": 1618290203.0249488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1299.0846633911133, \"count\": 1, \"min\": 1299.0846633911133, \"max\": 1299.0846633911133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1186.1164380808962 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=109, train loss <loss>=8.073973227520378\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[0] avg_epoch_loss=8.482383\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=8.482382774353027\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[5] avg_epoch_loss=8.316065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=8.316065470377604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [5]#011Speed: 2085.08 samples/sec#011loss=8.316065\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[10] avg_epoch_loss=8.285770\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=8.249414539337158\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [10]#011Speed: 943.48 samples/sec#011loss=8.249415\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[15] avg_epoch_loss=8.319468\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=15 train loss <loss>=8.39360408782959\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [15]#011Speed: 1882.47 samples/sec#011loss=8.393604\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[20] avg_epoch_loss=8.280060\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=20 train loss <loss>=8.153954219818115\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [20]#011Speed: 889.93 samples/sec#011loss=8.153954\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[25] avg_epoch_loss=8.218225\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=25 train loss <loss>=7.958519554138183\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [25]#011Speed: 2083.48 samples/sec#011loss=7.958520\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[30] avg_epoch_loss=8.134833\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=30 train loss <loss>=7.701190757751465\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [30]#011Speed: 953.34 samples/sec#011loss=7.701191\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch[35] avg_epoch_loss=8.070688\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=35 train loss <loss>=7.672990798950195\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:23 INFO 140529284842880] Epoch[110] Batch [35]#011Speed: 1701.91 samples/sec#011loss=7.672991\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch[40] avg_epoch_loss=8.057747\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=40 train loss <loss>=7.9645709037780765\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch [40]#011Speed: 876.27 samples/sec#011loss=7.964571\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch[45] avg_epoch_loss=8.088472\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=45 train loss <loss>=8.34041976928711\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch [45]#011Speed: 2073.26 samples/sec#011loss=8.340420\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch[50] avg_epoch_loss=8.040897\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, batch=50 train loss <loss>=7.603204727172852\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[110] Batch [50]#011Speed: 1751.58 samples/sec#011loss=7.603205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] processed a total of 1616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290203.0250266, \"EndTime\": 1618290204.331724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1306.2233924865723, \"count\": 1, \"min\": 1306.2233924865723, \"max\": 1306.2233924865723}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1237.0362984081132 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=110, train loss <loss>=8.040896855148615\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch[0] avg_epoch_loss=7.868067\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=7.868066787719727\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch[5] avg_epoch_loss=8.106486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=8.106486241022745\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch [5]#011Speed: 2077.36 samples/sec#011loss=8.106486\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch[10] avg_epoch_loss=8.110173\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=8.11459789276123\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch [10]#011Speed: 952.88 samples/sec#011loss=8.114598\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch[15] avg_epoch_loss=8.191644\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=15 train loss <loss>=8.370880126953125\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch [15]#011Speed: 2097.68 samples/sec#011loss=8.370880\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch[20] avg_epoch_loss=8.185576\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=20 train loss <loss>=8.166159629821777\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:24 INFO 140529284842880] Epoch[111] Batch [20]#011Speed: 943.74 samples/sec#011loss=8.166160\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch[25] avg_epoch_loss=8.151900\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=25 train loss <loss>=8.010457420349121\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch [25]#011Speed: 2056.01 samples/sec#011loss=8.010457\u001b[0m\n",
      "\n",
      "2021-04-13 05:03:41 Uploading - Uploading generated training model\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch[30] avg_epoch_loss=8.053839\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=30 train loss <loss>=7.543924045562744\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch [30]#011Speed: 950.88 samples/sec#011loss=7.543924\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch[35] avg_epoch_loss=7.999549\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=35 train loss <loss>=7.66295166015625\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch [35]#011Speed: 2083.73 samples/sec#011loss=7.662952\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch[40] avg_epoch_loss=7.936827\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=40 train loss <loss>=7.485226249694824\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch [40]#011Speed: 934.05 samples/sec#011loss=7.485226\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch[45] avg_epoch_loss=7.929205\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, batch=45 train loss <loss>=7.866707515716553\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Epoch[111] Batch [45]#011Speed: 2047.30 samples/sec#011loss=7.866708\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] processed a total of 1583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290204.3318098, \"EndTime\": 1618290205.6008062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1268.4917449951172, \"count\": 1, \"min\": 1268.4917449951172, \"max\": 1268.4917449951172}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #throughput_metric: host=algo-1, train throughput=1247.8010790022302 records/second\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, epoch=111, train loss <loss>=7.932836532592773\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Loading parameters from best epoch (71)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.600907, \"EndTime\": 1618290205.6091127, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 7.603883743286133, \"count\": 1, \"min\": 7.603883743286133, \"max\": 7.603883743286133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] stopping training now\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Final loss: 7.926837885154868 (occurred at epoch 71)\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #quality_metric: host=algo-1, train final_loss <loss>=7.926837885154868\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 WARNING 140529284842880] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.6091866, \"EndTime\": 1618290205.6956275, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 85.67333221435547, \"count\": 1, \"min\": 85.67333221435547, \"max\": 85.67333221435547}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.69569, \"EndTime\": 1618290205.7196422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 109.73334312438965, \"count\": 1, \"min\": 109.73334312438965, \"max\": 109.73334312438965}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.7196975, \"EndTime\": 1618290205.72377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 4.035711288452148, \"count\": 1, \"min\": 4.035711288452148, \"max\": 4.035711288452148}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] #memory_usage::<batchbuffer> = 0.60882568359375 mb\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:25 INFO 140529284842880] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.7238176, \"EndTime\": 1618290205.7247887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04172325134277344, \"count\": 1, \"min\": 0.04172325134277344, \"max\": 0.04172325134277344}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:28 INFO 140529284842880] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:30 INFO 140529284842880] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:32 INFO 140529284842880] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:35 INFO 140529284842880] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290205.7248538, \"EndTime\": 1618290217.7561986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 12031.446933746338, \"count\": 1, \"min\": 12031.446933746338, \"max\": 12031.446933746338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, RMSE): 30779.585785115087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, mean_absolute_QuantileLoss): 71278424.20808284\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, mean_wQuantileLoss): 0.34064237112863827\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.1]): 0.22230083329501343\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.2]): 0.35411411941950555\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.3]): 0.41386342872846565\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.4]): 0.4324106008730814\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.5]): 0.42514800250072\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.6]): 0.39810396884209176\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.7]): 0.35158739578848\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.8]): 0.28318642967201507\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #test_score (algo-1, wQuantileLoss[0.9]): 0.18506656103837177\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #quality_metric: host=algo-1, test RMSE <loss>=30779.585785115087\u001b[0m\n",
      "\u001b[34m[04/13/2021 05:03:37 INFO 140529284842880] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.34064237112863827\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1618290217.756305, \"EndTime\": 1618290217.7640827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.960630416870117, \"count\": 1, \"min\": 6.960630416870117, \"max\": 6.960630416870117}, \"totaltime\": {\"sum\": 167179.05688285828, \"count\": 1, \"min\": 167179.05688285828, \"max\": 167179.05688285828}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-04-13 05:03:48 Completed - Training job completed\n",
      "Training seconds: 232\n",
      "Billable seconds: 232\n",
      "CPU times: user 1.24 s, sys: 181 ms, total: 1.42 s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        super().__init__(*args, \n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:22: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:49: FutureWarning: Creating a DatetimeIndex by passing range endpoints is deprecated.  Use `pandas.date_range` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-09-24</th>\n",
       "      <td>-23.113403</td>\n",
       "      <td>19.265923</td>\n",
       "      <td>65.973511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-25</th>\n",
       "      <td>-27.542595</td>\n",
       "      <td>14.479571</td>\n",
       "      <td>49.086594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>-20.278811</td>\n",
       "      <td>20.404276</td>\n",
       "      <td>55.948685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-27</th>\n",
       "      <td>-17.502697</td>\n",
       "      <td>18.958078</td>\n",
       "      <td>49.140556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-28</th>\n",
       "      <td>-24.291796</td>\n",
       "      <td>16.675251</td>\n",
       "      <td>48.680645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-29</th>\n",
       "      <td>-14.774632</td>\n",
       "      <td>11.109956</td>\n",
       "      <td>46.623497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>-20.392916</td>\n",
       "      <td>9.677359</td>\n",
       "      <td>36.403214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0.1        0.5        0.9\n",
       "2019-09-24 -23.113403  19.265923  65.973511\n",
       "2019-09-25 -27.542595  14.479571  49.086594\n",
       "2019-09-26 -20.278811  20.404276  55.948685\n",
       "2019-09-27 -17.502697  18.958078  49.140556\n",
       "2019-09-28 -24.291796  16.675251  48.680645\n",
       "2019-09-29 -14.774632  11.109956  46.623497\n",
       "2019-09-30 -20.392916   9.677359  36.403214"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[0][start_dataset:end_test-1],quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a plotting function that queries the model and displays the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the function previously defined, to look at the forecast of any customer at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the consumption of an office after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1598852eae48c787a9f23ec6167362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=120, description='customer_id', max=1603, style=SliderStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "@interact_manual(\n",
    "    customer_id=IntSlider(min=0, max=1603, value=120, style=style), \n",
    "#     forecast_day= prediction_length,\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_days_plot=IntSlider(min=1, max=50, value=7, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(customer_id, confidence, history_days_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[customer_id],\n",
    "        forecast_date= start_predict -1,#end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_days_plot,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:22: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:49: FutureWarning: Creating a DatetimeIndex by passing range endpoints is deprecated.  Use `pandas.date_range` instead.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds = []\n",
    "quantiles=[0.10, 0.5, 0.90]\n",
    "\n",
    "for i, ts_dict in enumerate(timeseries):\n",
    "    ts = ts_dict[start_dataset:end_test-1]\n",
    "    if ts.shape[0] == 0:\n",
    "        preds.append(None)\n",
    "        continue\n",
    "    pred = predictor.predict(ts=ts, quantiles=quantiles)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predict result and label data into a csv for later use\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(timeseries)):\n",
    "    y =  timeseries[i][start_predict:end_predict].fillna(0)\n",
    "    y_hat = preds[i]['0.5']\n",
    "    dd = pd.DataFrame({'store_id':i,'y_label': y, 'y_pred': y_hat})\n",
    "    df = pd.concat([df, dd])\n",
    "df.index.name='date'\n",
    "df.to_csv(\"data/deepar-predict-result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print sample site stores prediction result, you can print more if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "\n",
    "def show_metrics(sample_sites, pred_result, target_quantile='0.5'):\n",
    "    preds = pred_result\n",
    "    for i in sample_sites:  # TODO len(timeseries)\n",
    "        if preds[i] is None:\n",
    "            continue\n",
    "        \n",
    "        s = timeseries[i].fillna(0)\n",
    "        \n",
    "        print(\"i:\", i)\n",
    "        p10 = preds[i]['0.1']\n",
    "        p90 = preds[i]['0.9']\n",
    "        y_label =  timeseries[i][start_predict:end_predict].fillna(0)\n",
    "        y_pred = preds[i][target_quantile]\n",
    "        if y_label.shape[0] != y_pred.shape[0]:\n",
    "            continue\n",
    "        \n",
    "        print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_label, y_pred)))\n",
    "        print(\"MAE:\",metrics.mean_absolute_error(y_label, y_pred))\n",
    "        print(\"Target Mean:\",y_label.mean())\n",
    "        print(pd.DataFrame({'y_pred': y_pred, 'y_label': y_label}))\n",
    "        plt.figure(figsize=(12,6))\n",
    "        s.plot(label='target %s'%str(i))\n",
    "        plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "        y_pred.plot(label='prediction median')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "show_metrics(sample_sites, preds, target_quantile='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional features\n",
    "\n",
    "We have seen how to prepare a dataset and run DeepAR for a simple example.\n",
    "\n",
    "In addition DeepAR supports the following features:\n",
    "\n",
    "* missing values: DeepAR can handle missing values in the time series during training as well as for inference.\n",
    "* Additional time features: DeepAR provides a set default time series features such as hour of day etc. However, you can provide additional feature time series via the `dynamic_feat` field. \n",
    "* generalize frequencies: any integer multiple of the previously supported base frequencies (minutes `min`, hours `H`, days `D`, weeks `W`, month `M`) are now allowed; e.g., `15min`. We already demonstrated this above by using `1D` frequency.\n",
    "* categories: If your time series belong to different groups (e.g. types of product, regions, etc), this information can be encoded as one or more categorical features using the `cat` field.\n",
    "\n",
    "We will now demonstrate categories and time features support. For this part we will reuse the stores sales dataset, we get categories of the stores from data owner and crawler weather data from http://lishi.tianqi.com/shenzhen as dynamic time series: \n",
    "* weather time series will contain: the high temperature, the low temperature of the day, whether is rainy, whether is sunshine, whether is cloudy;\n",
    "* besides that, we add addtional time series that whether the day is weekend;\n",
    "* categories: 1D array for each target series;\n",
    "\n",
    "dynamic_feat: total 6 dynamic_feat, the high temperature, the low temperature, is_sunshine, is_rain, is_cloudy, is_weekend. As the stores are in the same city, so each store time series will use same dynamic_feat values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read those data as we have prepared in advance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat is stand for category, here it is a one 1d vector for each store time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.read_csv('data/cat.csv', sep=\",\", index_col=0, parse_dates=True)\n",
    "cat_num_timeseries = cat.shape[0]\n",
    "cat_timeseries = []\n",
    "for i in range(cat_num_timeseries):\n",
    "    cat_timeseries.append(cat.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_weekend = pd.read_csv('data/is_weekend.csv', sep=\",\", index_col=0, parse_dates=True)\n",
    "is_weekend_timeseries = is_weekend.iloc[:,0]\n",
    "is_weekend_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('data/shenzhen_weather.csv', sep=\",\", index_col=0, parse_dates=True)\n",
    "w_num_timeseries = weather.shape[1]\n",
    "w_timeseries = []\n",
    "for i in range(w_num_timeseries):\n",
    "    w_timeseries.append(weather.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dynamic_feat= []\n",
    "for i in range(5):\n",
    "    train_dynamic_feat.append(w_timeseries[i][start_dataset:end_training -1].tolist())\n",
    "train_dynamic_feat.append(is_weekend_timeseries[start_dataset:end_training -1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dynamic_feat= []\n",
    "for i in range(5):\n",
    "    test_dynamic_feat.append(w_timeseries[i][start_dataset:end_test -1].tolist())\n",
    "test_dynamic_feat.append(is_weekend_timeseries[start_dataset:end_test -1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dynamic_feat= []\n",
    "for i in range(5):\n",
    "    predict_dynamic_feat.append(w_timeseries[i][start_dataset:end_predict -1].tolist())\n",
    "predict_dynamic_feat.append(is_weekend_timeseries[start_dataset:end_predict -1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the case each time series consists of a start time stamp (``start``) and a list of values (``target``)as well as ``dynamic_feat`` for time-series features and ``cat`` for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": timeseries[i][start_dataset:end_training -1].tolist(),\n",
    "        \"cat\": cat_timeseries[i].tolist(),\n",
    "        \"dynamic_feat\": train_dynamic_feat\n",
    "    }\n",
    "    for i in range(cat_num_timeseries)\n",
    "]\n",
    "print(len(training_data_new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": timeseries[i][start_dataset:end_test -1].tolist(),\n",
    "        \"cat\": cat_timeseries[i].tolist(),\n",
    "        \"dynamic_feat\": test_dynamic_feat\n",
    "    }\n",
    "    for i in range(cat_num_timeseries)\n",
    "]\n",
    "print(len(test_data_new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_consistency(train_dataset, test_dataset=None):\n",
    "    d = train_dataset[0]\n",
    "    has_dynamic_feat = 'dynamic_feat' in d\n",
    "    if has_dynamic_feat:\n",
    "        num_dynamic_feat = len(d['dynamic_feat'])\n",
    "    has_cat = 'cat' in d\n",
    "    if has_cat:\n",
    "        num_cat = len(d['cat'])\n",
    "    \n",
    "    def check_ds(ds):\n",
    "        for i, d in enumerate(ds):\n",
    "            if has_dynamic_feat:\n",
    "                assert 'dynamic_feat' in d\n",
    "                assert num_dynamic_feat == len(d['dynamic_feat'])\n",
    "                for f in d['dynamic_feat']:\n",
    "                    assert len(d['target']) == len(f)\n",
    "                    \n",
    "            if has_cat:\n",
    "                assert 'cat' in d\n",
    "                assert len(d['cat']) == num_cat\n",
    "    check_ds(train_dataset)\n",
    "    if test_dataset is not None:\n",
    "        \n",
    "        check_ds(test_dataset)\n",
    "        \n",
    "check_dataset_consistency(training_data_new_features, test_data_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the new train and test data to local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"data/train_new_features.json\", training_data_new_features)\n",
    "write_dicts_to_file(\"data/test_new_features.json\", test_data_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the new train and test data to S3 bucket, here we will use new bucket which has a extend name '-new-features' of the bucket name you used in before case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_data_path_new_features = \"s3://{}/{}-new-features/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path_new_features = \"s3://{}/{}-new-features/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "print('Uploading to S3 this may take a few minutes depending on your connection.')\n",
    "copy_to_s3(\"data/train_new_features.json\", s3_data_path_new_features + \"/train/train_new_features.json\", override=True)\n",
    "copy_to_s3(\"data/test_new_features.json\", s3_data_path_new_features + \"/test/test_new_features.json\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agagin, let's setup the estimator and hyperparameters, then let's fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_new_features = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-stores-sales-demo',\n",
    "    output_path=s3_output_path_new_features\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"epochs\": \"400\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"num_dynamic_feat\": \"auto\",  # this will use the `dynamic_feat` field if it's present in the data\n",
    "}\n",
    "estimator_new_features.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "estimator_new_features.fit(\n",
    "    inputs={\n",
    "        \"train\": \"{}/train/\".format(s3_data_path_new_features),\n",
    "        \"test\": \"{}/test/\".format(s3_data_path_new_features)\n",
    "    }, \n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we spawn an endpoint to visualize our forecasts on examples we send on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor_new_features = estimator_new_features.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the new predictor to predict the same time series. You can see this time we need pass **cat**, **dynamic_feat** to the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = 120\n",
    "predictor_new_features.predict(\n",
    "    ts=timeseries[store_id][start_dataset:end_test-1], \n",
    "    cat=cat_timeseries[store_id].tolist(),\n",
    "    dynamic_feat=predict_dynamic_feat, \n",
    "    quantiles=[0.1, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can query the endpoint to see predictions for arbitrary time series and time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "@interact_manual(\n",
    "    customer_id=IntSlider(min=0, max=1603, value=120, style=style), \n",
    "#     forecast_day=IntSlider(min=0, max=7, value=0,step=1, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_days_plot=IntSlider(min=1, max=50, value=7, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(customer_id, confidence, history_days_plot, show_samples):\n",
    "    target = timeseries[customer_id]\n",
    "    dynamic_feat = predict_dynamic_feat\n",
    "    cat=cat_timeseries[store_id].tolist()\n",
    "    plot(\n",
    "        predictor_new_features,\n",
    "        target_ts=target,\n",
    "        forecast_date= start_predict -1,\n",
    "        cat=cat,\n",
    "        dynamic_feat=dynamic_feat,\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_days_plot,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds_new = []\n",
    "quantiles=[0.10, 0.5, 0.90]\n",
    "\n",
    "for i, ts_dict in enumerate(timeseries):\n",
    "    ts = ts_dict[start_dataset:end_test-1]\n",
    "    if ts.shape[0] == 0:\n",
    "        preds_new.append(None)\n",
    "        continue\n",
    "    cat = cat_timeseries[i].tolist()\n",
    "    dynamic_feat = predict_dynamic_feat   \n",
    "    pred = predictor_new_features.predict(ts, cat=cat, dynamic_feat=dynamic_feat, quantiles=quantiles)\n",
    "    preds_new.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predict result and label data into a csv for later use\n",
    "df_new = pd.DataFrame()\n",
    "for i in range(len(timeseries)):\n",
    "    y =  timeseries[i][start_predict:end_predict].fillna(0)\n",
    "    y_hat = preds_new[i]['0.5']\n",
    "    dd = pd.DataFrame({'store_id':i,'y_label': y, 'y_pred': y_hat})\n",
    "    df_new = pd.concat([df_new, dd])\n",
    "df_new.index.name='date'\n",
    "df_new.to_csv(\"data/deepar-new-features-predict-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_metrics( sample_sites, preds_new, target_quantile='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_features.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
